text
"TensorFlow is a software library for machine learning and artificial intelligence. It can be used across a range of tasks, but is used mainly for training and inference of neural networks. It is one of the most popular deep learning frameworks, alongside others such as PyTorch and PaddlePaddle. It is free and open-source software released under the Apache License 2.0.
It was developed by the Google Brain team for Google's internal use in research and production. The initial version was released under the Apache License 2.0 in 2015. Google released an updated version, TensorFlow 2.0, in September 2019.
TensorFlow can be used in a wide variety of programming languages, including Python, JavaScript, C++, and Java, facilitating its use in a range of applications in many sectors.


== History ==


=== DistBelief ===
Starting in 2011, Google Brain built DistBelief as a proprietary machine learning system based on deep learning neural networks. Its use grew rapidly across diverse Alphabet companies in both research and commercial applications."
"In natural language processing, a word embedding is a representation of a word. The embedding is used in text analysis. Typically, the representation is a real-valued vector that encodes the meaning of the word in such a way that the words that are closer in the vector space are expected to be similar in meaning. Word embeddings can be obtained using language modeling and feature learning techniques, where words or phrases from the vocabulary are mapped to vectors of real numbers.
Methods to generate this mapping include neural networks, dimensionality reduction on the word co-occurrence matrix, probabilistic models, explainable knowledge base method, and explicit representation in terms of the context in which words appear.
Word and phrase embeddings, when used as the underlying input representation, have been shown to boost the performance in NLP tasks such as syntactic parsing and sentiment analysis.


== Development and history of the approach ==
In distributional semantics, a quantitative methodological approach for understanding meaning in observed language, word embeddings or semantic feature space models have been used as a knowledge representation for some time. Such models aim to quantify and categorize semantic similarities between linguistic items based on their distributional properties in large samples of language data.  The underlying idea that ""a word is characterized by the company it keeps"" was proposed in a 1957 article by John Rupert Firth, but also has roots in the contemporaneous work on search systems and in cognitive psychology.
The notion of a semantic space with lexical items (words or multi-word terms) represented as vectors or embeddings is based on the computational challenges of capturing distributional characteristics and using them for practical application to measure similarity between words, phrases, or entire documents."
"Neural architecture search (NAS) is a technique for automating the design of artificial neural networks (ANN), a widely used model in the field of machine learning. NAS has been used to design networks that are on par with or outperform hand-designed architectures. Methods for NAS can be categorized according to the search space, search strategy and performance estimation strategy used:

The search space defines the type(s) of ANN that can be designed and optimized.
The search strategy defines the approach used to explore the search space.
The performance estimation strategy evaluates the performance of a possible ANN from its design (without constructing and training it).
NAS is closely related to hyperparameter optimization and meta-learning and is a subfield of automated machine learning (AutoML).


== Reinforcement learning ==
Reinforcement learning (RL) can underpin a NAS search strategy. Barret Zoph and Quoc Viet Le applied NAS with RL targeting the CIFAR-10 dataset and achieved a network architecture that rivals the best manually-designed architecture for accuracy, with an error rate of 3.65, 0.09 percent better and 1.05x faster than a related hand-designed model. On the Penn Treebank dataset, that model composed a recurrent cell that outperforms LSTM, reaching a test set perplexity of 62.4, or 3.6 perplexity better than the prior leading system. On the PTB character language modeling task it achieved bits per character of 1.214."
"In statistics, naive Bayes classifiers are a family of linear ""probabilistic classifiers"" which assumes that the features are conditionally independent, given the target class. The strength (naivety) of this assumption is what gives the classifier its name. These classifiers are among the simplest Bayesian network models.
Naive Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables (features/predictors) in a learning problem. Maximum-likelihood training can be done by evaluating a closed-form expression,: 718  which takes linear time, rather than by expensive iterative approximation as used for many other types of classifiers.
In the statistics literature, naive Bayes models are known under a variety of names, including simple Bayes and independence Bayes. All these names reference the use of Bayes' theorem in the classifier's decision rule, but naive Bayes is not (necessarily) a Bayesian method.


== Introduction ==
Naive Bayes is a simple technique for constructing classifiers: models that assign class labels to problem instances, represented as vectors of feature values, where the class labels are drawn from some finite set. There is not a single algorithm for training such classifiers, but a family of algorithms based on a common principle: all naive Bayes classifiers assume that the value of a particular feature is independent of the value of any other feature, given the class variable. For example, a fruit may be considered to be an apple if it is red, round, and about 10 cm in diameter."
"TensorFlow is a software library for machine learning and artificial intelligence. It can be used across a range of tasks, but is used mainly for training and inference of neural networks. It is one of the most popular deep learning frameworks, alongside others such as PyTorch and PaddlePaddle. It is free and open-source software released under the Apache License 2.0.
It was developed by the Google Brain team for Google's internal use in research and production. The initial version was released under the Apache License 2.0 in 2015. Google released an updated version, TensorFlow 2.0, in September 2019.
TensorFlow can be used in a wide variety of programming languages, including Python, JavaScript, C++, and Java, facilitating its use in a range of applications in many sectors.


== History ==


=== DistBelief ===
Starting in 2011, Google Brain built DistBelief as a proprietary machine learning system based on deep learning neural networks. Its use grew rapidly across diverse Alphabet companies in both research and commercial applications."
"A Usenet newsgroup is a repository usually within the Usenet system, for messages posted from users in different locations using the Internet. They are discussion groups and are not devoted to publishing news. Newsgroups are technically distinct from, but functionally similar to, discussion forums on the World Wide Web. Newsreader software is used to read the content of newsgroups. Before the adoption of the World Wide Web, Usenet newsgroups were among the most popular Internet services.
Communication is facilitated by the Network News Transfer Protocol (NNTP) which allows connection to Usenet servers and data transfer over the internet.  Similar to another early (yet still used) protocol SMTP which is used for email messages, NNTP allows both server-server and client-server communication.  This means that newsgroups can be replicated from server to server which gives the Usenet network the ability to maintain a level of robust data persistence as a result of built-in data redundancy.  However, most users will access using only the client-server commands of NNTP and in almost all cases will use a GUI for browsing as opposed to command line based client-server communication specified in the NNTP protocol.


== Types ==

Newsgroups generally come in either of two types, binary or text."
"Hugging Face, Inc. is an American company incorporated under the Delaware General Corporation Law and based in New York City that develops computation tools for building applications using machine learning. It is most notable for its transformers library built for natural language processing applications and its platform that allows users to share machine learning models and datasets and showcase their work.


== History ==
The company was founded in 2016 by French entrepreneurs Clément Delangue, Julien Chaumond, and Thomas Wolf in New York City, originally as a company that developed a chatbot app targeted at teenagers. The company was named after the U+1F917 🤗 HUGGING FACE emoji. After open sourcing the model behind the chatbot, the company pivoted to focus on being a platform for machine learning.
In March 2021, Hugging Face raised US$40 million in a Series B funding round.
On April 28, 2021, the company launched the BigScience Research Workshop in collaboration with several other research groups to release an open large language model. In 2022, the workshop concluded with the announcement of BLOOM, a multilingual large language model with 176 billion parameters.
In December 2022, the company acquired Gradio, an open source library built for developing machine learning applications in Python."
"OpenAI is an American artificial intelligence (AI) research organization founded in December 2015 and headquartered in San Francisco, California. Its stated mission is to develop ""safe and beneficial"" artificial general intelligence (AGI), which it defines as ""highly autonomous systems that outperform humans at most economically valuable work"". As a leading organization in the ongoing AI boom, OpenAI is known for the GPT family of large language models, the DALL-E series of text-to-image models, and a text-to-video model named Sora. Its release of ChatGPT in November 2022 has been credited with catalyzing widespread interest in generative AI.
The organization consists of the non-profit OpenAI, Inc., registered in Delaware, and its for-profit subsidiary introduced in 2019, OpenAI Global, LLC. Microsoft owns roughly 49% of OpenAI's equity, having invested US$13 billion. It also provides computing resources to OpenAI through its cloud platform, Microsoft Azure.
In 2023 and 2024, OpenAI faced multiple lawsuits for alleged copyright infringement against authors and media companies whose work was used to train some of OpenAI's products. In November 2023, OpenAI's board removed Sam Altman as CEO, citing a lack of confidence in him, but reinstated him five days later after negotiations resulting in a reconstructed board. Many AI safety researchers left OpenAI in 2024.


== History ==


=== 2015–2018: Non-profit beginnings ===

In December 2015, OpenAI was founded by Sam Altman, Elon Musk, Ilya Sutskever, Greg Brockman, Trevor Blackwell, Vicki Cheung, Andrej Karpathy, Durk Kingma, John Schulman, Pamela Vagata, and Wojciech Zaremba, with Sam Altman and Elon Musk as the co-chairs. A total of $1 billion in capital was pledged by Sam Altman, Greg Brockman, Elon Musk, Reid Hoffman, Jessica Livingston, Peter Thiel, Amazon Web Services (AWS), Infosys, and YC Research."
"Quantum neural networks are computational neural network models which are based on the principles of quantum mechanics. The first ideas on quantum neural computation were published independently in 1995 by Subhash Kak and Ron Chrisley, engaging with the theory of quantum mind, which posits that quantum effects play a role in cognitive function. However, typical research in quantum neural networks involves combining classical artificial neural network models (which are widely used in machine learning for the important task of pattern recognition) with the advantages of quantum information in order to develop more efficient algorithms. One important motivation for these investigations is the difficulty to train classical neural networks, especially in big data applications. The hope is that features of quantum computing such as quantum parallelism or the effects of interference and entanglement can be used as resources. Since the technological implementation of a quantum computer is still in a premature stage, such quantum neural network models are mostly theoretical proposals that await their full implementation in physical experiments.
Most Quantum neural networks are developed as feed-forward networks. Similar to their classical counterparts, this structure intakes input from one layer of qubits, and passes that input onto another layer of qubits. This layer of qubits evaluates this information and passes on the output to the next layer. Eventually the path leads to the final layer of qubits."
"Deep reinforcement learning (deep RL) is a subfield of machine learning that combines reinforcement learning (RL) and deep learning. RL considers the problem of a computational agent learning to make decisions by trial and error. Deep RL incorporates deep learning into the solution, allowing agents to make decisions from unstructured input data without manual engineering of the state space. Deep RL algorithms are able to take in very large inputs (e.g. every pixel rendered to the screen in a video game) and decide what actions to perform to optimize an objective (e.g. maximizing the game score). Deep reinforcement learning has been used for a diverse set of applications including but not limited to robotics, video games, natural language processing, computer vision, education, transportation, finance and healthcare.


== Overview ==


=== Deep learning ===

Deep learning is a form of machine learning that utilizes a neural network to transform a set of inputs into a set of outputs via an artificial neural network. Deep learning methods, often using supervised learning with labeled datasets, have been shown to solve tasks that involve handling complex, high-dimensional raw input data (such as images) with less manual feature engineering than prior methods, enabling significant progress in several fields including computer vision and natural language processing. In the past decade, deep RL has achieved remarkable results on a range of problems, from single and multiplayer games such as Go, Atari Games, and Dota 2 to robotics."
"Weak supervision (also known as semi-supervised learning) is a paradigm in machine learning, the relevance and notability of which increased with the advent of large language models due to large amount of data required to train them.  It is characterized by using a combination of a small amount of human-labeled data (exclusively used in more expensive and time-consuming supervised learning paradigm), followed by a large amount of unlabeled data (used exclusively in unsupervised learning paradigm). In other words, the desired output values are provided only for a subset of the training data. The remaining data is unlabeled or imprecisely labeled. Intuitively, it can be seen as an exam and labeled data as sample problems that the teacher solves for the class as an aid in solving another set of problems. In the transductive setting, these unsolved problems act as exam questions. In the inductive setting, they become practice problems of the sort that will make up the exam. Technically, it could be viewed as performing clustering and then labeling the clusters with the labeled data, pushing the decision boundary away from high-density regions, or learning an underlying one-dimensional manifold where the data reside.


== Problem ==

The acquisition of labeled data for a learning problem often requires a skilled human agent (e.g. to transcribe an audio segment) or a physical experiment (e.g."
"A.I. Artificial Intelligence (or simply A.I.) is a 2001 American science fiction film directed by Steven Spielberg. The screenplay by Spielberg and screen story by Ian Watson are loosely based on the 1969 short story ""Supertoys Last All Summer Long"" by Brian Aldiss. Set in a futuristic society, the film stars Haley Joel Osment as David, a childlike android uniquely programmed with the ability to love. Jude Law, Frances O'Connor, Brendan Gleeson and William Hurt star in supporting roles.
Development of A.I. originally began after producer and director Stanley Kubrick acquired the rights to Aldiss's story in the early 1970s. Kubrick hired a series of writers, including Aldiss, Bob Shaw, Ian Watson and Sara Maitland, until the mid-1990s. The film languished in development hell for years, partly because Kubrick felt that computer-generated imagery was not advanced enough to create the David character, which he believed no child actor would convincingly portray. In 1995, Kubrick handed A.I. to Spielberg, but the film did not gain momentum until Kubrick died in 1999. Spielberg remained close to Watson's treatment for the screenplay and dedicated the film to Kubrick.
A.I. Artificial Intelligence was released on June 29, 2001, by Warner Bros."
"Sentiment analysis (also known as opinion mining or emotion AI) is the use of natural language processing, text analysis, computational linguistics, and biometrics to systematically identify, extract, quantify, and study affective states and subjective information. Sentiment analysis is widely applied to voice of the customer materials such as reviews and survey responses, online and social media, and healthcare materials for applications that range from marketing to customer service to clinical medicine. With the rise of deep language models, such as RoBERTa, also more difficult data domains can be analyzed, e.g., news texts where authors typically express their opinion/sentiment less explicitly.


== Simple cases ==
Coronet has the best lines of all day cruisers.
Bertram has a deep V hull and runs easily through seas.
Pastel-colored 1980s day cruisers from Florida are ugly.
I dislike old cabin cruisers.


== More challenging examples ==
I do not dislike cabin cruisers. (Negation handling)
Disliking watercraft is not really my thing. (Negation, inverted word order)
Sometimes I really hate RIBs."
"Gensim is an open-source library for unsupervised topic modeling, document indexing, retrieval by similarity, and other natural language processing functionalities, using modern statistical machine learning.
Gensim is implemented in Python and Cython for performance. Gensim is designed to handle large text collections using data streaming and incremental online algorithms, which differentiates it from most other machine learning software packages that target only in-memory processing.


== Main Features ==
Gensim includes streamed parallelized implementations of fastText, word2vec and doc2vec algorithms, as well as latent semantic analysis (LSA, LSI, SVD), non-negative matrix factorization (NMF), latent Dirichlet allocation (LDA), tf-idf and random projections.
Some of the novel online algorithms in Gensim were also published in the 2011 PhD dissertation Scalability of Semantic Analysis in Natural Language Processing of Radim Řehůřek, the creator of Gensim.


== Uses of Gensim ==
Gensim library has been used and cited in over 1400 commercial and academic applications as of 2018, in a diverse array of disciplines from medicine to insurance claim analysis to patent search. The software has been covered in several new articles, podcasts and interviews.


== Free and Commercial Support ==
The open source code is developed and hosted on GitHub and a public support forum is maintained on Google Groups and Gitter.
Gensim is commercially supported by the company rare-technologies.com, who also provide student mentorships and academic thesis projects for Gensim via their Student Incubator programme."
"Markov decision process (MDP), also called a stochastic dynamic program or stochastic control problem, is a model for sequential decision making when outcomes are uncertain.
Originating from operations research in the 1950s, MDPs have since gained recognition in a variety of fields, including ecology, economics, healthcare, telecommunications and reinforcement learning. Reinforcement learning utilizes the MDP framework to model the interaction between a learning agent and its environment. In this framework, the interaction is characterized by states, actions, and rewards. The MDP framework is designed to provide a simplified representation of key elements of artificial intelligence challenges. These elements encompass the understanding of cause and effect, the management of uncertainty and nondeterminism, and the pursuit of explicit goals.
The name comes from its connection to Markov chains, a concept developed by the Russian mathematician Andrey Markov. The ""Markov"" in ""Markov decision process"" refers to the underlying structure of state transitions that still follow the Markov property. The process is called a ""decision process"" because it involves making decisions that influence these state transitions, extending the concept of a Markov chain into the realm of decision-making under uncertainty.


== Definition ==

A Markov decision process is a 4-tuple 
  
    
      
        (
        S
        ,
        A
        ,
        
          P
          
            a
          
        
        ,
        
          R
          
            a
          
        
        )
      
    
    {\displaystyle (S,A,P_{a},R_{a})}
  
, where:

  
    
      
        S
      
    
    {\displaystyle S}
  
 is a set of states called the state space."
"In statistics, naive Bayes classifiers are a family of linear ""probabilistic classifiers"" which assumes that the features are conditionally independent, given the target class. The strength (naivety) of this assumption is what gives the classifier its name. These classifiers are among the simplest Bayesian network models.
Naive Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables (features/predictors) in a learning problem. Maximum-likelihood training can be done by evaluating a closed-form expression,: 718  which takes linear time, rather than by expensive iterative approximation as used for many other types of classifiers.
In the statistics literature, naive Bayes models are known under a variety of names, including simple Bayes and independence Bayes. All these names reference the use of Bayes' theorem in the classifier's decision rule, but naive Bayes is not (necessarily) a Bayesian method.


== Introduction ==
Naive Bayes is a simple technique for constructing classifiers: models that assign class labels to problem instances, represented as vectors of feature values, where the class labels are drawn from some finite set. There is not a single algorithm for training such classifiers, but a family of algorithms based on a common principle: all naive Bayes classifiers assume that the value of a particular feature is independent of the value of any other feature, given the class variable. For example, a fruit may be considered to be an apple if it is red, round, and about 10 cm in diameter."
"Ash or ashes are the solid remnants of fires. Specifically, ash refers to all non-aqueous, non-gaseous residues that remain after something burns. In analytical chemistry, to analyse the mineral and metal content of chemical samples,  ash is the non-gaseous, non-liquid residue after complete combustion.
Ashes as the end product of incomplete combustion are mostly mineral, but usually still contain an amount of combustible organic or other oxidizable residues. The best-known type of ash is wood ash, as a product of wood combustion in campfires, fireplaces, etc. The darker the wood ashes, the higher the content of remaining charcoal from incomplete combustion. The ashes are of different types. Some ashes contain natural compounds that make soil fertile. Others have chemical compounds that can be toxic but may break up in soil from chemical changes and microorganism activity.
Like soap, ash is also a disinfecting agent (alkaline)."
"""Attention Is All You Need"" is a 2017 landmark research paper in machine learning authored by eight scientists working at Google. The paper introduced a new deep learning architecture known as the transformer, based on the attention mechanism proposed in 2014 by Bahdanau et al. It is considered a foundational paper in modern artificial intelligence, as the transformer approach has become the main architecture of large language models like those based on GPT. At the time, the focus of the research was on improving Seq2seq techniques for machine translation, but the authors go further in the paper, foreseeing the technique's potential for other tasks like question answering and what is now known as multimodal Generative AI.
The paper's title is a reference to the song ""All You Need Is Love"" by the Beatles. The name ""Transformer"" was picked because Uszkoreit liked the sound of that word.
An early design document was titled ""Transformers: Iterative Self-Attention and Processing for Various Tasks"", and included an illustration of six characters from the Transformers animated show. The team was named Team Transformer.
Some early examples that the team tried their Transformer architecture on included English-to-German translation, generating Wikipedia articles on ""The Transformer"", and parsing. These convinced the team that the Transformer is a general purpose language model, and not just good for translation.
As of 2024, the paper has been cited more than 100,000 times.


== Authors ==
The authors of the paper are: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Lukasz Kaiser, and Illia Polosukhin."
"Dimensionality reduction, or dimension reduction, is the transformation of data from a high-dimensional space into a low-dimensional space so that the low-dimensional representation retains some meaningful properties of the original data, ideally close to its intrinsic dimension. Working in high-dimensional spaces can be undesirable for many reasons; raw data are often sparse as a consequence of the curse of dimensionality, and analyzing the data is usually computationally intractable. Dimensionality reduction is common in fields that deal with large numbers of observations and/or large numbers of variables, such as signal processing, speech recognition, neuroinformatics, and bioinformatics.
Methods are commonly divided into linear and nonlinear approaches. Approaches can also be divided into feature selection and feature extraction. Dimensionality reduction can be used for noise reduction, data visualization, cluster analysis, or as an intermediate step to facilitate other analyses.


== Feature selection ==

The process of feature selection aims to find a suitable subset of the input variables (features, or attributes) for the task at hand. The three strategies are: the filter strategy (e.g., information gain), the wrapper strategy (e.g., accuracy-guided search), and the embedded strategy (features are added or removed while building the model based on prediction errors).
Data analysis such as regression or classification can be done in the reduced space more accurately than in the original space.


== Feature projection ==

Feature projection (also called feature extraction) transforms the data from the high-dimensional space to a space of fewer dimensions."
"In statistics and natural language processing, a topic model is a type of statistical model for discovering the abstract ""topics"" that occur in a collection of documents. Topic modeling is a frequently used text-mining tool for discovery of hidden semantic structures in a text body. Intuitively, given that a document is about a particular topic, one would expect particular words to appear in the document more or less frequently: ""dog"" and ""bone"" will appear more often in documents about dogs, ""cat"" and ""meow"" will appear in documents about cats, and ""the"" and ""is"" will appear approximately equally in both. A document typically concerns multiple topics in different proportions; thus, in a document that is 10% about cats and 90% about dogs, there would probably be about 9 times more dog words than cat words. The ""topics"" produced by topic modeling techniques are clusters of similar words. A topic model captures this intuition in a mathematical framework, which allows examining a set of documents and discovering, based on the statistics of the words in each, what the topics might be and what each document's balance of topics is.
Topic models are also referred to as probabilistic topic models, which refers to statistical algorithms for discovering the latent semantic structures of an extensive text body. In the age of information, the amount of the written material we encounter each day is simply beyond our processing capacity. Topic models can help to organize and offer insights for us to understand large collections of unstructured text bodies. Originally developed as a text-mining tool, topic models have been used to detect instructive structures in data such as genetic information, images, and networks."
"Amazon SageMaker is a cloud-based machine-learning platform that allows the creation, training, and deployment by developers of machine-learning (ML) models on the cloud. It can be used to deploy ML models on embedded systems and edge-devices. The platform was launched in November 2017.


== Capabilities ==
SageMaker enables developers to operate at a number of different levels of abstraction when training and deploying machine learning models. At its highest level of abstraction, SageMaker provides pre-trained ML models that can be deployed as-is. In addition, it offers a number of built-in ML algorithms that developers can train on their own data. 
The platform also features managed instances of TensorFlow and Apache MXNet, where developers can create their own ML algorithms from scratch. Regardless of which level of abstraction is used, a developer can connect their SageMaker-enabled ML models to other AWS services, such as the Amazon DynamoDB database for structured data storage, AWS Batch for offline batch processing, or Amazon Kinesis for real-time processing.


== Development interfaces ==
A number of interfaces are available for developers to interact with SageMaker. First, there is a web API that remotely controls a SageMaker server instance."
"In digital circuits and machine learning, a one-hot is a group of bits among which the legal combinations of values are only those with a single high (1) bit and all the others low (0). A similar implementation in which all bits are '1' except one '0' is sometimes called one-cold. In statistics, dummy variables represent a similar technique for representing categorical data.


== Applications ==


=== Digital circuitry ===
One-hot encoding is often used for indicating the state of a state machine. When using binary, a decoder is needed to determine the state. A one-hot state machine, however, does not need a decoder as the state machine is in the nth state if, and only if, the nth bit is high.
A ring counter with 15 sequentially ordered states is an example of a state machine. A 'one-hot' implementation would have 15 flip flops chained in series with the Q output of each flip flop connected to the D input of the next and the D input of the first flip flop connected to the Q output of the 15th flip flop. The first flip flop in the chain represents the first state, the second represents the second state, and so on to the 15th flip flop, which represents the last state. Upon reset of the state machine all of the flip flops are reset to '0' except the first in the chain, which is set to '1'."
"Explainable AI (XAI), often overlapping with interpretable AI, or explainable machine learning (XML), either refers to an artificial intelligence (AI) system over which it is possible for humans to retain intellectual oversight, or refers to the methods to achieve this. The main focus is usually on the reasoning behind the decisions or predictions made by the AI which are made more understandable and transparent. XAI counters the ""black box"" tendency of machine learning, where even the AI's designers cannot explain why it arrived at a specific decision.
XAI hopes to help users of AI-powered systems perform more effectively by improving their understanding of how those systems reason. XAI may be an implementation of the social right to explanation. Even if there is no such legal right or regulatory requirement, XAI can improve the user experience of a product or service by helping end users trust that the AI is making good decisions. XAI aims to explain what has been done, what is being done, and what will be done next, and to unveil which information these actions are based on. This makes it possible to confirm existing knowledge, challenge existing knowledge, and generate new assumptions.
Machine learning (ML) algorithms used in AI can be categorized as white-box or black-box. White-box models provide results that are understandable to experts in the domain."
"Home automation or domotics is building automation for a home. A home automation system will monitor and/or control home attributes such as lighting, climate, entertainment systems, and appliances. It may also include home security such as access control and alarm systems. 
The phrase smart home refers to home automation devices that have internet access.  Home automation, a broader category, includes any device that can be monitored or controlled via wireless radio signals, not just those having internet access.  When connected with the Internet, home sensors and activation devices are an important constituent of the Internet of Things (""IoT"").
A home automation system typically connects controlled devices to a central smart home hub (sometimes called a ""gateway""). The user interface for control of the system uses either wall-mounted terminals, tablet or desktop computers, a mobile phone application, or a Web interface that may also be accessible off-site through the Internet.


== History ==
Early home automation began with labor-saving machines. Self-contained electric or gas powered home appliances became viable in the 1900s with the introduction of electric power distribution and led to the introduction of washing machines (1904), water heaters (1889), refrigerators (1913), sewing machines, dishwashers, and clothes dryers."
"In neural networks, a pooling layer is a kind of network layer that downsamples and aggregates information that is dispersed among many vectors into fewer vectors. It has several uses. It removes redundant information, reducing the amount of computation and memory required, makes the model more robust to small variations in the input, and increases the receptive field of neurons in later layers in the network.


== Convolutional neural network pooling ==
Pooling is most commonly used in convolutional neural networks (CNN). Below is a description of pooling in 2-dimensional CNNs. The generalization to n-dimensions is immediate.
As notation, we consider a tensor 
  
    
      
        x
        ∈
        
          
            R
          
          
            H
            ×
            W
            ×
            C
          
        
      
    
    {\displaystyle x\in \mathbb {R} ^{H\times W\times C}}
  
, where 
  
    
      
        H
      
    
    {\displaystyle H}
  
 is height, 
  
    
      
        W
      
    
    {\displaystyle W}
  
 is width, and 
  
    
      
        C
      
    
    {\displaystyle C}
  
 is the number of channels. A pooling layer outputs a tensor 
  
    
      
        y
        ∈
        
          
            R
          
          
            
              H
              ′
            
            ×
            
              W
              ′
            
            ×
            
              C
              ′
            
          
        
      
    
    {\displaystyle y\in \mathbb {R} ^{H'\times W'\times C'}}
  
.
We define two variables 
  
    
      
        f
        ,
        s
      
    
    {\displaystyle f,s}
  
 called ""filter size"" (aka ""kernel size"") and ""stride"". Sometimes, it is necessary to use a different filter size and stride for horizontal and vertical directions."
"In statistics, a mixture model is a probabilistic model for representing the presence of subpopulations within an overall population, without requiring that an observed data set should identify the sub-population to which an individual observation belongs. Formally a mixture model corresponds to the mixture distribution that represents the probability distribution of observations in the overall population. However, while problems associated with ""mixture distributions"" relate to deriving the properties of the overall population from those of the sub-populations, ""mixture models"" are used to make statistical inferences about the properties of the sub-populations given only observations on the pooled population, without sub-population identity information. Mixture models are used for clustering, under the name model-based clustering, and also for density estimation.
Mixture models should not be confused with models for compositional data, i.e., data whose components are constrained to sum to a constant value (1, 100%, etc.). However, compositional models can be thought of as mixture models, where members of the population are sampled at random. Conversely, mixture models can be thought of as compositional models, where the total size reading population has been normalized to 1.


== Structure ==


=== General mixture model ===
A typical finite-dimensional mixture model is a hierarchical model consisting of the following components:

N random variables that are observed, each distributed according to a mixture of K components, with the components belonging to the same parametric family of distributions (e.g., all normal, all Zipfian, etc.) but with different parameters
N random latent variables specifying the identity of the mixture component of each observation, each distributed according to a K-dimensional categorical distribution
A set of K mixture weights, which are probabilities that sum to 1.
A set of K parameters, each specifying the parameter of the corresponding mixture component.  In many cases, each ""parameter"" is actually a set of parameters."
"Meta-learning is a branch of metacognition concerned with learning about one's own learning and learning processes. 
The term comes from the meta prefix's modern meaning of an abstract recursion, or ""X about X"", similar to its use in metaknowledge, metamemory, and meta-emotion.


== Meta learning model for teams and relationships ==
Marcial Losada and other researchers have attempted to create a meta learning model to analyze teams and relationships. A 2013 paper provided a strong critique of this attempt, arguing that it was based on misapplication of complex mathematical modelling. This led to its abandonment by at least one former proponent.
The meta learning model proposed by Losada is identical to the Lorenz system, which was originally proposed as a simplified mathematical model for atmospheric convection. It comprises one control parameter and three state variables, which in this case have been mapped to ""connectivity"", ""inquiry-advocacy"", ""positivity-negativity"", and ""other-self"" (external-internal focus) respectively. The state variables are linked by a set of nonlinear differential equations. This has been criticized as a poorly defined, poorly justified, and invalid application of differential equations.
Losada and colleagues claim to have arrived at the meta learning model from thousands of time series data generated at two human interaction laboratories in Ann Arbor, Michigan, and Cambridge, Massachusetts, although the details of the collection of this data, and the connection between the time series data and the model is unclear."
"In video games, artificial intelligence (AI) is used to generate responsive, adaptive or intelligent behaviors primarily in non-playable characters (NPCs) similar to human-like intelligence. Artificial intelligence has been an integral part of video games since their inception in 1948, first seen in the game Nim. AI in video games is a distinct subfield and differs from academic AI. It serves to improve the game-player experience rather than machine learning or decision making. During the golden age of arcade video games the idea of AI opponents was largely popularized in the form of graduated difficulty levels, distinct movement patterns, and in-game events dependent on the player's input. Modern games often implement existing techniques such as pathfinding and decision trees to guide the actions of NPCs. AI is often used in mechanisms which are not immediately visible to the user, such as data mining and procedural-content generation. One of the most infamous examples of this NPC technology and gradual difficulty levels can be found in the game Mike Tyson's Punch-Out!! (1987).  
In general, game AI does not, as might be thought and sometimes is depicted to be the case, mean a realization of an artificial person corresponding to an NPC in the manner of the Turing test or an artificial general intelligence.


== Overview ==
The term game AI is used to refer to a broad set of algorithms that also include techniques from control theory, robotics, computer graphics and computer science in general, and so video game AI may often not constitute ""true AI"" in that such techniques do not necessarily facilitate computer learning or other standard criteria, only constituting ""automated computation"" or a predetermined and limited set of responses to a predetermined and limited set of inputs."
"In statistics, naive Bayes classifiers are a family of linear ""probabilistic classifiers"" which assumes that the features are conditionally independent, given the target class. The strength (naivety) of this assumption is what gives the classifier its name. These classifiers are among the simplest Bayesian network models.
Naive Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables (features/predictors) in a learning problem. Maximum-likelihood training can be done by evaluating a closed-form expression,: 718  which takes linear time, rather than by expensive iterative approximation as used for many other types of classifiers.
In the statistics literature, naive Bayes models are known under a variety of names, including simple Bayes and independence Bayes. All these names reference the use of Bayes' theorem in the classifier's decision rule, but naive Bayes is not (necessarily) a Bayesian method.


== Introduction ==
Naive Bayes is a simple technique for constructing classifiers: models that assign class labels to problem instances, represented as vectors of feature values, where the class labels are drawn from some finite set. There is not a single algorithm for training such classifiers, but a family of algorithms based on a common principle: all naive Bayes classifiers assume that the value of a particular feature is independent of the value of any other feature, given the class variable. For example, a fruit may be considered to be an apple if it is red, round, and about 10 cm in diameter."
"A torch is a stick with combustible material at one end which can be used as a light source or to set something on fire. Torches have been used throughout history, and are still used in processions, symbolic and religious events, and in juggling entertainment. In some countries, notably the United Kingdom and Australia, ""torch"" in modern usage is also the term for a battery-operated portable light. 


== Etymology ==
From the Old French ""torche"" meaning ""twisted thing"", hence ""torch formed of twisted tow dipped in wax"", probably from Vulgar Latin *torca, alteration of Late Latin torqua, variant of classical Latin torques ""collar of twisted metal"", from torquere ""to twist"".


== Torch construction ==
Torch construction has varied through history depending on the torch's purpose. Torches were usually constructed of a wooden stave with one end wrapped in a material which was soaked in a flammable substance. In the United States, black bear bones may have been used. Modern procession torches are made from coarse hessian rolled into a tube and soaked in wax. A wooden handle is usually used, and a cardboard collar is attached to deflect any wax droplets. They are an easy, safe and relatively cheap way to hold a flame aloft in a parade or to provide illumination in any after-dark celebration."
"Reinforcement learning (RL) is an interdisciplinary area of machine learning and optimal control concerned with how an intelligent agent should take actions in a dynamic environment in order to maximize a reward signal. Reinforcement learning is one of the three basic machine learning paradigms, alongside supervised learning and unsupervised learning.
Q-learning at its simplest stores data in tables. This approach becomes infeasible as the number of states/actions increases (e.g., if the state space or action space were continuous), as the probability of the agent visiting a particular state and performing a particular action diminishes. 
Reinforcement learning differs from supervised learning in not needing labelled input-output pairs to be presented, and in not needing sub-optimal actions to be explicitly corrected. Instead, the focus is on finding a balance between exploration (of uncharted territory) and exploitation (of current knowledge) with the goal of maximizing the cumulative reward (the feedback of which might be incomplete or delayed). The search for this balance is known as the exploration-exploitation dilemma.

The environment is typically stated in the form of a Markov decision process (MDP), as many reinforcement learning algorithms use dynamic programming techniques. The main difference between classical dynamic programming methods and reinforcement learning algorithms is that the latter do not assume knowledge of an exact mathematical model of the Markov decision process, and they target large MDPs where exact methods become infeasible. 


== Introduction ==

Due to its generality, reinforcement learning is studied in many disciplines, such as game theory, control theory, operations research, information theory, simulation-based optimization, multi-agent systems, swarm intelligence, and statistics."
"Random search (RS) is a family of numerical optimization methods that do not require the gradient of the problem to be optimized, and RS can hence be used on functions that are not continuous or differentiable. Such optimization methods are also known as direct-search, derivative-free, or black-box methods.
Anderson in 1953 reviewed the progress of methods in finding maximum or minimum of problems using a series of guesses distributed with a certain order or pattern in the parameter searching space, e.g. a confounded design with exponentially distributed spacings/steps. This search goes on sequentially on each parameter and refines iteratively on the best guesses from the last sequence. The pattern can be a grid (factorial) search of all parameters, a sequential search on each parameter, or a combination of both. The method was developed to screen the experimental conditions in chemical reactions by a number of scientists listed in Anderson's paper. A MATLAB code reproducing the sequential procedure for the general non-linear regression of an example mathematical model can be found here (JCFit @ GitHub).
The name ""random search"" is attributed to Rastrigin who made an early presentation of RS along with basic mathematical analysis. RS works by iteratively moving to better positions in the search space, which are sampled from a hypersphere surrounding the current position."
"In statistics, the standard score is the number of standard deviations by which the value of a raw score (i.e., an observed value or data point) is above or below the mean value of what is being observed or measured. Raw scores above the mean have positive standard scores, while those below the mean have negative standard scores. 
It is calculated by subtracting the population mean from an individual raw score and then dividing the difference by the population standard deviation. This process of converting a raw score into a standard score is called standardizing or normalizing (however, ""normalizing"" can refer to many types of ratios; see Normalization for more). 
Standard scores are most commonly called z-scores; the two terms may be used interchangeably, as they are in this article. Other equivalent terms in use include z-value, z-statistic, normal score, standardized variable and pull in high energy physics. 
Computing a z-score requires knowledge of the mean and standard deviation of the complete population to which a data point belongs; if one only has a sample of observations from the population, then the analogous computation using the sample mean and sample standard deviation yields the t-statistic.


== Calculation ==
If the population mean and population standard deviation are known, a raw score 
x is converted into a standard score by

  
    
      
        z
        =
        
          
            
              x
              −
              μ
            
            σ
          
        
      
    
    {\displaystyle z={x-\mu  \over \sigma }}
  

where:

μ is the mean of the population,
σ is the standard deviation of the population.
The absolute value of z represents the distance between that raw score x and the population mean in units of the standard deviation. z is negative when the raw score is below the mean, positive when above."
"Neural machine translation (NMT) is an approach to machine translation that uses an artificial neural network to predict the likelihood of a sequence of words, typically modeling entire sentences in a single integrated model.
It is the dominant approach today: 293 : 1  and can produce translations that rival human translations when translating between high-resource languages under specific conditions. However, there still remain challenges, especially with languages where less high-quality data is available,: 293  and with domain shift between the data a system was trained on and the texts it is supposed to translate.: 293  NMT systems also tend to produce fairly literal translations.


== Overview ==
In the translation task, a sentence 
  
    
      
        
          x
        
        =
        
          x
          
            1
            ,
            I
          
        
      
    
    {\displaystyle \mathbf {x} =x_{1,I}}
  
 (consisting of 
  
    
      
        I
      
    
    {\displaystyle I}
  
 tokens 
  
    
      
        
          x
          
            i
          
        
      
    
    {\displaystyle x_{i}}
  
) in the source language is to be translated into a sentence 
  
    
      
        
          y
        
        =
        
          x
          
            1
            ,
            J
          
        
      
    
    {\displaystyle \mathbf {y} =x_{1,J}}
  
 (consisting of 
  
    
      
        J
      
    
    {\displaystyle J}
  
 tokens 
  
    
      
        
          x
          
            j
          
        
      
    
    {\displaystyle x_{j}}
  
) in the target language. The source and target tokens (which in the simple event are used for each other in order for a particular game ] vectors, so they can be processed mathematically.
NMT models assign a probability 
  
    
      
        P
        (
        y
        
          |
        
        x
        )
      
    
    {\displaystyle P(y|x)}
  
: 5 : 1  to potential translations y and then search a subset of potential translations for the one with the highest probability. Most NMT models are auto-regressive: They model the probability of each target token as a function of the source sentence and the previously predicted target tokens. The probability of the whole translation then is the product of the probabilities of the individual predicted tokens:: 5 : 2 

  
    
      
        P
        (
        y
        
          |
        
        x
        )
        =
        
          ∏
          
            j
            =
            1
          
          
            J
          
        
        P
        (
        
          y
          
            j
          
        
        
          |
        
        
          y
          
            1
            ,
            i
            −
            1
          
        
        ,
        
          x
        
        )
      
    
    {\displaystyle P(y|x)=\prod _{j=1}^{J}P(y_{j}|y_{1,i-1},\mathbf {x} )}
  

NMT models differ in how exactly they model this function 
  
    
      
        P
      
    
    {\displaystyle P}
  
, but most use some variation of the encoder-decoder architecture:: 2 : 469  They first use an encoder network to process 
  
    
      
        
          x
        
      
    
    {\displaystyle \mathbf {x} }
  
 and encode it into a vector or matrix representation of the source sentence. Then they use a decoder network that usually produces one target word at a time, taking into account the source representation and the tokens it previously produced. As soon as the decoder produces a special end of sentence token, the decoding process is finished."
fastText is a library for learning of word embeddings and text classification created by Facebook's AI Research (FAIR) lab. The model allows one to create an unsupervised learning or supervised learning algorithm for obtaining vector representations for words. Facebook makes available pretrained models for 294 languages. Several papers describe the techniques used by fastText.
"In machine learning, supervised learning (SL) is a paradigm where a model is trained using input objects (e.g. a vector of predictor variables) and desired output values (also known as a supervisory signal), which are often human-made labels. The training process builds a function that maps new data to expected output values. An optimal scenario will allow for the algorithm to accurately determine output values for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a ""reasonable"" way (see inductive bias). This statistical quality of an algorithm is measured via a generalization error.


== Steps to follow ==
To solve a given problem of supervised learning, the following steps must be performed:

Determine the type of training samples. Before doing anything else, the user should decide what kind of data is to be used as a training set. In the case of handwriting analysis, for example, this might be a single handwritten character, an entire handwritten word, an entire sentence of handwriting, or a full paragraph of handwriting.
Gather a training set."
"spaCy ( spay-SEE) is an open-source software library for advanced natural language processing, written in the programming languages Python and Cython. The library is published under the MIT license and its main developers are Matthew Honnibal and Ines Montani, the founders of the software company Explosion.
Unlike NLTK, which is widely used for teaching and research, spaCy focuses on providing software for production usage. spaCy also supports deep learning workflows that allow connecting statistical models trained by popular machine learning libraries like TensorFlow, PyTorch or MXNet through its own machine learning library Thinc. Using Thinc as its backend, spaCy features convolutional neural network models for part-of-speech tagging, dependency parsing, text categorization and named entity recognition (NER). Prebuilt statistical neural network models to perform these tasks are available for 23 languages, including English, Portuguese, Spanish, Russian and Chinese, and there is also a multi-language NER model. Additional support for tokenization for more than 65 languages allows users to train custom models on their own datasets as well.


== History ==
Version 1.0 was released on October 19, 2016, and included preliminary support for deep learning workflows by supporting custom processing pipelines. It further included a rule matcher that supported entity annotations, and an officially documented training API.
Version 2.0 was released on November 7, 2017, and introduced convolutional neural network models for 7 different languages. It also supported custom processing pipeline components and extension attributes, and featured a built-in trainable text classification component."
"GE HealthCare Technologies, Inc., organized in Delaware and headquartered in Chicago, Illinois, focuses on health technology. The company operates 4 divisions: Medical imaging, which includes molecular imaging, computed tomography, magnetic resonance, women’s health screening and X-ray systems; Ultrasound; Patient Care Solutions, which is focused on remote patient monitoring, anesthesia and respiratory care, diagnostic cardiology, and infant care; and Pharmaceutical Diagnostics, which manufactures contrast agents and radiopharmaceuticals.
The company's primary customers are hospitals and health networks. In 2023, the company received 42% of its revenue in the United States and 13% of its revenue from China, where the company faces increasing competition.
The company operates in more than 100 countries. GE HealthCare has major regional operations in Buc (suburb of Paris), France; Helsinki, Finland; Kraków, Poland; Budapest, Hungary; Yizhuang (suburb of Beijing), China; Hino & Tokyo, Japan, and Bangalore, India. Its biggest R&D center is in Bangalore, India, built at a cost of $50 million.
In May 2022, General Electric formed the company to own its healthcare division; it completed the corporate spin-off of the company in January 2023.


== History ==


=== 19th century ===
The company traces its roots to the Victor Electric Company, founded in 1893 in a basement by Charles F. Samms and Julius B. Wantz, previously employees of the assembly lines at the Knapp Electrical Works and Midland Electric Co. and then in their early 20s."
"AdaBoost (short for Adaptive Boosting) is a statistical classification meta-algorithm formulated by Yoav Freund and Robert Schapire in 1995, who won the 2003 Gödel Prize for their work. It can be used in conjunction with many types of learning algorithm to improve performance. The output of multiple weak learners is combined into a weighted sum that represents the final output of the boosted classifier. Usually, AdaBoost is presented for binary classification, although it can be generalized to multiple classes or bounded intervals of real values.
AdaBoost is adaptive in the sense that subsequent weak learners (models) are adjusted in favor of instances misclassified by previous models. In some problems, it can be less susceptible to overfitting than other learning algorithms. The individual learners can be weak, but as long as the performance of each one is slightly better than random guessing, the final model can be proven to converge to a strong learner.
Although AdaBoost is typically used to combine weak base learners (such as decision stumps), it has been shown to also effectively combine strong base learners (such as deeper decision trees), producing an even more accurate model.
Every learning algorithm tends to suit some problem types better than others, and typically has many different parameters and configurations to adjust before it achieves optimal performance on a dataset. AdaBoost (with decision trees as the weak learners) is often referred to as the best out-of-the-box classifier."
"Vehicular automation is the use of technology to assist or replace the operator of a vehicle such as a car, truck, aircraft, rocket, military vehicle, or boat. Assisted vehicles are semi-autonomous, whereas vehicles that can travel without a human operator are autonomous. The degree of autonomy may be subject to various constraints such as conditions. Autonomy is enabled by advanced driver-assistance systems (ADAS) of varying capacity.
Related technology includes advanced software, maps, vehicle changes, and support outside the vehicle. 
Autonomy presents varying issues for road travel, air travel, and marine travel. Roads present the greatest complexity given the unpredictability of the driving environment, including diverse road designs, driving conditions, traffic, obstacles, and geographical/cultural differences.
Autonomy implies that the vehicle is responsible for all perception, monitoring, and control functions.


== SAE autonomy levels ==

The Society of Automotive Engineers (SAE) classifies road vehicle autonomy in six levels: 

0: No automation.
1: Driver assistance, the vehicle controls steering or speed autonomously in specific circumstances."
"Plotly is a technical computing company headquartered in Montreal, Quebec, that develops online data analytics and visualization tools. Plotly provides online graphing, analytics, and statistics tools for individuals and collaboration, as well as scientific graphing libraries for Python, R, MATLAB, Perl, Julia, Arduino, JavaScript and REST.


== History ==
Plotly was founded by Alex Johnson, Jack Parmer, Chris Parmer, and Matthew Sundquist.
The founders' backgrounds are in science, energy, and data analysis and visualization. Early employees include Christophe Viau, a Canadian software engineer and Ben Postlethwaite, a Canadian geophysicist. Plotly was named one of the Top 20 Hottest Innovative Companies in Canada by the Canadian Innovation Exchange. Plotly was featured in ""startup row"" at PyCon 2013, and sponsored the SciPy 2018 conference.
Plotly raised $5.5 million during its Series A funding, led by MHS Capital, Siemens Venture Capital, Rho Ventures, Real Ventures, and Silicon Valley Bank.
The Boston Globe and Washington Post newsrooms have produced data journalism using Plotly. In 2020, Plotly was named a Best Place to Work by the Canadian SME National Business Awards, and nominated as Business of the Year.


== Products ==
Plotly offers open-source and enterprise products."
"In statistics, the k-nearest neighbors algorithm (k-NN) is a non-parametric supervised learning method. It was first developed by Evelyn Fix and Joseph Hodges in 1951, and later expanded by Thomas Cover. 
Most often, it is used for classification, as a k-NN classifier, the output of which is a class membership. An object is classified by a plurality vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor.
The k-NN algorithm can also be generalized for regression. In k-NN regression, also known as nearest neighbor smoothing, the output is the property value for the object. This value is the average of the values of k nearest neighbors. If k = 1, then the output is simply assigned to the value of that single nearest neighbor, also known as nearest neighbor interpolation.
For both classification and regression, a useful technique can be to assign weights to the contributions of the neighbors, so that nearer neighbors contribute more to the average than distant ones."
"In signal processing, independent component analysis (ICA) is a computational method for separating a multivariate signal into additive subcomponents. This is done by assuming that at most one subcomponent is Gaussian and that the subcomponents are statistically independent from each other. ICA was invented by Jeanny Hérault and Christian Jutten in 1985. ICA is a special case of blind source separation. A common example application of ICA is the ""cocktail party problem"" of listening in on one person's speech in a noisy room.


== Introduction ==

Independent component analysis attempts to decompose a multivariate signal into independent non-Gaussian signals. As an example, sound is usually a signal that is composed of the numerical addition, at each time t, of signals from several sources. The question then is whether it is possible to separate these contributing sources from the observed total signal. When the statistical independence assumption is correct, blind ICA separation of a mixed signal gives very good results. It is also used for signals that are not supposed to be generated by mixing for analysis purposes."
"In statistics and machine learning, the bias–variance tradeoff describes the relationship between a model's complexity, the accuracy of its predictions, and how well it can make predictions on previously unseen data that were not used to train the model. In general, as we increase the number of tunable parameters in a model, it becomes more flexible, and can better fit a training data set. It is said to have lower error, or bias. However, for more flexible models, there will tend to be greater variance to the model fit each time we take a set of samples to create a new training data set. It is said that there is greater variance in the model's estimated parameters.
The bias–variance dilemma or bias–variance problem is the conflict in trying to simultaneously minimize these two sources of error that prevent supervised learning algorithms from generalizing beyond their training set:

The bias error is an error from erroneous assumptions in the learning algorithm. High bias can cause an algorithm to miss the relevant relations between features and target outputs (underfitting).
The variance is an error from sensitivity to small fluctuations in the training set. High variance may result from an algorithm modeling the random noise in the training data (overfitting).
The bias–variance decomposition is a way of analyzing a learning algorithm's expected generalization error with respect to a particular problem as a sum of three terms, the bias, variance, and a quantity called the irreducible error, resulting from noise in the problem itself."
"Neuro-symbolic AI is a type of artificial intelligence that integrates neural and symbolic AI architectures to address the weaknesses of each, providing a robust AI capable of reasoning, learning, and cognitive modeling. As argued by Leslie Valiant and others, the effective construction of rich computational cognitive models demands the combination of symbolic reasoning and efficient machine learning. Gary Marcus argued, ""We cannot construct rich cognitive models in an adequate, automated way without the triumvirate of hybrid architecture, rich prior knowledge, and sophisticated techniques for reasoning."" Further, ""To build a robust, knowledge-driven approach to AI we must have the machinery of symbol manipulation in our toolkit. Too much useful knowledge is abstract to proceed without tools that represent and manipulate abstraction, and to date, the only known machinery that can manipulate such abstract knowledge reliably is the apparatus of symbol manipulation.""
Henry Kautz, Francesca Rossi, and Bart Selman also argued for a synthesis. Their arguments attempt to address the two kinds of thinking, as discussed in Daniel Kahneman's book Thinking Fast and Slow. It describes cognition as encompassing two components: System 1 is fast, reflexive, intuitive, and unconscious. System 2 is slower, step-by-step, and explicit. System 1 is used for pattern recognition. System 2 handles planning, deduction, and deliberative thinking. In this view, deep learning best handles the first kind of cognition while symbolic reasoning best handles the second kind."
"Markov decision process (MDP), also called a stochastic dynamic program or stochastic control problem, is a model for sequential decision making when outcomes are uncertain.
Originating from operations research in the 1950s, MDPs have since gained recognition in a variety of fields, including ecology, economics, healthcare, telecommunications and reinforcement learning. Reinforcement learning utilizes the MDP framework to model the interaction between a learning agent and its environment. In this framework, the interaction is characterized by states, actions, and rewards. The MDP framework is designed to provide a simplified representation of key elements of artificial intelligence challenges. These elements encompass the understanding of cause and effect, the management of uncertainty and nondeterminism, and the pursuit of explicit goals.
The name comes from its connection to Markov chains, a concept developed by the Russian mathematician Andrey Markov. The ""Markov"" in ""Markov decision process"" refers to the underlying structure of state transitions that still follow the Markov property. The process is called a ""decision process"" because it involves making decisions that influence these state transitions, extending the concept of a Markov chain into the realm of decision-making under uncertainty.


== Definition ==

A Markov decision process is a 4-tuple 
  
    
      
        (
        S
        ,
        A
        ,
        
          P
          
            a
          
        
        ,
        
          R
          
            a
          
        
        )
      
    
    {\displaystyle (S,A,P_{a},R_{a})}
  
, where:

  
    
      
        S
      
    
    {\displaystyle S}
  
 is a set of states called the state space."
"OpenCL (Open Computing Language) is a framework for writing programs that execute across heterogeneous platforms consisting of central processing units (CPUs), graphics processing units (GPUs), digital signal processors (DSPs), field-programmable gate arrays (FPGAs) and other processors or hardware accelerators. OpenCL specifies a programming language (based on C99) for programming these devices and application programming interfaces (APIs) to control the platform and execute programs on the compute devices. OpenCL provides a standard interface for parallel computing using task- and data-based parallelism.
OpenCL is an open standard maintained by the Khronos Group, a non-profit, open standards organisation. Conformant implementations (passed the Conformance Test Suite) are available from a range of companies including AMD, ARM, Cadence, Google, Imagination, Intel, Nvidia, Qualcomm, Samsung, SPI and Verisilicon.


== Overview ==
OpenCL views a computing system as consisting of a number of compute devices, which might be central processing units (CPUs) or ""accelerators"" such as graphics processing units (GPUs), attached to a host processor (a CPU). It defines a C-like language for writing programs. Functions executed on an OpenCL device are called ""kernels"".: 17  A single compute device typically consists of several compute units, which in turn comprise multiple processing elements (PEs). A single kernel execution can run on all or many of the PEs in parallel. How a compute device is subdivided into compute units and PEs is up to the vendor; a compute unit can be thought of as a ""core"", but the notion of core is hard to define across all the types of devices supported by OpenCL (or even within the category of ""CPUs""),: 49–50  and the number of compute units may not correspond to the number of cores claimed in vendors' marketing literature (which may actually be counting SIMD lanes)."
"In computational intelligence (CI), an evolutionary algorithm (EA) is a subset of evolutionary computation, a generic population-based metaheuristic optimization algorithm. An EA uses mechanisms inspired by biological evolution, such as reproduction, mutation, recombination, and selection. Candidate solutions to the optimization problem play the role of individuals in a population, and the fitness function determines the quality of the solutions (see also loss function). Evolution of the population then takes place after the repeated application of the above operators.
Evolutionary algorithms often perform well approximating solutions to all types of problems because they ideally do not make any assumption about the underlying fitness landscape. Techniques from evolutionary algorithms applied to the modeling of biological evolution are generally limited to explorations of microevolutionary processes and planning models based upon cellular processes. In most real applications of EAs, computational complexity is a prohibiting factor. In fact, this computational complexity is due to fitness function evaluation. Fitness approximation is one of the solutions to overcome this difficulty. However, seemingly simple EA can solve often complex problems; therefore, there may be no direct link between algorithm complexity and problem complexity."
"In corpus linguistics, part-of-speech tagging (POS tagging or PoS tagging or POST), also called grammatical tagging is the process of marking up a word in a text (corpus) as corresponding to a particular part of speech, based on both its definition and its context.
A simplified form of this is commonly taught to school-age children, in the identification of words as nouns, verbs, adjectives, adverbs, etc.
Once performed by hand, POS tagging is now done in the context of computational linguistics, using algorithms which associate discrete terms, as well as hidden parts of speech, by a set of descriptive tags. POS-tagging algorithms fall into two distinctive groups: rule-based and stochastic. E. Brill's tagger, one of the first and most widely used English POS-taggers, employs rule-based algorithms.


== Principle ==

Part-of-speech tagging is harder than just having a list of words and their parts of speech, because some words can represent more than one part of speech at different times, and because some parts of speech are complex. This is not rare—in natural languages (as opposed to many artificial languages), a large percentage of word-forms are ambiguous. For example, even ""dogs"", which is usually thought of as just a plural noun, can also be a verb:

The sailor dogs the hatch.
Correct grammatical tagging will reflect that ""dogs"" is here used as a verb, not as the more common plural noun. Grammatical context is one way to determine this; semantic analysis can also be used to infer that ""sailor"" and ""hatch"" implicate ""dogs"" as 1) in the nautical context and 2) an action applied to the object ""hatch"" (in this context, ""dogs"" is a nautical term meaning ""fastens (a watertight door) securely"")."
"IMDb (an initialism for Internet Movie Database) is an online database of information related to films, television series, podcasts, home videos, video games, and streaming content online – including cast, production crew and personal biographies, plot summaries, trivia, ratings, and fan and critical reviews. IMDb began as a fan-operated movie database on the Usenet group ""rec.arts.movies"" in 1990, and moved to the Web in 1993. Since 1998, it has been owned and operated by IMDb.com, Inc., a subsidiary of Amazon.
The site's message boards were disabled in February 2017.
As of 2019, IMDb was the 52nd most visited website on the Internet, as ranked by Alexa. As of March 2022, the database contained some 10.1 million titles (including television episodes), 11.5 million person records, and 83 million registered users.


== Features ==

The title and talent pages of IMDb are accessible to all users, but only registered and logged-in users can submit new material and suggest edits to existing entries. Most of the site's data has been provided by these volunteers. Registered users with a proven track record are able to add and make corrections to cast lists, credits, and some other data points. However, the addition and removal of images, and alterations to titles, cast and crew names, character names, and plot summaries are subject to an approval process; this usually takes between 24 and 72 hours."
"An altar is a table or platform for the presentation of religious offerings, for sacrifices, or for other ritualistic purposes. Altars are found at shrines, temples, churches, and other places of worship. They are used particularly in paganism, Christianity, Buddhism, Hinduism, Judaism, modern paganism, and in certain Islamic communities around Caucasia and Asia Minor. Many historical-medieval faiths also made use of them, including the Roman, Greek, and Norse religions.


== Etymology ==
The modern English word altar was derived from Middle English altar, from Old English alter, taken from Latin altare (""altar""), probably related to adolere (""burn""); thus ""burning place"", influenced by altus (""high""). It displaced the native Old English word wēofod.


== Altars in antiquity ==

	Altars in antiquity
		
			
			
		
		
			
			
		
		
			
			
		
		
			
			
		


== Judaism ==

Altars  in the Hebrew Bible were typically made of earth or unwrought stone. Altars were generally erected in conspicuous places. The first altar recorded in the Hebrew Bible is that erected by Noah. Altars were erected by Abraham, by Isaac, by Jacob, and by Moses."
"In machine learning, support vector machines (SVMs, also support vector networks) are supervised max-margin models with associated learning algorithms that analyze data for classification and regression analysis. Developed at AT&T Bell Laboratories, SVMs are one of the most studied models, being based on statistical learning frameworks of VC theory proposed by Vapnik (1982, 1995) and Chervonenkis (1974).
In addition to performing linear classification, SVMs can efficiently perform non-linear classification using the kernel trick, representing the data only through a set of pairwise similarity comparisons between the original data points using a kernel function, which transforms them into coordinates in a higher-dimensional feature space. Thus, SVMs use the kernel trick to implicitly map their inputs into high-dimensional feature spaces, where linear classification can be performed.  Being max-margin models, SVMs are resilient to noisy data (e.g., misclassified examples). SVMs can also be used for regression tasks, where the objective becomes 
  
    
      
        ϵ
      
    
    {\displaystyle \epsilon }
  
-sensitive.
The support vector clustering algorithm, created by Hava Siegelmann and Vladimir Vapnik, applies the statistics of support vectors, developed in the support vector machines algorithm, to categorize unlabeled data. These data sets require unsupervised learning approaches, which attempt to find natural clustering of the data into groups, and then to map new data according to these clusters.
The popularity of SVMs is likely due to their amenability to theoretical analysis, and their flexibility in being applied to a wide variety of tasks, including structured prediction problems. It is not clear that SVMs have better predictive performance than other linear models, such as logistic regression and linear regression."
"In religion, a prophet or prophetess is an individual who is regarded as being in contact with a divine being and is said to speak on behalf of that being, serving as an intermediary with humanity by delivering messages or teachings from the supernatural source to other people. The message that the prophet conveys is called a prophecy.
Prophethood has existed in many cultures and religions throughout history, including Mesopotamian religion, Zoroastrianism, Judaism, Christianity, Manichaeism, Islam, the Baháʼí Faith, and Thelema.


== Etymology ==
The English word prophet is the transliteration of a compound Greek word derived from pro (before/toward) and phesein (to tell); thus, a προφήτης (prophḗtēs) is someone who conveys messages from the divine to humans, including occasionally foretelling future events. In a different interpretation, it means advocate or speaker.  It is used to translate the Hebrew word נָבִיא (nāvî) in the Septuagint and the Arabic word نبي (nabī). W.F. Albright points to the Akkadian Nabu for the origin of these Hebrew (נָבִיא (nāvî) and the Arabic نبي (nabī) words.
The Akkadian nabû means ""announcer"" or ""authorised person"", derived from the Semitic root n-b-y or nbʾ. It is cognate with Classical Syriac: ܢܒܝܐ, romanized: nəḇiyyā, Arabic: نبي, romanized: nabiyy, and Hebrew: נביא, romanized: nāḇi, all meaning 'prophet'.
In Hebrew, the word נָבִיא (nāvî), ""spokesperson"", traditionally translates as ""prophet""."
"In machine learning, backpropagation is a gradient estimation method commonly used for training a neural network to compute its parameter updates.
It is an efficient application of the chain rule to neural networks. Backpropagation computes the gradient of a loss function with respect to the weights of the network for a single input–output example, and does so efficiently, computing the gradient one layer at a time, iterating backward from the last layer to avoid redundant calculations of intermediate terms in the chain rule; this can be derived through dynamic programming.
Strictly speaking, the term backpropagation refers only to an algorithm for efficiently computing the gradient, not how the gradient is used; but the term is often used loosely to refer to the entire learning algorithm – including how the gradient is used, such as by stochastic gradient descent, or as an intermediate step in a more complicated optimizer, such as Adam. 
Backpropagation had multiple discoveries and partial discoveries, with a tangled history and terminology. See the history section for details. Some other names for the technique include ""reverse mode of automatic differentiation"" or ""reverse accumulation"". 


== Overview ==
Backpropagation computes the gradient in weight space of a feedforward neural network, with respect to a loss function. Denote:

  
    
      
        x
      
    
    {\displaystyle x}
  
: input (vector of features)

  
    
      
        y
      
    
    {\displaystyle y}
  
: target output
For classification, output will be a vector of class probabilities (e.g., 
  
    
      
        (
        0.1
        ,
        0.7
        ,
        0.2
        )
      
    
    {\displaystyle (0.1,0.7,0.2)}
  
, and target output is a specific class, encoded by the one-hot/dummy variable (e.g., 
  
    
      
        (
        0
        ,
        1
        ,
        0
        )
      
    
    {\displaystyle (0,1,0)}
  
).

  
    
      
        C
      
    
    {\displaystyle C}
  
: loss function or ""cost function""
For classification, this is usually cross-entropy (XC, log loss), while for regression it is usually squared error loss (SEL)."
"Quantum machine learning is the integration of quantum algorithms within machine learning programs.
The most common use of the term refers to machine learning algorithms for the analysis of classical data executed on a quantum computer, i.e. quantum-enhanced machine learning. While machine learning algorithms are used to compute immense quantities of data, quantum machine learning utilizes qubits and quantum operations or specialized quantum systems to improve computational speed and data storage done by algorithms in a program. This includes hybrid methods that involve both classical and quantum processing, where computationally difficult subroutines are outsourced to a quantum device. These routines can be more complex in nature and executed faster on a quantum computer. Furthermore, quantum algorithms can be used to analyze quantum states instead of classical data.
Beyond quantum computing, the term ""quantum machine learning"" is also associated with classical machine learning methods applied to data generated from quantum experiments (i.e. machine learning of quantum systems), such as learning the phase transitions of a quantum system or creating new quantum experiments.
Quantum machine learning also extends to a branch of research that explores methodological and structural similarities between certain physical systems and learning systems, in particular neural networks."
"In mathematics, a time series is a series of data points indexed (or listed or graphed) in time order.  Most commonly, a time series is a sequence taken at successive equally spaced points in time. Thus it is a sequence of discrete-time data. Examples of time series are heights of ocean tides, counts of sunspots, and the daily closing value of the Dow Jones Industrial Average.
A time series is very frequently plotted via a run chart (which is a temporal line chart). Time series are used in statistics, signal processing, pattern recognition, econometrics, mathematical finance, weather forecasting, earthquake prediction, electroencephalography, control engineering, astronomy, communications engineering, and largely in any domain of applied science and engineering which involves temporal measurements.
Time series analysis comprises methods for analyzing time series data in order to extract meaningful statistics and other characteristics of the data. Time series forecasting is the use of a model to predict future values based on previously observed values. Generally, time series data is modelled as a stochastic process. While regression analysis is often employed in such a way as to test relationships between one or more different time series, this type of analysis is not usually called ""time series analysis"", which refers in particular to relationships between different points in time within a single series."
"Neural machine translation (NMT) is an approach to machine translation that uses an artificial neural network to predict the likelihood of a sequence of words, typically modeling entire sentences in a single integrated model.
It is the dominant approach today: 293 : 1  and can produce translations that rival human translations when translating between high-resource languages under specific conditions. However, there still remain challenges, especially with languages where less high-quality data is available,: 293  and with domain shift between the data a system was trained on and the texts it is supposed to translate.: 293  NMT systems also tend to produce fairly literal translations.


== Overview ==
In the translation task, a sentence 
  
    
      
        
          x
        
        =
        
          x
          
            1
            ,
            I
          
        
      
    
    {\displaystyle \mathbf {x} =x_{1,I}}
  
 (consisting of 
  
    
      
        I
      
    
    {\displaystyle I}
  
 tokens 
  
    
      
        
          x
          
            i
          
        
      
    
    {\displaystyle x_{i}}
  
) in the source language is to be translated into a sentence 
  
    
      
        
          y
        
        =
        
          x
          
            1
            ,
            J
          
        
      
    
    {\displaystyle \mathbf {y} =x_{1,J}}
  
 (consisting of 
  
    
      
        J
      
    
    {\displaystyle J}
  
 tokens 
  
    
      
        
          x
          
            j
          
        
      
    
    {\displaystyle x_{j}}
  
) in the target language. The source and target tokens (which in the simple event are used for each other in order for a particular game ] vectors, so they can be processed mathematically.
NMT models assign a probability 
  
    
      
        P
        (
        y
        
          |
        
        x
        )
      
    
    {\displaystyle P(y|x)}
  
: 5 : 1  to potential translations y and then search a subset of potential translations for the one with the highest probability. Most NMT models are auto-regressive: They model the probability of each target token as a function of the source sentence and the previously predicted target tokens. The probability of the whole translation then is the product of the probabilities of the individual predicted tokens:: 5 : 2 

  
    
      
        P
        (
        y
        
          |
        
        x
        )
        =
        
          ∏
          
            j
            =
            1
          
          
            J
          
        
        P
        (
        
          y
          
            j
          
        
        
          |
        
        
          y
          
            1
            ,
            i
            −
            1
          
        
        ,
        
          x
        
        )
      
    
    {\displaystyle P(y|x)=\prod _{j=1}^{J}P(y_{j}|y_{1,i-1},\mathbf {x} )}
  

NMT models differ in how exactly they model this function 
  
    
      
        P
      
    
    {\displaystyle P}
  
, but most use some variation of the encoder-decoder architecture:: 2 : 469  They first use an encoder network to process 
  
    
      
        
          x
        
      
    
    {\displaystyle \mathbf {x} }
  
 and encode it into a vector or matrix representation of the source sentence. Then they use a decoder network that usually produces one target word at a time, taking into account the source representation and the tokens it previously produced. As soon as the decoder produces a special end of sentence token, the decoding process is finished."
"Feature engineering is a preprocessing step in supervised machine learning and statistical modeling which transforms raw data into a more effective set of inputs. Each input comprises several attributes, known as features. By providing models with relevant information, feature engineering significantly enhances their predictive accuracy and decision-making capability. 
Beyond machine learning, the principles of feature engineering are applied in various scientific fields, including physics. For example, physicists construct dimensionless numbers such as the Reynolds number in fluid dynamics, the Nusselt number in heat transfer, and the Archimedes number in sedimentation. They also develop first approximations of solutions, such as analytical solutions for the strength of materials in mechanics.


== Clustering ==
One of the applications of feature engineering has been clustering of feature-objects or sample-objects in a dataset. Especially, feature engineering based on matrix decomposition has been extensively used for data clustering under non-negativity constraints on the feature coefficients. These include Non-Negative Matrix Factorization (NMF), Non-Negative Matrix-Tri Factorization (NMTF), Non-Negative Tensor Decomposition/Factorization (NTF/NTD), etc. The non-negativity constraints on coefficients of the feature vectors mined by the above-stated algorithms yields a part-based representation, and different factor matrices exhibit natural clustering properties."
"In data analysis, anomaly detection (also referred to as outlier detection and sometimes as novelty detection) is generally understood to be the identification of rare items, events or observations which deviate significantly from the majority of the data and do not conform to a well defined notion of normal behavior. Such examples may arouse suspicions of being generated by a different mechanism, or appear inconsistent with the remainder of that set of data.
Anomaly detection finds application in many domains including cybersecurity, medicine, machine vision, statistics, neuroscience, law enforcement and financial fraud to name only a few. Anomalies were initially searched for clear rejection or omission from the data to aid statistical analysis, for example to compute the mean or standard deviation. They were also removed to better predictions from models such as linear regression, and more recently their removal aids the performance of machine learning algorithms. However, in many applications anomalies themselves are of interest and are the observations most desirous in the entire data set, which need to be identified and separated from noise or irrelevant outliers.
Three broad categories of anomaly detection techniques exist. Supervised anomaly detection techniques require a data set that has been labeled as ""normal"" and ""abnormal"" and involves training a classifier. However, this approach is rarely used in anomaly detection due to the general unavailability of labelled data and the inherent unbalanced nature of the classes. Semi-supervised anomaly detection techniques assume that some portion of the data is labelled."
"Batch normalization (also known as batch norm) is a method used to make training of artificial neural networks faster and more stable through normalization of the layers' inputs by re-centering and re-scaling. It was proposed by Sergey Ioffe and Christian Szegedy in 2015.
While the effect of batch normalization is evident, the reasons behind its effectiveness remain under discussion. It was believed that it can mitigate the problem of internal covariate shift, where parameter initialization and changes in the distribution of the inputs of each layer affect the learning rate of the network. Recently, some scholars have argued that batch normalization does not reduce internal covariate shift, but rather smooths the objective function, which in turn improves the performance. However, at initialization, batch normalization in fact induces severe gradient explosion in deep networks, which is only alleviated by skip connections in residual networks. Others maintain that batch normalization achieves length-direction decoupling, and thereby accelerates neural networks. 


== Internal covariate shift ==
Each layer of a neural network has inputs with a corresponding distribution, which is affected during the training process by the randomness in the parameter initialization and the randomness in the input data. The effect of these sources of randomness on the distribution of the inputs to internal layers during training is described as internal covariate shift. Although a clear-cut precise definition seems to be missing, the phenomenon observed in experiments is the change on means and variances of the inputs to internal layers during training."
"Random forests or random decision forests is an ensemble learning method for classification, regression and other tasks that works by creating a multitude of decision trees during training. For classification tasks, the output of the random forest is the class selected by most trees. For regression tasks, the output is the average of the predictions of the trees. Random forests correct for decision trees' habit of overfitting to their training set.: 587–588 
The first algorithm for random decision forests was created in 1995 by Tin Kam Ho using the random subspace method, which, in Ho's formulation, is a way to implement the ""stochastic discrimination"" approach to classification proposed by Eugene Kleinberg.
An extension of the algorithm was developed by Leo Breiman and Adele Cutler, who registered ""Random Forests"" as a trademark in 2006 (as of 2019, owned by Minitab, Inc.). The extension combines Breiman's ""bagging"" idea and random selection of features, introduced first by Ho and later independently by Amit and Geman in order to construct a collection of decision trees with controlled variance.


== History ==
The general method of random decision forests was first proposed by Salzberg and Heath in 1993, with a method that used a randomized decision tree algorithm to create multiple trees and then combine them using majority voting.  This idea was developed further by Ho in 1995.  Ho established that forests of trees splitting with oblique hyperplanes can gain accuracy as they grow without suffering from overtraining, as long as the forests are randomly restricted to be sensitive to only selected feature dimensions.  A subsequent work along the same lines concluded that other splitting methods behave similarly, as long as they are randomly forced to be insensitive to some feature dimensions."
"Multi-agent reinforcement learning (MARL) is a sub-field of reinforcement learning. It focuses on studying the behavior of multiple learning agents that coexist in a shared environment. Each agent is motivated by its own rewards, and does actions to advance its own interests; in some environments these interests are opposed to the interests of other agents, resulting in complex group dynamics.
Multi-agent reinforcement learning is closely related to game theory and especially repeated games, as well as multi-agent systems. Its study combines the pursuit of finding ideal algorithms that maximize rewards with a more sociological set of concepts. While research in single-agent reinforcement learning is concerned with finding the algorithm that gets the biggest number of points for one agent, research in multi-agent reinforcement learning evaluates and quantifies social metrics, such as cooperation, reciprocity, equity, social influence, language and discrimination.


== Definition ==
Similarly to single-agent reinforcement learning, multi-agent reinforcement learning is modeled as some form of a Markov decision process (MDP). For example, 

A set 
  
    
      
        S
      
    
    {\displaystyle S}
  
 of environment states.
One set 
  
    
      
        
          
            
              A
            
          
          
            i
          
        
      
    
    {\displaystyle {\mathcal {A}}_{i}}
  
 of actions for each of the agents 
  
    
      
        i
        ∈
        I
        =
        {
        1
        ,
        .
        ."
"Speech recognition is an interdisciplinary subfield of computer science and computational linguistics that develops methodologies and technologies that enable the recognition and translation of spoken language into text by computers. It is also known as automatic speech recognition (ASR), computer speech recognition or speech-to-text (STT). It incorporates knowledge and research in the computer science, linguistics and computer engineering fields. The reverse process is speech synthesis.
Some speech recognition systems require ""training"" (also called ""enrollment"") where an individual speaker reads text or isolated vocabulary into the system. The system analyzes the person's specific voice and uses it to fine-tune the recognition of that person's speech, resulting in increased accuracy. Systems that do not use training are called ""speaker-independent"" systems. Systems that use training are called ""speaker dependent"".
Speech recognition applications include voice user interfaces such as voice dialing (e.g. ""call home""), call routing (e.g."
"fast.ai is a non-profit research group focused on deep learning and artificial intelligence. It was founded in 2016 by Jeremy Howard and Rachel Thomas with the goal of democratizing deep learning. They do this by providing a massive open online course (MOOC) named ""Practical Deep Learning for Coders,"" which has no other prerequisites except for knowledge of the programming language Python.


== Massive Open Online Course ==
The free MOOC ""Practical Deep Learning for Coders"" is available as recorded videos, initially taught by Howard and Thomas at the University of San Francisco. In contrast to other online learning platforms such as Coursera or Udemy, a certificate is not granted to those successfully finishing the course online. Only the students following the in-person classes can obtain a certificate from the University of San Francisco. 
The MOOC consists of two parts, each containing seven lessons. Topics include image classification, stochastic gradient descent, natural language processing (NLP), and various deep learning architectures such as convolutional neural networks (CNNs), recursive neural networks (RNNs) and generative adversarial networks (GANs).


== Applications and alumni ==
In 2018, students of fast.ai participated in the Stanford’s DAWNBench challenge alongside big tech companies such as Google and Intel. While Google could obtain an edge in some challenges due to its highly specialized TPU chips, the CIFAR-10 challenge was won by the fast.ai students, programming the fastest and cheapest algorithms."
"Financial modeling is the task of building an abstract representation (a model) of a real world financial situation. This is a mathematical model designed to represent (a simplified version of) the performance of a financial asset or portfolio of a business, project, or any other investment.
Typically, then, financial modeling is understood to mean an exercise in either asset pricing or corporate finance, of a quantitative nature. It is about translating a set of hypotheses about the behavior of markets or agents into numerical predictions.  At the same time, ""financial modeling"" is a general term that means different things to different users; the reference usually relates either to accounting and corporate finance applications or to quantitative finance applications.


== Accounting ==

In corporate finance and the accounting profession, financial modeling typically entails financial statement forecasting; usually the preparation of detailed company-specific models used for  decision making purposes, valuation and financial analysis.
Applications include: 

Business valuation and stock valuation - especially via discounted cash flow, but including other valuation approaches
Scenario planning and management decision making (""what is""; ""what if""; ""what has to be done"")
Budgeting:  revenue forecasting and analytics; production budgeting; operations budgeting
Capital budgeting, including cost of capital (i.e. WACC) calculations
Cash flow forecasting; working capital- and treasury management; asset and liability management
Financial statement analysis / ratio analysis (including of operating- and finance leases, and R&D)
Transaction analytics: M&A, PE, VC, LBO, IPO, Project finance, P3
Credit decisioning: Credit analysis, Consumer credit risk; impairment- and provision-modeling
Management accounting: Activity-based costing, Profitability analysis, Cost analysis, Whole-life cost, Managerial risk accounting
Public sector procurement
To generalize  as to the nature of these models: 
firstly, as they are built around financial statements, calculations and outputs are monthly, quarterly or annual; 
secondly, the inputs take the form of ""assumptions"", where the analyst specifies the values that will apply in each period for external / global variables (exchange rates, tax percentage, etc....; may be thought of as the model parameters), and for internal / company specific variables (wages, unit costs, etc....). Correspondingly, both characteristics are reflected (at least implicitly) in the mathematical form of these models: 
firstly, the models are in discrete time; 
secondly, they are deterministic.
For discussion of the issues that may arise, see below; for discussion as to more sophisticated approaches sometimes employed, see Corporate finance § Quantifying uncertainty and Financial economics § Corporate finance theory."
"Multi-task learning (MTL) is a subfield of machine learning in which multiple learning tasks are solved at the same time, while exploiting commonalities and differences across tasks. This can result in improved learning efficiency and prediction accuracy for the task-specific models, when compared to training the models separately. 
Inherently, Multi-task learning is a multi-objective optimization problem having trade-offs between different tasks.
Early versions of MTL were called ""hints"".

In a widely cited 1997 paper, Rich Caruana gave the following characterization:Multitask Learning is an approach to inductive transfer that improves generalization by using the domain information contained in the training signals of related tasks as an inductive bias. It does this by learning tasks in parallel while using a shared representation; what is learned for each task can help other tasks be learned better.
In the classification context, MTL aims to improve the performance of multiple classification tasks by learning them jointly. One example is a spam-filter, which can be treated as distinct but related classification tasks across different users. To make this more concrete, consider that different people have different distributions of features which distinguish spam emails from legitimate ones, for example an English speaker may find that all emails in Russian are spam, not so for Russian speakers. Yet there is a definite commonality in this classification task across users, for example one common feature might be text related to money transfer."
"A self-organizing map (SOM) or self-organizing feature map (SOFM) is an unsupervised machine learning technique used to produce a low-dimensional (typically two-dimensional) representation of a higher-dimensional data set while preserving the topological structure of the data. For example, a data set with 
  
    
      
        p
      
    
    {\displaystyle p}
  
 variables measured in 
  
    
      
        n
      
    
    {\displaystyle n}
  
 observations could be represented as clusters of observations with similar values for the variables. These clusters then could be visualized as a two-dimensional ""map"" such that observations in proximal clusters have more similar values than observations in distal clusters. This can make high-dimensional data easier to visualize and analyze.
An SOM is a type of artificial neural network but is trained using competitive learning rather than the error-correction learning (e.g., backpropagation with gradient descent) used by other artificial neural networks. The SOM was introduced by the Finnish professor Teuvo Kohonen in the 1980s and therefore is sometimes called a Kohonen map or Kohonen network. The Kohonen map or network is a computationally convenient abstraction building on biological models of neural systems from the 1970s and morphogenesis models dating back to Alan Turing in the 1950s.
SOMs create internal representations reminiscent of the cortical homunculus, a distorted representation of the human body, based on a neurological ""map"" of the areas and proportions of the human brain dedicated to processing sensory functions, for different parts of the body.


== Overview ==
Self-organizing maps, like most artificial neural networks, operate in two modes: training and mapping. First, training uses an input data set (the ""input space"") to generate a lower-dimensional representation of the input data (the ""map space"")."
"In natural language processing, latent Dirichlet allocation (LDA) is a Bayesian network (and, therefore, a generative statistical model) for modeling automatically extracted topics in textual corpora. The LDA is an example of a Bayesian topic model. In this, observations (e.g., words) are collected into documents, and each word's presence is attributable to one of the document's topics. Each document will contain a small number of topics.


== History ==
In the context of population genetics, LDA was proposed by J. K. Pritchard, M. Stephens and P. Donnelly in 2000.
LDA was applied in machine learning by David Blei, Andrew Ng and Michael I. Jordan in 2003.


== Overview ==


=== Evolutionary biology and bio-medicine ===
In evolutionary biology and bio-medicine, the model is used to detect the presence of structured genetic variation in a group of individuals. The model assumes that alleles carried by individuals under study have origin in various extant or past populations. The model and various inference algorithms allow scientists to estimate the allele frequencies in those source populations and the origin of alleles carried by individuals under study. The source populations can be interpreted ex-post in terms of various evolutionary scenarios."
"In information retrieval, tf–idf (also TF*IDF, TFIDF, TF–IDF, or Tf–idf), short for term frequency–inverse document frequency,  is a measure of importance of a word to a document in a collection or corpus, adjusted for the fact that some words appear more frequently in general. Like the bag-of-words model, it models a document as a multiset of words, without word order. It is a refinement over the simple bag-of-words model, by allowing the weight of words to depend on the rest of the corpus.
It was often used as a weighting factor in searches of information retrieval, text mining, and user modeling. A survey conducted in 2015 showed that 83% of text-based recommender systems in digital libraries used tf–idf. Variations of the tf–idf weighting scheme were often used by search engines as a central tool in scoring and ranking a document's relevance given a user query.
One of the simplest ranking functions is computed by summing the tf–idf for each query term; many more sophisticated ranking functions are variants of this simple model.


== Motivations ==
Karen Spärck Jones (1972) conceived a statistical interpretation of term-specificity called Inverse Document Frequency (idf), which became a cornerstone of term weighting:

The specificity of a term can be quantified as an inverse function of the number of documents in which it occurs.For example, the df (document frequency) and idf for some words in Shakespeare's 37 plays are as follows:

We see that ""Romeo"", ""Falstaff"", and ""salad"" appears in very few plays, so seeing these words, one could get a good idea as to which play it might be. In contrast, ""good"" and ""sweet"" appears in every play and are completely uninformative as to which play it is.


== Definition ==
The tf–idf is the product of two statistics, term frequency and inverse document frequency."
"In the context of artificial neural networks, the rectifier or ReLU (rectified linear unit) activation function is an activation function defined as the non-negative part of its argument, i.e., the ramp function:

  
    
      
        ReLU
        ⁡
        (
        x
        )
        =
        
          x
          
            +
          
        
        =
        max
        (
        0
        ,
        x
        )
        =
        
          
            
              x
              +
              
                |
              
              x
              
                |
              
            
            2
          
        
        =
        
          
            {
            
              
                
                  x
                
                
                  
                    if 
                  
                  x
                  >
                  0
                  ,
                
              
              
                
                  0
                
                
                  x
                  ≤
                  0
                
              
            
            
          
        
      
    
    {\displaystyle \operatorname {ReLU} (x)=x^{+}=\max(0,x)={\frac {x+|x|}{2}}={\begin{cases}x&{\text{if }}x>0,\\0&x\leq 0\end{cases}}}
  

where 
  
    
      
        x
      
    
    {\displaystyle x}
  
 is the input to a neuron. This is analogous to half-wave rectification in electrical engineering.
ReLU is one of the most popular activation function for artificial neural networks, and finds application in computer vision and speech recognition using deep neural nets and computational neuroscience.
It was first used by Alston Householder in 1941 as a mathematical abstraction of biological neural networks. It was introduced by Kunihiko Fukushima in 1969 in the context of visual feature extraction in hierarchical neural networks. It was later argued that it has strong biological motivations and mathematical justifications. In 2011, ReLU activation enabled training deep supervised neural networks without unsupervised pre-training, compared to the widely used activation functions prior to 2011, e.g., the logistic sigmoid (which is inspired by probability theory; see logistic regression) and its more practical counterpart, the hyperbolic tangent. 


== Advantages ==
Advantages of ReLU include:

Sparse activation: for example, in a randomly initialized network, only about 50% of hidden units are activated (i.e. have a non-zero output).
Better gradient propagation: fewer vanishing gradient problems compared to sigmoidal activation functions that saturate in both directions."
"t-distributed stochastic neighbor embedding (t-SNE) is a statistical method for visualizing high-dimensional data by giving each datapoint a location in a two or three-dimensional map. It is based on Stochastic Neighbor Embedding originally developed by Geoffrey Hinton and Sam Roweis, where Laurens van der Maaten and Hinton proposed the t-distributed variant. It is a nonlinear dimensionality reduction technique for embedding high-dimensional data for visualization in a low-dimensional space of two or three dimensions. Specifically, it models each high-dimensional object by a two- or three-dimensional point in such a way that similar objects are modeled by nearby points and dissimilar objects are modeled by distant points with high probability.
The t-SNE algorithm comprises two main stages. First, t-SNE constructs a probability distribution over pairs of high-dimensional objects in such a way that similar objects are assigned a higher probability while dissimilar points are assigned a lower probability. Second, t-SNE defines a similar probability distribution over the points in the low-dimensional map, and it minimizes the Kullback–Leibler divergence (KL divergence) between the two distributions with respect to the locations of the points in the map. While the original algorithm uses the Euclidean distance between objects as the base of its similarity metric, this can be changed as appropriate. A Riemannian variant is UMAP.
t-SNE has been used for visualization in a wide range of applications, including genomics, computer security research, natural language processing, music analysis, cancer research, bioinformatics, geological domain interpretation, and biomedical signal processing.
For a data set with n elements, t-SNE runs in O(n2) time and requires O(n2) space."
"Stochastic gradient descent (often abbreviated SGD) is an iterative method for optimizing an objective function with suitable smoothness properties (e.g. differentiable or subdifferentiable). It can be regarded as a stochastic approximation of gradient descent optimization, since it replaces the actual gradient (calculated from the entire data set) by an estimate thereof (calculated from a randomly selected subset of the data). Especially in high-dimensional optimization problems this reduces the very high computational burden, achieving faster iterations in exchange for a lower convergence rate.
The basic idea behind stochastic approximation can be traced back to the Robbins–Monro algorithm of the 1950s. Today, stochastic gradient descent has become an important optimization method in machine learning.


== Background ==

Both statistical estimation and machine learning consider the problem of minimizing an objective function that has the form of a sum:

  
    
      
        Q
        (
        w
        )
        =
        
          
            1
            n
          
        
        
          ∑
          
            i
            =
            1
          
          
            n
          
        
        
          Q
          
            i
          
        
        (
        w
        )
        ,
      
    
    {\displaystyle Q(w)={\frac {1}{n}}\sum _{i=1}^{n}Q_{i}(w),}
  

where the parameter 
  
    
      
        w
      
    
    {\displaystyle w}
  
 that minimizes 
  
    
      
        Q
        (
        w
        )
      
    
    {\displaystyle Q(w)}
  
 is to be estimated. Each summand function 
  
    
      
        
          Q
          
            i
          
        
      
    
    {\displaystyle Q_{i}}
  
 is typically associated with the 
  
    
      
        i
      
    
    {\displaystyle i}
  
-th observation in the data set (used for training).
In classical statistics, sum-minimization problems arise in least squares and in maximum-likelihood estimation (for independent observations).  The general class of estimators that arise as minimizers of sums are called M-estimators."
"Robert Louis Stevenson (born Robert Lewis Balfour Stevenson; 13 November 1850 – 3 December 1894) was a Scottish novelist, essayist, poet and travel writer. He is best known for works such as Treasure Island, Strange Case of Dr Jekyll and Mr Hyde, Kidnapped and A Child's Garden of Verses.
Born and educated in Edinburgh, Stevenson suffered from serious bronchial trouble for much of his life but continued to write prolifically and travel widely in defiance of his poor health. As a young man, he mixed in London literary circles, receiving encouragement from Sidney Colvin, Andrew Lang, Edmund Gosse, Leslie Stephen and W. E. Henley, the last of whom may have provided the model for Long John Silver in Treasure Island. In 1890, he settled in Samoa where, alarmed at increasing European and American influence in the South Sea islands, his writing turned from romance and adventure fiction toward a darker realism. He died of a stroke in his island home in 1894 at age 44.
A celebrity in his lifetime, Stevenson's critical reputation has fluctuated since his death, though today his works are held in general acclaim. In 2018, he was ranked just behind Charles Dickens as the 26th-most-translated author in the world.


== Family and education ==


=== Childhood and youth ===

Stevenson was born at 8 Howard Place, Edinburgh, Scotland, on 13 November 1850 to Thomas Stevenson (1818–1887), a leading lighthouse engineer, and his wife, Margaret Isabella (born Balfour, 1829–1897). He was christened Robert Lewis Balfour Stevenson."
"Quantum machine learning is the integration of quantum algorithms within machine learning programs.
The most common use of the term refers to machine learning algorithms for the analysis of classical data executed on a quantum computer, i.e. quantum-enhanced machine learning. While machine learning algorithms are used to compute immense quantities of data, quantum machine learning utilizes qubits and quantum operations or specialized quantum systems to improve computational speed and data storage done by algorithms in a program. This includes hybrid methods that involve both classical and quantum processing, where computationally difficult subroutines are outsourced to a quantum device. These routines can be more complex in nature and executed faster on a quantum computer. Furthermore, quantum algorithms can be used to analyze quantum states instead of classical data.
Beyond quantum computing, the term ""quantum machine learning"" is also associated with classical machine learning methods applied to data generated from quantum experiments (i.e. machine learning of quantum systems), such as learning the phase transitions of a quantum system or creating new quantum experiments.
Quantum machine learning also extends to a branch of research that explores methodological and structural similarities between certain physical systems and learning systems, in particular neural networks."
"Markov decision process (MDP), also called a stochastic dynamic program or stochastic control problem, is a model for sequential decision making when outcomes are uncertain.
Originating from operations research in the 1950s, MDPs have since gained recognition in a variety of fields, including ecology, economics, healthcare, telecommunications and reinforcement learning. Reinforcement learning utilizes the MDP framework to model the interaction between a learning agent and its environment. In this framework, the interaction is characterized by states, actions, and rewards. The MDP framework is designed to provide a simplified representation of key elements of artificial intelligence challenges. These elements encompass the understanding of cause and effect, the management of uncertainty and nondeterminism, and the pursuit of explicit goals.
The name comes from its connection to Markov chains, a concept developed by the Russian mathematician Andrey Markov. The ""Markov"" in ""Markov decision process"" refers to the underlying structure of state transitions that still follow the Markov property. The process is called a ""decision process"" because it involves making decisions that influence these state transitions, extending the concept of a Markov chain into the realm of decision-making under uncertainty.


== Definition ==

A Markov decision process is a 4-tuple 
  
    
      
        (
        S
        ,
        A
        ,
        
          P
          
            a
          
        
        ,
        
          R
          
            a
          
        
        )
      
    
    {\displaystyle (S,A,P_{a},R_{a})}
  
, where:

  
    
      
        S
      
    
    {\displaystyle S}
  
 is a set of states called the state space."
"In information theory, the cross-entropy between two probability distributions 
  
    
      
        p
      
    
    {\displaystyle p}
  
 and 
  
    
      
        q
      
    
    {\displaystyle q}
  
, over the same underlying set of events, measures the average number of bits needed to identify an event drawn from the set when the coding scheme used for the set is optimized for an estimated probability distribution 
  
    
      
        q
      
    
    {\displaystyle q}
  
, rather than the true distribution 
  
    
      
        p
      
    
    {\displaystyle p}
  
.


== Definition ==
The cross-entropy of the distribution 
  
    
      
        q
      
    
    {\displaystyle q}
  
  relative to a distribution 
  
    
      
        p
      
    
    {\displaystyle p}
  
 over a given set is defined as follows:

  
    
      
        H
        (
        p
        ,
        q
        )
        =
        −
        
          E
          
            p
          
        
        ⁡
        [
        log
        ⁡
        q
        ]
        ,
      
    
    {\displaystyle H(p,q)=-\operatorname {E} _{p}[\log q],}
  

where 
  
    
      
        
          E
          
            p
          
        
        [
        ⋅
        ]
      
    
    {\displaystyle E_{p}[\cdot ]}
  
 is the expected value operator with respect to the distribution 
  
    
      
        p
      
    
    {\displaystyle p}
  
.
The definition may be formulated using the Kullback–Leibler divergence 
  
    
      
        
          D
          
            
              K
              L
            
          
        
        (
        p
        ∥
        q
        )
      
    
    {\displaystyle D_{\mathrm {KL} }(p\parallel q)}
  
, divergence of 
  
    
      
        p
      
    
    {\displaystyle p}
  
 from 
  
    
      
        q
      
    
    {\displaystyle q}
  
 (also known as the relative entropy of 
  
    
      
        p
      
    
    {\displaystyle p}
  
 with respect to 
  
    
      
        q
      
    
    {\displaystyle q}
  
).

  
    
      
        H
        (
        p
        ,
        q
        )
        =
        H
        (
        p
        )
        +
        
          D
          
            
              K
              L
            
          
        
        (
        p
        ∥
        q
        )
        ,
      
    
    {\displaystyle H(p,q)=H(p)+D_{\mathrm {KL} }(p\parallel q),}
  

where 
  
    
      
        H
        (
        p
        )
      
    
    {\displaystyle H(p)}
  
 is the entropy of 
  
    
      
        p
      
    
    {\displaystyle p}
  
.
For discrete probability distributions 
  
    
      
        p
      
    
    {\displaystyle p}
  
 and 
  
    
      
        q
      
    
    {\displaystyle q}
  
 with the same support 
  
    
      
        
          
            X
          
        
      
    
    {\displaystyle {\mathcal {X}}}
  
, this means

The situation for continuous distributions is analogous. We have to assume that 
  
    
      
        p
      
    
    {\displaystyle p}
  
 and 
  
    
      
        q
      
    
    {\displaystyle q}
  
 are absolutely continuous with respect to some reference measure 
  
    
      
        r
      
    
    {\displaystyle r}
  
 (usually 
  
    
      
        r
      
    
    {\displaystyle r}
  
 is a Lebesgue measure on a Borel σ-algebra). Let 
  
    
      
        P
      
    
    {\displaystyle P}
  
 and 
  
    
      
        Q
      
    
    {\displaystyle Q}
  
 be probability density functions of 
  
    
      
        p
      
    
    {\displaystyle p}
  
 and 
  
    
      
        q
      
    
    {\displaystyle q}
  
 with respect to 
  
    
      
        r
      
    
    {\displaystyle r}
  
. Then

  
    
      
        −
        
          ∫
          
            
              X
            
          
        
        P
        (
        x
        )
        
        log
        ⁡
        Q
        (
        x
        )
        
        
          d
        
        x
        =
        
          E
          
            p
          
        
        ⁡
        [
        −
        log
        ⁡
        Q
        ]
        ,
      
    
    {\displaystyle -\int _{\mathcal {X}}P(x)\,\log Q(x)\,\mathrm {d} x=\operatorname {E} _{p}[-\log Q],}
  

and therefore

NB: The notation 
  
    
      
        H
        (
        p
        ,
        q
        )
      
    
    {\displaystyle H(p,q)}
  
 is also used for a different concept, the joint entropy of 
  
    
      
        p
      
    
    {\displaystyle p}
  
 and 
  
    
      
        q
      
    
    {\displaystyle q}
  
.


== Motivation ==
In information theory, the Kraft–McMillan theorem establishes that any directly decodable coding scheme for coding a message to identify one value 
  
    
      
        
          x
          
            i
          
        
      
    
    {\displaystyle x_{i}}
  
 out of a set of possibilities 
  
    
      
        {
        
          x
          
            1
          
        
        ,
        …
        ,
        
          x
          
            n
          
        
        }
      
    
    {\displaystyle \{x_{1},\ldots ,x_{n}\}}
  
 can be seen as representing an implicit probability distribution 
  
    
      
        q
        (
        
          x
          
            i
          
        
        )
        =
        
          
            (
            
              
                1
                2
              
            
            )
          
          
            
              ℓ
              
                i
              
            
          
        
      
    
    {\displaystyle q(x_{i})=\left({\frac {1}{2}}\right)^{\ell _{i}}}
  
 over 
  
    
      
        {
        
          x
          
            1
          
        
        ,
        …
        ,
        
          x
          
            n
          
        
        }
      
    
    {\displaystyle \{x_{1},\ldots ,x_{n}\}}
  
, where 
  
    
      
        
          ℓ
          
            i
          
        
      
    
    {\displaystyle \ell _{i}}
  
 is the length of the code for 
  
    
      
        
          x
          
            i
          
        
      
    
    {\displaystyle x_{i}}
  
 in bits. Therefore, cross-entropy can be interpreted as the expected message-length per datum when a wrong distribution 
  
    
      
        q
      
    
    {\displaystyle q}
  
 is assumed while the data actually follows a distribution 
  
    
      
        p
      
    
    {\displaystyle p}
  
."
"Data preprocessing can refer to manipulation, filtration or augmentation of data before it is analyzed, and is often an important step in the data mining process. Data collection methods are often loosely controlled, resulting in out-of-range values, impossible data combinations, and missing values, amongst other issues.
The preprocessing pipeline used can often have large effects on the conclusions drawn from the downstream analysis. Thus, representation and quality of data is necessary before running any analysis. 
Often, data preprocessing is the most important phase of a machine learning project, especially in computational biology. If there is a high proportion of irrelevant and redundant information present or noisy and unreliable data, then knowledge discovery during the training phase may be more difficult. Data preparation and filtering steps can take a considerable amount of processing time. Examples of methods used in data preprocessing include cleaning, instance selection, normalization, one-hot encoding, data transformation, feature extraction and feature selection.


== Applications ==


=== Data mining ===

Data preprocessing allows for the removal of unwanted data with the use of data cleaning, this allows the user to have a dataset to contain more valuable information after the preprocessing stage for data manipulation later in the data mining process. Editing such dataset to either correct data corruption or human error is a crucial step to get accurate quantifiers like true positives, true negatives, false positives and false negatives found in a confusion matrix that are commonly used for a medical diagnosis."
"Self-supervised learning (SSL) is a paradigm in machine learning where a model is trained on a task using the data itself to generate supervisory signals, rather than relying on externally-provided labels. In the context of neural networks, self-supervised learning aims to leverage inherent structures or relationships within the input data to create meaningful training signals. SSL tasks are designed so that solving them requires capturing essential features or relationships in the data. The input data is typically augmented or transformed in a way that creates pairs of related samples, where one sample serves as the input, and the other is used to formulate the supervisory signal. This augmentation can involve introducing noise, cropping, rotation, or other transformations. Self-supervised learning more closely imitates the way humans learn to classify objects. 
During SSL, the model learns in two steps. First, the task is solved based on an auxiliary or pretext classification task using pseudo-labels, which help to initialize the model parameters. Next, the actual task is performed with supervised or unsupervised learning.
Self-supervised learning has produced promising results in recent years, and has found practical application in fields such as audio processing, and is being used by Facebook and others for speech recognition."
"A multi-agent system (MAS or ""self-organized system"") is a computerized system composed of multiple interacting intelligent agents. Multi-agent systems can solve problems that are difficult or impossible for an individual agent or a monolithic system to solve. Intelligence may include methodic, functional, procedural approaches, algorithmic search or reinforcement learning. With advancements in Large language model (LLMs), LLM-based multi-agent systems have emerged as a new area of research, enabling more sophisticated interactions and coordination among agents.
Despite considerable overlap, a multi-agent system is not always the same as an agent-based model (ABM).  The goal of an ABM is to search for explanatory insight into the collective behavior of agents (which do not necessarily need to be ""intelligent"") obeying simple rules, typically in natural systems, rather than in solving specific practical or engineering problems. The terminology of ABM tends to be used more often in the science, and MAS in engineering and technology. Applications where multi-agent systems research may deliver an appropriate approach include online trading, disaster response, target surveillance and social structure modelling.


== Concept ==
Multi-agent systems consist of agents and their environment. Typically multi-agent systems research refers to software agents."
"A Bayesian network (also known as a Bayes network, Bayes net, belief network, or decision network) is a probabilistic graphical model that represents a set of variables and their conditional dependencies via a directed acyclic graph (DAG). While it is one of several forms of causal notation, causal networks are special cases of Bayesian networks. Bayesian networks are ideal for taking an event that occurred and predicting the likelihood that any one of several possible known causes was the contributing factor.  For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases.
Efficient algorithms can perform inference and learning in Bayesian networks. Bayesian networks that model sequences of variables (e.g. speech signals or protein sequences) are called dynamic Bayesian networks. Generalizations of Bayesian networks that can represent and solve decision problems under uncertainty are called influence diagrams.


== Graphical model ==
Formally, Bayesian networks are directed acyclic graphs (DAGs) whose nodes represent variables in the Bayesian sense: they may be observable quantities, latent variables, unknown parameters or hypotheses."
"Machine translation is use of computational techniques to translate text or speech from one language to another, including the contextual, idiomatic and pragmatic nuances of both languages.
Early approaches were mostly rule-based or statistical. These methods have since been superseded by neural machine translation and large language models.


== History ==


=== Origins ===
The origins of machine translation can be traced back to the work of Al-Kindi, a ninth-century Arabic cryptographer who developed techniques for systemic language translation, including cryptanalysis, frequency analysis, and probability and statistics, which are used in modern machine translation. The idea of machine translation later appeared in the 17th century. In 1629, René Descartes proposed a universal language, with equivalent ideas in different tongues sharing one symbol.
The idea of using digital computers for translation of natural languages was proposed as early as 1947 by England's A. D. Booth and Warren Weaver at Rockefeller Foundation in the same year. ""The memorandum written by Warren Weaver in 1949 is perhaps the single most influential publication in the earliest days of machine translation."" Others followed. A demonstration was made in 1954 on the APEXC machine at Birkbeck College (University of London) of a rudimentary translation of English into French. Several papers on the topic were published at the time, and even articles in popular journals (for example an article by Cleave and Zacharov in the September 1955 issue of Wireless World)."
"In machine learning, a neural network (also artificial neural network or neural net, abbreviated ANN or NN) is a model inspired by the structure and function of biological neural networks in animal brains.
An ANN consists of connected units or nodes called artificial neurons, which loosely model the neurons in the brain. These are connected by edges, which model the synapses in the brain. Each artificial neuron receives signals from connected neurons, then processes them and sends a signal to other connected neurons. The ""signal"" is a real number, and the output of each neuron is computed by some non-linear function of the sum of its inputs, called the activation function. The strength of the signal at each connection is determined by a weight, which adjusts during the learning process.
Typically, neurons are aggregated into layers. Different layers may perform different transformations on their inputs. Signals travel from the first layer (the input layer) to the last layer (the output layer), possibly passing through multiple intermediate layers (hidden layers). A network is typically called a deep neural network if it has at least two hidden layers."
"In the fields of medicine, biotechnology, and pharmacology, drug discovery is the process by which new candidate medications are discovered.
Historically, drugs were discovered by identifying the active ingredient from traditional remedies or by serendipitous discovery, as with penicillin. More recently, chemical libraries of synthetic small molecules, natural products, or extracts were screened in intact cells or whole organisms to identify substances that had a desirable therapeutic effect in a process known as classical pharmacology. After sequencing of the human genome allowed rapid cloning and synthesis of large quantities of purified proteins, it has become common practice to use high throughput screening of large compounds libraries against isolated biological targets which are hypothesized to be disease-modifying in a process known as reverse pharmacology. Hits from these screens are then tested in cells and then in animals for efficacy.
Modern drug discovery involves the identification of screening hits, medicinal chemistry, and optimization of those hits to increase the affinity, selectivity (to reduce the potential of side effects), efficacy/potency, metabolic stability (to increase the half-life), and oral bioavailability. Once a compound that fulfills all of these requirements has been identified, the process of drug development can continue. If successful, clinical trials are developed.
Modern drug discovery is thus usually a capital-intensive process that involves large investments by pharmaceutical industry corporations as well as national governments (who provide grants and loan guarantees). Despite advances in technology and understanding of biological systems, drug discovery is still a lengthy, ""expensive, difficult, and inefficient process"" with low rate of new therapeutic discovery."
"A convolutional neural network (CNN) is a regularized type of feed-forward neural network that learns features by itself via filter (or kernel) optimization. This type of deep learning network has been applied to process and make predictions from many different types of data including text, images and audio. Convolution-based networks are the de-facto standard in deep learning-based approaches to computer vision and image processing, and have only recently have been replaced -- in some cases -- by newer deep learning architectures such as the transformer. Vanishing gradients and exploding gradients, seen during backpropagation in earlier neural networks, are prevented by using regularized weights over fewer connections. For example, for each neuron in the fully-connected layer, 10,000 weights would be required for processing an image sized 100 × 100 pixels. However, applying cascaded convolution (or cross-correlation) kernels, only 25 neurons are required to process 5x5-sized tiles. Higher-layer features are extracted from wider context windows, compared to lower-layer features.
Some applications of CNNs include: 

image and video recognition,
recommender systems,
image classification,
image segmentation,
medical image analysis,
natural language processing,
brain–computer interfaces, and
financial time series.
CNNs are also known as shift invariant or space invariant artificial neural networks, based on the shared-weight architecture of the convolution kernels or filters that slide along input features and provide translation-equivariant responses known as feature maps. Counter-intuitively, most convolutional neural networks are not invariant to translation, due to the downsampling operation they apply to the input."
"The Fashion MNIST dataset is a large freely available database of fashion images that is commonly used for training and testing various machine learning systems. Fashion-MNIST was intended to serve as a replacement for the original MNIST database for benchmarking machine learning algorithms, as it shares the same image size, data format and the structure of training and testing splits.
The dataset contains 70,000 28x28 grayscale images of fashion products from 10 categories from a dataset of Zalando article images, with 7,000 images per category. The training set consists of 60,000 images and the test set consists of 10,000 images. The dataset is commonly included in standard machine learning libraries.


== History ==
The set of images in the Fashion MNIST database was created in 2017 to pose a more challenging classification task than the simple MNIST digits data, which saw performance reaching upwards of 99.7%. 
The GitHub repository has collected over 4000 stars and is referred to more than 400 repositories, 1000 commits and 7000 code snippets.
Numerous machine learning algorithms have used the dataset as a benchmark, with the top algorithm achieving 96.91% accuracy in 2020 according to the benchmark rankings website.  The dataset was also used as a benchmark in the 2018 Science paper using all optical hardware to classify images at the speed of light. Google, University of Cambridge, IBM Research, Université de Montréal, and Peking University are the repositories most published institutions as of 2021."
"Temporal difference (TD) learning refers to a class of model-free reinforcement learning methods which learn by bootstrapping from the current estimate of the value function. These methods sample from the environment, like Monte Carlo methods, and perform updates based on current estimates, like dynamic programming methods.
While Monte Carlo methods only adjust their estimates once the final outcome is known, TD methods adjust predictions to match later, more accurate, predictions about the future before the final outcome is known. This is a form of bootstrapping, as illustrated with the following example:

Suppose you wish to predict the weather for Saturday, and you have some model that predicts Saturday's weather, given the weather of each day in the week. In the standard case, you would wait until Saturday and then adjust all your models. However, when it is, for example, Friday, you should have a pretty good idea of what the weather would be on Saturday – and thus be able to change, say, Saturday's model before Saturday arrives.
Temporal difference methods are related to the temporal difference model of animal learning.


== Mathematical formulation ==
The tabular TD(0) method is one of the simplest TD methods. It is a special case of more general stochastic approximation methods. It estimates the state value function of a finite-state Markov decision process (MDP) under a policy 
  
    
      
        π
      
    
    {\displaystyle \pi }
  
."
"In data mining and statistics, hierarchical clustering (also called hierarchical cluster analysis or HCA) is a method of cluster analysis that seeks to build a hierarchy of clusters. Strategies for hierarchical clustering generally fall into two categories:

Agglomerative: This is a ""bottom-up"" approach: Each observation starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy.
Divisive: This is a ""top-down"" approach: All observations start in one cluster, and splits are performed recursively as one moves down the hierarchy.
In general, the merges and splits are determined in a greedy manner. The results of hierarchical clustering are usually presented in a dendrogram.
Hierarchical clustering has the distinct advantage that any valid measure of distance can be used. In fact, the observations themselves are not required: all that is used is a matrix of distances. On the other hand, except for the special case of single-linkage distance, none of the algorithms (except exhaustive search in 
  
    
      
        
          
            O
          
        
        (
        
          2
          
            n
          
        
        )
      
    
    {\displaystyle {\mathcal {O}}(2^{n})}
  
) can be guaranteed to find the optimum solution.


== Complexity ==
The standard algorithm for hierarchical agglomerative clustering (HAC) has a time complexity of 
  
    
      
        
          
            O
          
        
        (
        
          n
          
            3
          
        
        )
      
    
    {\displaystyle {\mathcal {O}}(n^{3})}
  
 and requires 
  
    
      
        Ω
        (
        
          n
          
            2
          
        
        )
      
    
    {\displaystyle \Omega (n^{2})}
  
 memory, which makes it too slow for even medium data sets. However, for some special cases, optimal efficient agglomerative methods (of complexity 
  
    
      
        
          
            O
          
        
        (
        
          n
          
            2
          
        
        )
      
    
    {\displaystyle {\mathcal {O}}(n^{2})}
  
) are known: SLINK for single-linkage and CLINK for complete-linkage clustering."
"Reinforcement learning (RL) is an interdisciplinary area of machine learning and optimal control concerned with how an intelligent agent should take actions in a dynamic environment in order to maximize a reward signal. Reinforcement learning is one of the three basic machine learning paradigms, alongside supervised learning and unsupervised learning.
Q-learning at its simplest stores data in tables. This approach becomes infeasible as the number of states/actions increases (e.g., if the state space or action space were continuous), as the probability of the agent visiting a particular state and performing a particular action diminishes. 
Reinforcement learning differs from supervised learning in not needing labelled input-output pairs to be presented, and in not needing sub-optimal actions to be explicitly corrected. Instead, the focus is on finding a balance between exploration (of uncharted territory) and exploitation (of current knowledge) with the goal of maximizing the cumulative reward (the feedback of which might be incomplete or delayed). The search for this balance is known as the exploration-exploitation dilemma.

The environment is typically stated in the form of a Markov decision process (MDP), as many reinforcement learning algorithms use dynamic programming techniques. The main difference between classical dynamic programming methods and reinforcement learning algorithms is that the latter do not assume knowledge of an exact mathematical model of the Markov decision process, and they target large MDPs where exact methods become infeasible. 


== Introduction ==

Due to its generality, reinforcement learning is studied in many disciplines, such as game theory, control theory, operations research, information theory, simulation-based optimization, multi-agent systems, swarm intelligence, and statistics."
"Gradient boosting is a machine learning technique based on boosting in a functional space, where the target is pseudo-residuals instead of residuals as in traditional boosting. It gives a prediction model in the form of an ensemble of weak prediction models, i.e., models that make very few assumptions about the data, which are typically simple decision trees. When a decision tree is the weak learner, the resulting algorithm is called gradient-boosted trees; it usually outperforms random forest. As with other boosting methods, a gradient-boosted trees model is built in stages, but it generalizes the other methods by allowing optimization of an arbitrary differentiable loss function.


== History ==
The idea of gradient boosting originated in the observation by Leo Breiman that boosting can be interpreted as an optimization algorithm on a suitable cost function. Explicit regression gradient boosting algorithms were subsequently developed, by Jerome H. Friedman, (in 1999 and later in 2001) simultaneously with the more general functional gradient boosting perspective of Llew Mason, Jonathan Baxter, Peter Bartlett and Marcus Frean.
The latter two papers introduced the view of boosting algorithms as iterative functional gradient descent algorithms. That is, algorithms that optimize a cost function over function space by iteratively choosing a function (weak hypothesis) that points in the negative gradient direction. This functional gradient view of boosting has led to the development of boosting algorithms in many areas of machine learning and statistics beyond regression and classification.


== Informal introduction ==
(This section follows the exposition by Cheng Li.)
Like other boosting methods, gradient boosting combines weak ""learners"" into a single strong learner iteratively."
"In digital circuits and machine learning, a one-hot is a group of bits among which the legal combinations of values are only those with a single high (1) bit and all the others low (0). A similar implementation in which all bits are '1' except one '0' is sometimes called one-cold. In statistics, dummy variables represent a similar technique for representing categorical data.


== Applications ==


=== Digital circuitry ===
One-hot encoding is often used for indicating the state of a state machine. When using binary, a decoder is needed to determine the state. A one-hot state machine, however, does not need a decoder as the state machine is in the nth state if, and only if, the nth bit is high.
A ring counter with 15 sequentially ordered states is an example of a state machine. A 'one-hot' implementation would have 15 flip flops chained in series with the Q output of each flip flop connected to the D input of the next and the D input of the first flip flop connected to the Q output of the 15th flip flop. The first flip flop in the chain represents the first state, the second represents the second state, and so on to the 15th flip flop, which represents the last state. Upon reset of the state machine all of the flip flops are reset to '0' except the first in the chain, which is set to '1'."
"Dask is an open-source Python library for parallel computing. Dask scales Python code from multi-core local machines to large distributed clusters in the cloud. Dask provides a familiar user interface by mirroring the APIs of other libraries in the PyData ecosystem including: Pandas, scikit-learn and NumPy. It also exposes low-level APIs that help programmers run custom algorithms in parallel.
Dask was created by Matthew Rocklin in December 2014 and has over 9.8k stars and 500 contributors on GitHub.
Dask is used by retail, financial, governmental organizations, as well as life science and geophysical institutes. Walmart, Wayfair, JDA, GrubHub, General Motors, Nvidia, Harvard Medical School, Capital One and NASA are among the organizations that use Dask.


== Overview ==
Dask has two parts: 

Big data collections (high level and low level)
Dynamic task scheduling
Dask's high-level parallel collections – DataFrames, Bags, and Arrays – operate in parallel on datasets that may not fit into memory.
Dask’s task scheduler executes task graphs in parallel. It can scale to thousand-node clusters."
"Personalization (broadly known as customization) consists of tailoring a service or product to accommodate specific individuals. It is sometimes tied to groups or segments of individuals. Personalization involves collecting data on individuals, including web browsing history, web cookies, and location. Various organizations use personalization (along with the opposite mechanism of popularization) to improve customer satisfaction, digital sales conversion, marketing results, branding, and improved website metrics as well as for advertising. Personalization acts as a key element in social media and recommender systems. Personalization influences every sector of society — be it work, leisure, or citizenship.


== History ==
The idea of personalization is rooted in ancient rhetoric as part of the practice of an agent or communicator being responsive to the needs of the audience. When industrialization influenced the rise of mass communication, the practice of message personalization diminished for a time. 
In the recent times, there has been a significant increase in the number of mass media outlets that use advertising as a primary revenue stream. These companies gain knowledge about the specific demographic and psychographic characteristics of readers and viewers."
"Feature scaling is a method used to normalize the range of independent variables or features of data. In data processing, it is also known as data normalization and is generally performed during the data preprocessing step.


== Motivation ==
Since the range of values of raw data varies widely, in some machine learning algorithms, objective functions will not work properly without normalization. For example, many classifiers calculate the distance between two points by the Euclidean distance. If one of the features has a broad range of values, the distance will be governed by this particular feature. Therefore, the range of all features should be normalized so that each feature contributes approximately proportionately to the final distance.
Another reason why feature scaling is applied is that gradient descent converges much faster with feature scaling than without it.
It's also important to apply feature scaling if regularization is used as part of the loss function (so that coefficients are penalized appropriately).
Empirically, feature scaling can improve the convergence speed of stochastic gradient descent. In support vector machines, it can reduce the time to find support vectors."
"Stock market prediction is the act of trying to determine the future value of a company stock or other financial instrument traded on an exchange. The successful prediction of a stock's future price could yield significant profit. The efficient market hypothesis suggests that stock prices reflect all currently available information and any price changes that are not based on newly revealed information thus are inherently unpredictable. Others disagree and those with this viewpoint possess myriad methods and technologies which purportedly allow them to gain future price information.


== The efficient markets hypothesis and the random walk ==
The efficient market hypothesis posits that stock prices are a function of information and rational expectations, and that newly revealed information about a company's prospects is almost immediately reflected in the current stock price. This would imply that all publicly known information about a company, which obviously includes its price history, would already be reflected in the current price of the stock. Accordingly, changes in the stock price reflect release of new information, changes in the market generally, or random movements around the value that reflects the existing information set. 
Burton Malkiel, in his influential 1973 work A Random Walk Down Wall Street, claimed that stock prices could therefore not be accurately predicted by looking at price history.  As a result, Malkiel argued, stock prices are best described by a statistical process called a ""random walk"" meaning each day's deviations from the central value are random and unpredictable. This led Malkiel to conclude that paying financial services persons to predict the market actually hurt, rather than helped, net portfolio return."
"PAQ  is a series of lossless data compression archivers that have gone through collaborative development to top rankings on several benchmarks measuring compression ratio (although at the expense of speed and memory usage).  Specialized versions of PAQ have won the Hutter Prize and the Calgary Challenge.  PAQ is free software distributed under the GNU General Public License.


== Algorithm ==
PAQ uses a context mixing algorithm. Context mixing is related to prediction by partial matching (PPM) in that the compressor is divided into a predictor and an arithmetic coder, but differs in that the next-symbol prediction is computed using a weighted combination of probability estimates from a large number of models conditioned on different contexts.  Unlike PPM, a context doesn't need to be contiguous.  Most PAQ versions collect next-symbol statistics for the following contexts:

n-grams; the context is the last n bytes before the predicted symbol (as in PPM);
whole-word n-grams, ignoring case and nonalphabetic characters (useful in text files);
""sparse"" contexts, for example, the second and fourth bytes preceding the predicted symbol (useful in some binary formats);
""analog"" contexts, consisting of the high-order bits of previous 8- or 16-bit words (useful for multimedia files);
two-dimensional contexts (useful for images, tables, and spreadsheets); the row length is determined by finding the stride length of repeating byte patterns;
specialized models, such as x86 executables, BMP, TIFF, or JPEG images; these models are active only when the particular file type is detected.
All PAQ versions predict and compress one bit at a time, but differ in the details of the models and how the predictions are combined and postprocessed.  Once the next-bit probability is determined, it is encoded by arithmetic coding.  There are three methods for combining predictions, depending on the version:

In PAQ1 through PAQ3, each prediction is represented as a pair of bit counts 
  
    
      
        (
        
          n
          
            0
          
        
        ,
        
          n
          
            1
          
        
        )
      
    
    {\displaystyle (n_{0},n_{1})}
  
."
"Kaggle is a data science competition platform and online community for data scientists and machine learning practitioners under Google LLC. Kaggle enables users to find and publish datasets, explore and build models in a web-based data science environment, work with other data scientists and machine learning engineers, and enter competitions to solve data science challenges.


== History ==
Kaggle was founded by Anthony Goldbloom in April 2010. Jeremy Howard, one of the first Kaggle users, joined in November 2010 and served as the President and Chief Scientist. Also on the team was Nicholas Gruen serving as the founding chair. In 2011, the company raised $12.5 million and Max Levchin became the chairman. On March 8, 2017, Fei-Fei Li, Chief Scientist at Google, announced that Google was acquiring Kaggle.
In June 2017, Kaggle surpassed 1 million registered users, and as of October 2023, it has over 15 million users in 194 countries. 
In 2022, founders Goldbloom and Hamner stepped down from their positions and D. Sculley became the CEO.
In February 2023, Kaggle introduced Models, allowing users to discover and use pre-trained models through deep integrations with the rest of Kaggle’s platform.


== Site overview ==


=== Competitions ===
Many machine-learning competitions have been run on Kaggle since the company was founded. Notable competitions include gesture recognition for Microsoft Kinect, making a football AI for Manchester City, coding a trading algorithm for Two Sigma Investments, and improving the search for the Higgs boson at CERN.
The competition host prepares the data and a description of the problem; the host may choose whether it's going to be rewarded with money or be unpaid."
"In machine learning, feature selection is the process of selecting a subset of relevant features (variables, predictors) for use in model construction. Feature selection techniques are used for several reasons:

simplification of models to make them easier to interpret,
shorter training times,
to avoid the curse of dimensionality,
improve the compatibility of the data with a certain learning model class,
to encode inherent symmetries present in the input space.
The central premise when using feature selection is that data sometimes contains features that are redundant or irrelevant, and can thus be removed without incurring much loss of information. Redundancy and irrelevance are two distinct notions, since one relevant feature may be redundant in the presence of another relevant feature with which it is strongly correlated.
Feature extraction creates new features from functions of the original features, whereas feature selection finds a subset of the features. Feature selection techniques are often used in domains where there are many features and comparatively few samples (data points).


== Introduction ==
A feature selection algorithm can be seen as the combination of a search technique for proposing new feature subsets, along with an evaluation measure which scores the different feature subsets. The simplest algorithm is to test each possible subset of features finding the one which minimizes the error rate. This is an exhaustive search of the space, and is computationally intractable for all but the smallest of feature sets. The choice of evaluation metric heavily influences the algorithm, and it is these evaluation metrics which distinguish between the three main categories of feature selection algorithms: wrappers, filters and embedded methods."
"A language model is a probabilistic model of a natural language. In 1980, the first significant statistical language model was proposed, and during the decade IBM performed ‘Shannon-style’ experiments, in which potential sources for language modeling improvement were identified by observing and analyzing the performance of human subjects in predicting or correcting text.
Language models are useful for a variety of tasks, including speech recognition (helping prevent predictions of low-probability (e.g. nonsense) sequences), machine translation, natural language generation (generating more human-like text), optical character recognition, route optimization, handwriting recognition, grammar induction, and information retrieval.
Large language models, currently their most advanced form, are a combination of larger datasets (frequently using words scraped from the public internet), feedforward neural networks, and transformers. They have superseded recurrent neural network-based models, which had previously superseded the pure statistical models, such as word n-gram language model.


== Pure statistical models ==


=== Models based on word n-grams ===


=== Exponential ===
Maximum entropy language models encode the relationship between a word and the n-gram history using feature functions. The equation is

  
    
      
        P
        (
        
          w
          
            m
          
        
        ∣
        
          w
          
            1
          
        
        ,
        …
        ,
        
          w
          
            m
            −
            1
          
        
        )
        =
        
          
            1
            
              Z
              (
              
                w
                
                  1
                
              
              ,
              …
              ,
              
                w
                
                  m
                  −
                  1
                
              
              )
            
          
        
        exp
        ⁡
        (
        
          a
          
            T
          
        
        f
        (
        
          w
          
            1
          
        
        ,
        …
        ,
        
          w
          
            m
          
        
        )
        )
      
    
    {\displaystyle P(w_{m}\mid w_{1},\ldots ,w_{m-1})={\frac {1}{Z(w_{1},\ldots ,w_{m-1})}}\exp(a^{T}f(w_{1},\ldots ,w_{m}))}
  

where 
  
    
      
        Z
        (
        
          w
          
            1
          
        
        ,
        …
        ,
        
          w
          
            m
            −
            1
          
        
        )
      
    
    {\displaystyle Z(w_{1},\ldots ,w_{m-1})}
  
 is the partition function, 
  
    
      
        a
      
    
    {\displaystyle a}
  
 is the parameter vector, and 
  
    
      
        f
        (
        
          w
          
            1
          
        
        ,
        …
        ,
        
          w
          
            m
          
        
        )
      
    
    {\displaystyle f(w_{1},\ldots ,w_{m})}
  
 is the feature function. In the simplest case, the feature function is just an indicator of the presence of a certain n-gram. It is helpful to use a prior on 
  
    
      
        a
      
    
    {\displaystyle a}
  
 or some form of regularization."
"When classification is performed by a computer, statistical methods are normally used to develop the algorithm.
Often, the individual observations are analyzed into a set of quantifiable properties, known variously as explanatory variables or features.  These properties may variously be categorical (e.g. ""A"", ""B"", ""AB"" or ""O"", for blood type), ordinal (e.g. ""large"", ""medium"" or ""small""), integer-valued (e.g. the number of occurrences of a particular word in an email) or real-valued (e.g. a measurement of blood pressure). Other classifiers work by comparing observations to previous observations by means of a similarity or distance function.
An algorithm that implements classification, especially in a concrete implementation, is known as a classifier.  The term ""classifier"" sometimes also refers to the mathematical function, implemented by a classification algorithm, that maps input data to a category."
"In general, detection is the action of accessing information without specific cooperation from  with the sender.
In the history of radio communications, the term ""detector"" was first used for a device that detected the simple presence or absence of a radio signal, since all communications were in Morse code.  The term is still in use today to describe a component that extracts a particular signal from all of the electromagnetic waves present.  Detection is usually based on the frequency of the carrier signal, as in the familiar frequencies of radio broadcasting, but it may also involve filtering a faint signal from noise, as in radio astronomy, or reconstructing a hidden signal, as in steganography.
In optoelectronics, ""detection"" means converting a received optical input to an electrical output. For example, the light signal received through an optical fiber is converted to an electrical signal in a detector such as a photodiode.
In steganography, attempts to detect hidden signals in suspected carrier material is referred to as steganalysis.  Steganalysis has an interesting difference from most other types of detection, in that it can often only determine the probability that a hidden message exists; this is in contrast to the detection of signals which are simply encrypted, as the ciphertext can often be identified with certainty, even if it cannot be decoded. 
In the military, detection refers to the special discipline of reconnaissance with the aim to recognize the presence of an object in a location or ambiance.
Finally, the art of detection, also known as following clues, is the work of a detective in attempting to reconstruct a sequence of events by identifying the relevant information in a situation."
"Word2vec is a technique in natural language processing (NLP) for obtaining vector representations of words. These vectors capture information about the meaning of the word based on the surrounding words. The word2vec algorithm estimates these representations by modeling text in a large corpus. Once trained, such a model can detect synonymous words or suggest additional words for a partial sentence. Word2vec was developed by Tomáš Mikolov and colleagues at Google and published in 2013.
Word2vec represents a word as a high-dimension vector of numbers which capture relationships between words. In particular, words which appear in similar contexts are mapped to vectors which are nearby as measured by cosine similarity. This indicates the level of semantic similarity between the words, so for example the vectors for walk and ran are nearby, as are those for ""but"" and ""however"", and ""Berlin"" and ""Germany"".


== Approach ==
Word2vec is a group of related models that are used to produce word embeddings. These models are shallow, two-layer neural networks that are trained to reconstruct linguistic contexts of words."
"Principal component analysis (PCA) is a linear dimensionality reduction technique with applications in exploratory data analysis, visualization and data preprocessing.
The data is linearly transformed onto a new coordinate system such that the directions (principal components) capturing the largest variation in the data can be easily identified.
The principal components of a collection of points in a real coordinate space are a sequence of 
  
    
      
        p
      
    
    {\displaystyle p}
  
 unit vectors, where the 
  
    
      
        i
      
    
    {\displaystyle i}
  
-th vector is the direction of a line that best fits the data while being orthogonal to the first 
  
    
      
        i
        −
        1
      
    
    {\displaystyle i-1}
  
 vectors. Here, a best-fitting line is defined as one that minimizes the average squared perpendicular distance from the points to the line. These directions (i.e., principal components) constitute an orthonormal basis in which different individual dimensions of the data are linearly uncorrelated. Many studies use the first two principal components in order to plot the data in two dimensions and to visually identify clusters of closely related data points.
Principal component analysis has applications in many fields such as population genetics, microbiome studies, and atmospheric science.


== Overview ==
When performing PCA, the first principal component of a set of 
  
    
      
        p
      
    
    {\displaystyle p}
  
 variables is the derived variable formed as a linear combination of the original variables that explains the most variance. The second principal component explains the most variance in what is left once the effect of the first component is removed, and we may proceed through 
  
    
      
        p
      
    
    {\displaystyle p}
  
 iterations until all the variance is explained. PCA is most commonly used when many of the variables are highly correlated with each other and it is desirable to reduce their number to an independent set."
"Robotics is the interdisciplinary study and practice of the design, construction, operation, and use of robots.
Within mechanical engineering, robotics is the design and construction of the physical structures of robots, while in computer science, robotics focuses on robotic automation algorithms. Other disciplines contributing to robotics include electrical, control, software, information, electronic, telecommunication, computer, mechatronic, and materials engineering.
The goal of most robotics is to design machines that can help and assist humans. Many robots are built to do jobs that are hazardous to people, such as finding survivors in unstable ruins, and exploring space, mines and shipwrecks. Others replace people in jobs that are boring, repetitive, or unpleasant, such as cleaning, monitoring, transporting, and assembling. Today, robotics is a rapidly growing field, as technological advances continue; researching, designing, and building new robots serve various practical purposes.


== Robotics aspects ==

Robotics usually combines three aspects of design work to create robot systems:

Mechanical construction: a frame, form or shape designed to achieve a particular task. For example, a robot designed to travel across heavy dirt or mud might use caterpillar tracks. Origami inspired robots can sense and analyze in extreme environments."
"In machine learning, a neural network (also artificial neural network or neural net, abbreviated ANN or NN) is a model inspired by the structure and function of biological neural networks in animal brains.
An ANN consists of connected units or nodes called artificial neurons, which loosely model the neurons in the brain. These are connected by edges, which model the synapses in the brain. Each artificial neuron receives signals from connected neurons, then processes them and sends a signal to other connected neurons. The ""signal"" is a real number, and the output of each neuron is computed by some non-linear function of the sum of its inputs, called the activation function. The strength of the signal at each connection is determined by a weight, which adjusts during the learning process.
Typically, neurons are aggregated into layers. Different layers may perform different transformations on their inputs. Signals travel from the first layer (the input layer) to the last layer (the output layer), possibly passing through multiple intermediate layers (hidden layers). A network is typically called a deep neural network if it has at least two hidden layers."
"In statistics and machine learning, ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone.
Unlike a statistical ensemble in statistical mechanics, which is usually infinite, a machine learning ensemble consists of only a concrete finite set of alternative models, but typically allows for much more flexible structure to exist among those alternatives.


== Overview ==
Supervised learning algorithms search through a hypothesis space to find a suitable hypothesis that will make good predictions with a particular problem. Even if this space contains hypotheses that are very well-suited for a particular problem, it may be very difficult to find a good one. Ensembles combine multiple hypotheses to form one which should be theoretically better.
Ensemble learning trains two or more machine learning algorithms on a specific classification or regression task. The algorithms within the ensemble model are generally referred as ""base models"", ""base learners"", or ""weak learners"" in literature. These base models can be constructed using a single modelling algorithm, or several different algorithms. The idea is to train a diverse set of weak models on the same modelling task, such that the outputs of each weak learner have poor predictive ability (i.e., high bias), and among all weak learners, the outcome and error values exhibit high variance. Fundamentally, an ensemble learning model trains at least two high-bias (weak) and high-variance (diverse) models to be combined into a better-performing model."
"The ethics of artificial intelligence covers a broad range of topics within the field that are considered to have particular ethical stakes. This includes algorithmic biases, fairness, automated decision-making, accountability, privacy, and regulation. 
It also covers various emerging or potential future challenges such as machine ethics (how to make machines that behave ethically), lethal autonomous weapon systems, arms race dynamics, AI safety and alignment, technological unemployment, AI-enabled misinformation, how to treat certain AI systems if they have a moral status (AI welfare and rights), artificial superintelligence and existential risks. 
Some application areas may also have particularly important ethical implications, like healthcare, education, criminal justice, or the military.


== Machine ethics ==

Machine ethics (or machine morality) is the field of research concerned with designing Artificial Moral Agents (AMAs), robots or artificially intelligent computers that behave morally or as though moral. To account for the nature of these agents, it has been suggested to consider certain philosophical ideas, like the standard characterizations of agency, rational agency, moral agency, and artificial agency, which are related to the concept of AMAs.
There are discussions on creating tests to see if an AI is capable of making ethical decisions. Alan Winfield concludes that the Turing test is flawed and the requirement for an AI to pass the test is too low. A proposed alternative test is one called the Ethical Turing Test, which would improve on the current test by having multiple judges decide if the AI's decision is ethical or unethical. Neuromorphic AI could be one way to create morally capable robots, as it aims to process information similarly to humans, nonlinearly and with millions of interconnected artificial neurons."
"Matplotlib (portmanteau of MATLAB, plot, and library) is a plotting library for the Python programming language and its numerical mathematics extension NumPy. It provides an object-oriented API for embedding plots into applications using general-purpose GUI toolkits like Tkinter, wxPython, Qt, or GTK. There is also a procedural ""pylab"" interface based on a state machine (like OpenGL), designed to closely resemble that of MATLAB, though its use is discouraged. SciPy makes use of Matplotlib.
Matplotlib was originally written by John D. Hunter. Since then it has had an active development community and is distributed under a BSD-style license. Michael Droettboom was nominated as matplotlib's lead developer shortly before John Hunter's death in August 2012 and was further joined by Thomas Caswell. Matplotlib is a NumFOCUS fiscally sponsored project.


== Comparison with MATLAB ==
Pyplot is a Matplotlib module that provides a MATLAB-like interface. Matplotlib is designed to be as usable as MATLAB, with the ability to use Python, and the advantage of being free and open-source.


== Plot Types ==
Matplotlib supports various types of 2 dimensional and 3 dimensional plots."
"In statistics, the coefficient of determination, denoted R2 or r2 and pronounced ""R squared"", is the proportion of the variation in the dependent variable that is predictable from the independent variable(s).
It is a statistic used in the context of statistical models whose main purpose is either the prediction of future outcomes or the testing of hypotheses, on the basis of other related information. It provides a measure of how well observed outcomes are replicated by the model, based on the proportion of total variation of outcomes explained by the model.
There are several definitions of R2 that are only sometimes equivalent. One class of such cases includes that of simple linear regression where r2 is used instead of R2. When only an intercept is included, then r2 is simply the square of the sample correlation coefficient (i.e., r) between the observed outcomes and the observed predictor values. If additional regressors are included, R2 is the square of the coefficient of multiple correlation. In both such cases, the coefficient of determination normally ranges from 0 to 1.
There are cases where R2 can yield negative values. This can arise when the predictions that are being compared to the corresponding outcomes have not been derived from a model-fitting procedure using those data."
"Swarm intelligence (SI) is the collective behavior of decentralized, self-organized systems, natural or artificial. The concept is employed in work on artificial intelligence. The expression was introduced by Gerardo Beni and Jing Wang in 1989, in the context of cellular robotic systems.
SI systems consist typically of a population of simple agents or boids interacting locally with one another and with their environment. The inspiration often comes from nature, especially biological systems. The agents follow very simple rules, and although there is no centralized control structure dictating how individual agents should behave, local, and to a certain degree random, interactions between such agents lead to the emergence of ""intelligent"" global behavior, unknown to the individual agents. Examples of swarm intelligence in natural systems include ant colonies, bee colonies, bird flocking, hawks hunting, animal herding, bacterial growth, fish schooling and microbial intelligence.
The application of swarm principles to robots is called swarm robotics while swarm intelligence refers to the more general set of algorithms. Swarm prediction has been used in the context of forecasting problems. Similar approaches to those proposed for swarm robotics are considered for genetically modified organisms in synthetic collective intelligence."
"A convolutional neural network (CNN) is a regularized type of feed-forward neural network that learns features by itself via filter (or kernel) optimization. This type of deep learning network has been applied to process and make predictions from many different types of data including text, images and audio. Convolution-based networks are the de-facto standard in deep learning-based approaches to computer vision and image processing, and have only recently have been replaced -- in some cases -- by newer deep learning architectures such as the transformer. Vanishing gradients and exploding gradients, seen during backpropagation in earlier neural networks, are prevented by using regularized weights over fewer connections. For example, for each neuron in the fully-connected layer, 10,000 weights would be required for processing an image sized 100 × 100 pixels. However, applying cascaded convolution (or cross-correlation) kernels, only 25 neurons are required to process 5x5-sized tiles. Higher-layer features are extracted from wider context windows, compared to lower-layer features.
Some applications of CNNs include: 

image and video recognition,
recommender systems,
image classification,
image segmentation,
medical image analysis,
natural language processing,
brain–computer interfaces, and
financial time series.
CNNs are also known as shift invariant or space invariant artificial neural networks, based on the shared-weight architecture of the convolution kernels or filters that slide along input features and provide translation-equivariant responses known as feature maps. Counter-intuitively, most convolutional neural networks are not invariant to translation, due to the downsampling operation they apply to the input."
"Kubernetes (, K8s) is an open-source container orchestration system for automating software deployment, scaling, and management. Originally designed by Google, the project is now maintained by a worldwide community of contributors, and the trademark is held by the Cloud Native Computing Foundation.
The name Kubernetes originates from the Greek κυβερνήτης (kubernḗtēs), meaning governor, 'helmsman' or 'pilot'. Kubernetes is often abbreviated as K8s, counting the eight letters between the K and the s (a numeronym).
Kubernetes assembles one or more computers, either virtual machines or bare metal, into a cluster which can run workloads in containers. It works with various container runtimes, such as containerd and CRI-O. Its suitability for running and managing workloads of all sizes and styles has led to its widespread adoption in clouds and data centers. There are multiple distributions of this platform – from independent software vendors (ISVs) as well as hosted-on-cloud offerings from all the major public cloud vendors.
Kubernetes is one of the most widely deployed software systems in the world being used across companies including Google, Microsoft, Amazon, Apple, Meta, Nvidia, Reddit and Pinterest.


== History ==

Kubernetes (Ancient Greek: κυβερνήτης, romanized: kubernḗtēs, 'steersman, navigator' or 'guide', and the etymological root of cybernetics) was announced by Google on June 6, 2014. The project was conceived and created by Google employees Joe Beda, Brendan Burns, and Craig McLuckie."
"In Germanic mythology, Sigmund (Old Norse: Sigmundr [ˈsiɣˌmundz̠], Old English: Sigemund) is a hero whose story is told in the Völsunga saga. He and his sister, Signý, are the children of Völsung and his wife Hljod. Sigmund is best known as the father of Sigurð the dragon-slayer, though Sigurð's tale has almost no connections to the Völsung cycle except that he was a dragonslayer.


== Völsunga saga ==

In the Völsunga saga, Signý marries Siggeir, the king of Gautland (modern Västergötland). Völsung and Sigmund are attending the wedding feast (which lasted for some time before and after the marriage), when Odin, disguised as a beggar, plunges a sword (Gram) into the living tree Barnstokk (""offspring-trunk"") around which Völsung's hall is built. The disguised Odin announces that the man who can remove the sword will have it as a gift. Only Sigmund is able to free the sword from the tree.
Siggeir is smitten with envy and desire for the sword. He tries to buy it but Sigmund refuses. Siggeir invites Sigmund, his father Völsung and Sigmund's nine brothers to visit him in Gautland to see the newlyweds three months later."
"Google AI is a division of Google dedicated to artificial intelligence. It was announced at Google I/O 2017 by CEO Sundar Pichai.
This division has expanded its reach with research facilities in various parts of the world such as Zurich, Paris, Israel, and Beijing. In 2023, Google AI was part of the reorganization initiative that elevated its head, Jeff Dean, to the position of chief scientist at Google. This reorganization involved the merging of Google Brain and DeepMind, a UK-based company that Google acquired in 2014 that operated separately from the company's core research.
In March 2019 Google announced the creation of an Advanced Technology External Advisory Council (ATEAC) comprising eight members: Alessandro Acquisti, Bubacarr Bah, De Kai, Dyan Gibbens, Joanna Bryson, Kay Coles James, Luciano Floridi and William Joseph Burns.  Following objections from a large number of Google staff to the appointment of Kay Coles James, the Council was abandoned within one month of its establishment.


== Projects ==
Google Vids: AI-powered video creation for work.
Google Assistant: is a virtual assistant software application since 2023 developed by Google AI.
Serving cloud-based TPUs (tensor processing units) in order to develop machine learning software. The TPU research cloud provides free access to a cluster of cloud TPUs to researchers engaged in open-source machine learning research."
"A capsule neural network (CapsNet) is a machine learning system that is a type of artificial neural network (ANN) that can be used to better model hierarchical relationships. The approach is an attempt to more closely mimic biological neural organization.
The idea is to add structures called ""capsules"" to a convolutional neural network (CNN), and to reuse output from several of those capsules to form more stable (with respect to various perturbations) representations for higher capsules. The output is a vector consisting of the probability of an observation, and a pose for that observation. This vector is similar to what is done for example when doing classification with localization in CNNs.
Among other benefits, capsnets address the ""Picasso problem"" in image recognition: images that have all the right parts but that are not in the correct spatial relationship (e.g., in a ""face"", the positions of the mouth and one eye are switched). For image recognition, capsnets exploit the fact that while viewpoint changes have nonlinear effects at the pixel level, they have linear effects at the part/object level. This can be compared to inverting the rendering of an object of multiple parts.


== History ==
In 2000, Geoffrey Hinton et al. described an imaging system that combined segmentation and recognition into a single inference process using parse trees."
"A receiver operating characteristic curve, or ROC curve, is a graphical plot that illustrates the performance of a binary classifier model (can be used for multi class classification as well) at varying threshold values.
The ROC curve is the plot of the true positive rate (TPR) against the false positive rate (FPR) at each threshold setting.
The ROC can also be thought of as a plot of the statistical power as a function of the Type I Error of the decision rule (when the performance is calculated from just a sample of the population, it can be thought of as estimators of these quantities). The ROC curve is thus the sensitivity as a function of false positive rate.
Given that the probability distributions for both true positive and false positive are known, the ROC curve is obtained as the cumulative distribution function (CDF, area under the probability distribution from 
  
    
      
        −
        ∞
      
    
    {\displaystyle -\infty }
  
 to the discrimination threshold) of the detection probability in the y-axis versus the CDF of the false positive probability on the x-axis.
ROC analysis provides tools to select possibly optimal models and to discard suboptimal ones independently from (and prior to specifying) the cost context or the class distribution. ROC analysis is related in a direct and natural way to the cost/benefit analysis of diagnostic decision making.


== Terminology ==
The true-positive rate is also known as sensitivity or probability of detection. The false-positive rate is also known as the probability of false alarm and equals (1 − specificity). 
The ROC is also known as a relative operating characteristic curve, because it is a comparison of two operating characteristics (TPR and FPR) as the criterion changes."
"Question answering (QA) is a computer science discipline within the fields of information retrieval and natural language processing (NLP) that is concerned with building systems that automatically answer questions that are posed by humans in a natural language.


== Overview ==
A question-answering implementation, usually a computer program, may construct its answers by querying a structured database of knowledge or information, usually a knowledge base. More commonly, question-answering systems can pull answers from an unstructured collection of natural language documents.
Some examples of natural language document collections used for question answering systems include:

a local collection of reference texts
internal organization documents and web pages
compiled newswire reports
a set of Wikipedia pages
a subset of World Wide Web pages


== Types of question answering ==
Question-answering research attempts to develop ways of answering a wide range of question types, including fact, list, definition, how, why, hypothetical, semantically constrained, and cross-lingual questions.

Answering questions related to an article in order to evaluate reading comprehension is one of the simpler form of question answering, since a given article is relatively short compared to the domains of other types of question-answering problems. An example of such a question is ""What did Albert Einstein win the Nobel Prize for?"" after an article about this subject is given to the system.
Closed-book question answering is when a system has memorized some facts during training and can answer questions without explicitly being given a context. This is similar to humans taking closed-book exams.
Closed-domain question answering deals with questions under a specific domain (for example, medicine or automotive maintenance) and can exploit domain-specific knowledge frequently formalized in ontologies. Alternatively, ""closed-domain"" might refer to a situation where only a limited type of questions are accepted, such as questions asking for descriptive rather than procedural information."
"3D pose estimation is a process of predicting the transformation of an object from a user-defined reference pose, given an image or a 3D scan.  It arises in computer vision or robotics where the pose or transformation of an object can be used for alignment of a computer-aided design models, identification, grasping, or manipulation of the object.
The image data from which the pose of an object is determined can be either a single image, a stereo image pair, or an image sequence where, typically, the camera is moving with a known velocity.  The objects which are considered can be rather general, including a living being or body parts, e.g., a head or hands.  The methods which are used for determining the pose of an object, however, are usually specific for a class of objects and cannot generally be expected to work well for other types of objects.


== From an uncalibrated 2D camera ==
It is possible to estimate the 3D rotation and translation of a 3D object from a single 2D photo, if an approximate 3D model of the object is known and the corresponding points in the 2D image are known. A common technique developed in 1995 for solving this is POSIT, where the 3D pose is estimated directly from the 3D model points and the 2D image points, and corrects the errors iteratively until a good estimate is found from a single image. Most implementations of POSIT only work on non-coplanar points (in other words, it won't work with flat objects or planes).
Another approach is to register a 3D CAD model over the photograph of a known object by optimizing a suitable distance measure with respect to the pose parameters.
The distance measure is computed between the object in the photograph and the 3D CAD model projection at a given pose."
"Automatic summarization is the process of shortening a set of data computationally, to create a subset (a summary) that represents the most important or relevant information within the original content. Artificial intelligence algorithms are commonly developed and employed to achieve this, specialized for different types of data.
Text summarization is usually implemented by natural language processing methods, designed to locate the most informative sentences in a given document. On the other hand, visual content can be summarized using computer vision algorithms. Image summarization is the subject of ongoing research; existing approaches typically attempt to display the most representative images from a given image collection, or generate a video that only includes the most important content from the entire collection. Video summarization algorithms identify and extract from the original video content the most important frames (key-frames), and/or the most important video segments (key-shots), normally in a temporally ordered fashion. Video summaries simply retain a carefully selected subset of the original video frames and, therefore, are not identical to the output of video synopsis algorithms, where new video frames are being synthesized based on the original video content.


== Commercial products ==
In 2022 Google Docs released an automatic summarization feature.


== Approaches ==
There are two general approaches to automatic summarization: extraction and abstraction.


=== Extraction-based summarization ===
Here, content is extracted from the original data, but the extracted content is not modified in any way."
"A vision transformer (ViT) is a transformer designed for computer vision. A ViT decomposes an input image into a series of patches (rather than text into tokens), serializes each patch into a vector, and maps it to a smaller dimension with a single matrix multiplication. These vector embeddings are then processed by a transformer encoder as if they were token embeddings.
ViTs were designed as alternatives to convolutional neural networks (CNNs) in computer vision applications. They have different inductive biases, training stability, and data efficiency. Compared to CNNs, ViTs are less data efficient, but have higher capacity. Some of the largest modern computer vision models are ViTs, such as one with 22B parameters. In 2024, a 113 billion-parameter ViT model was proposed (the largest ViT to date) for weather and climate prediction, and trained on the Frontier supercomputer with a throughput of 1.6 exaFLOPs.
Subsequent to its publication, many variants were proposed, with hybrid architectures with both features of ViTs and CNNs. ViTs have found application in image recognition, image segmentation, and autonomous driving."
"Stochastic gradient descent (often abbreviated SGD) is an iterative method for optimizing an objective function with suitable smoothness properties (e.g. differentiable or subdifferentiable). It can be regarded as a stochastic approximation of gradient descent optimization, since it replaces the actual gradient (calculated from the entire data set) by an estimate thereof (calculated from a randomly selected subset of the data). Especially in high-dimensional optimization problems this reduces the very high computational burden, achieving faster iterations in exchange for a lower convergence rate.
The basic idea behind stochastic approximation can be traced back to the Robbins–Monro algorithm of the 1950s. Today, stochastic gradient descent has become an important optimization method in machine learning.


== Background ==

Both statistical estimation and machine learning consider the problem of minimizing an objective function that has the form of a sum:

  
    
      
        Q
        (
        w
        )
        =
        
          
            1
            n
          
        
        
          ∑
          
            i
            =
            1
          
          
            n
          
        
        
          Q
          
            i
          
        
        (
        w
        )
        ,
      
    
    {\displaystyle Q(w)={\frac {1}{n}}\sum _{i=1}^{n}Q_{i}(w),}
  

where the parameter 
  
    
      
        w
      
    
    {\displaystyle w}
  
 that minimizes 
  
    
      
        Q
        (
        w
        )
      
    
    {\displaystyle Q(w)}
  
 is to be estimated. Each summand function 
  
    
      
        
          Q
          
            i
          
        
      
    
    {\displaystyle Q_{i}}
  
 is typically associated with the 
  
    
      
        i
      
    
    {\displaystyle i}
  
-th observation in the data set (used for training).
In classical statistics, sum-minimization problems arise in least squares and in maximum-likelihood estimation (for independent observations).  The general class of estimators that arise as minimizers of sums are called M-estimators."
"In deep learning, weight initialization describes the initial step in creating a neural network. A neural network contains trainable parameters that are modified during training: weight initalization is the pre-training step of assigning initial values to these parameters.
The choice of weight initialization method affects the speed of convergence, the scale of neural activation within the network, the scale of gradient signals during backpropagation, and the quality of the final model. Proper initialization is necessary for avoiding issues such as vanishing and exploding gradients and activation function saturation.
Note that even though this article is titled ""weight initialization"", both weights and biases are used in a neural network as trainable parameters, so this article describes how both of these are initialized. Similarly, trainable parameters in convolutional neural networks (CNNs) are called kernels and biases, and this article also describes these.


== Constant initialization ==
We discuss the main methods of initialization in the context of a multilayer perceptron (MLP). Specific strategies for initializing other network architectures are discussed in later sections.
For an MLP, there are only two kinds of trainable parameters, called weights and biases. Each layer 
  
    
      
        l
      
    
    {\displaystyle l}
  
 contains a weight matrix 
  
    
      
        
          W
          
            (
            l
            )
          
        
        ∈
        
          
            R
          
          
            
              n
              
                l
                −
                1
              
            
            ×
            
              n
              
                l
              
            
          
        
      
    
    {\displaystyle W^{(l)}\in \mathbb {R} ^{n_{l-1}\times n_{l}}}
  
and a bias vector 
  
    
      
        
          b
          
            (
            l
            )
          
        
        ∈
        
          
            R
          
          
            
              n
              
                l
              
            
          
        
      
    
    {\displaystyle b^{(l)}\in \mathbb {R} ^{n_{l}}}
  
, where 
  
    
      
        
          n
          
            l
          
        
      
    
    {\displaystyle n_{l}}
  
 is the number of neurons in that layer."
"Object detection is a computer technology related to computer vision and image processing that deals with detecting instances of semantic objects of a certain class (such as humans, buildings, or cars) in digital images and videos. Well-researched domains of object detection include face detection and pedestrian detection. Object detection has applications in many areas of computer vision, including image retrieval and video surveillance.


== Uses ==

It is widely used in computer vision tasks such as image annotation, vehicle counting, activity recognition, face detection, face recognition, video object co-segmentation. It is also used in tracking objects, for example tracking a ball during a football match, tracking movement of a cricket bat, or tracking a person in a video.
Often, the test images are sampled from a different data distribution, making the object detection task significantly more difficult. To address the challenges caused by the domain gap between training and test data, many unsupervised domain adaptation approaches have been proposed. A simple and straightforward solution for reducing the domain gap is to apply an image-to-image translation approach, such as cycle-GAN. Among other uses, cross-domain object detection is applied in autonomous driving, where models can be trained on a vast amount of video game scenes, since the labels can be generated without manual labor.


== Concept ==
Every object class has its own special features that help in classifying the class – for example all circles are round.
Object class detection uses these special features."
"In mathematical modeling, overfitting is ""the production of an analysis that corresponds too closely or exactly to a particular set of data, and may therefore fail to fit to additional data or predict future observations reliably"". An overfitted model is a mathematical model that contains more parameters than can be justified by the data. In a mathematical sense, these parameters represent the degree of a polynomial. The essence of overfitting is to have unknowingly extracted some of the residual variation (i.e., the noise) as if that variation represented underlying model structure.: 45 
Underfitting occurs when a mathematical model cannot adequately capture the underlying structure of the data. An under-fitted model is a model where some parameters or terms that would appear in a correctly specified model are missing. Underfitting would occur, for example, when fitting a linear model to nonlinear data. Such a model will tend to have poor predictive performance.
The possibility of over-fitting exists because the criterion used for selecting the model is not the same as the criterion used to judge the suitability of a model. For example, a model might be selected by maximizing its performance on some set of training data, and yet its suitability might be determined by its ability to perform well on unseen data; overfitting occurs when a model begins to ""memorize"" training data rather than ""learning"" to generalize from a trend. 
As an extreme example, if the number of parameters is the same as or greater than the number of observations, then a model can perfectly predict the training data simply by memorizing the data in its entirety."
"scikit-learn (formerly scikits.learn and also known as sklearn) is a free and open-source machine learning library for the Python programming language.
It features various classification, regression and clustering algorithms including support-vector machines, random forests, gradient boosting, k-means and DBSCAN, and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy. Scikit-learn is a NumFOCUS fiscally sponsored project.


== Overview ==
The scikit-learn project started as scikits.learn, a Google Summer of Code project by French data scientist David Cournapeau. The name of the project stems from the notion that it is a ""SciKit"" (SciPy Toolkit), a separately developed and distributed third-party extension to SciPy.
The original codebase was later rewritten by other developers. In 2010, contributors Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort and Vincent Michel, from the French Institute for Research in Computer Science and Automation in Saclay, France, took leadership of the project and released the first public version of the library on February 1, 2010. In November 2012, scikit-learn as well as scikit-image were described as two of the ""well-maintained and popular""  scikits libraries. In 2019, it was noted that scikit-learn is one of the most popular machine learning libraries on GitHub.


== Implementation ==
scikit-learn is largely written in Python, and uses NumPy extensively for high-performance linear algebra and array operations."
"In statistics, linear regression is a model that estimates the linear relationship between a scalar response (dependent variable) and one or more explanatory variables (regressor or independent variable). A model with exactly one explanatory variable is a simple linear regression; a model with two or more explanatory variables is a multiple linear regression. This term is distinct from multivariate linear regression, which predicts multiple correlated dependent variables rather than a single dependent variable.
In linear regression, the relationships are modeled using linear predictor functions whose unknown model parameters are estimated from the data. Most commonly, the conditional mean of the response given the values of the explanatory variables (or predictors) is assumed to be an affine function of those values; less commonly, the conditional median or some other quantile is used. Like all forms of regression analysis, linear regression focuses on the conditional probability distribution of the response given the values of the predictors, rather than on the joint probability distribution of all of these variables, which is the domain of multivariate analysis.
Linear regression is also a type of machine learning algorithm, more specifically a supervised algorithm, that learns from the labelled datasets and maps the data points to the most optimized linear functions that can be used for prediction on new datasets. 
Linear regression was the first type of regression analysis to be studied rigorously, and to be used extensively in practical applications. This is because models which depend linearly on their unknown parameters are easier to fit than models which are non-linearly related to their parameters and because the statistical properties of the resulting estimators are easier to determine.
Linear regression has many practical uses."
"In machine learning, hyperparameter optimization or tuning is the problem of choosing a set of optimal hyperparameters for a learning algorithm. A hyperparameter is a parameter whose value is used to control the learning process, which must be configured before the process starts.
Hyperparameter optimization determines the set of hyperparameters that yields an optimal model which minimizes a predefined loss function on a given data set.  The objective function takes a set of hyperparameters and returns the associated loss. Cross-validation is often used to estimate this generalization performance, and therefore choose the set of values for hyperparameters that maximize it.


== Approaches ==


=== Grid search ===
The traditional method for hyperparameter optimization has been grid search, or a parameter sweep, which is simply an exhaustive searching through a manually specified subset of the hyperparameter space of a learning algorithm. A grid search algorithm must be guided by some performance metric, typically measured by cross-validation on the training set
or evaluation on a hold-out validation set.
Since the parameter space of a machine learner may include real-valued or unbounded value spaces for certain parameters, manually set bounds and discretization may be necessary before applying grid search.
For example, a typical soft-margin SVM classifier equipped with an RBF kernel has at least two hyperparameters that need to be tuned for good performance on unseen data: a regularization constant C and a kernel hyperparameter γ. Both parameters are continuous, so to perform grid search, one selects a finite set of ""reasonable"" values for each, say

  
    
      
        C
        ∈
        {
        10
        ,
        100
        ,
        1000
        }
      
    
    {\displaystyle C\in \{10,100,1000\}}
  

  
    
      
        γ
        ∈
        {
        0.1
        ,
        0.2
        ,
        0.5
        ,
        1.0
        }
      
    
    {\displaystyle \gamma \in \{0.1,0.2,0.5,1.0\}}
  

Grid search then trains an SVM with each pair (C, γ) in the Cartesian product of these two sets and evaluates their performance on a held-out validation set (or by internal cross-validation on the training set, in which case multiple SVMs are trained per pair)."
"Zero-shot learning (ZSL) is a problem setup in deep learning where, at test time, a learner observes samples from classes which were not observed during training, and needs to predict the class that they belong to. The name is a play on words based on the earlier concept of one-shot learning, in which classification can be learned from only one, or a few, examples.
Zero-shot methods generally work by associating observed and non-observed classes through some form of auxiliary information, which encodes observable distinguishing properties of objects. For example, given a set of images of animals to be classified, along with auxiliary textual descriptions of what animals look like, an artificial intelligence model which has been trained to recognize horses, but has never been given a zebra, can still recognize a zebra when it also knows that zebras look like striped horses. This problem is widely studied in computer vision, natural language processing, and machine perception.


== Background and history ==
The first paper on zero-shot learning in natural language processing appeared in a 2008 paper by Chang, Ratinov, Roth, and Srikumar, at the AAAI’08, but the name given to the learning paradigm there was dataless classification. The first paper on zero-shot learning in computer vision appeared at the same conference, under the name zero-data learning. The term zero-shot learning itself first appeared in the literature in a 2009 paper from Palatucci, Hinton, Pomerleau, and Mitchell at NIPS’09. This terminology was repeated later in another computer vision paper and the term zero-shot learning caught on, as a take-off on one-shot learning that was introduced in computer vision years earlier. 
In computer vision, zero-shot learning models learned parameters for seen classes along with their class representations and rely on representational similarity among class labels so that, during inference, instances can be classified into new classes."
"""Penny Lane"" is a song by the English rock band the Beatles that was released as a double A-side single with ""Strawberry Fields Forever"" in February 1967. It was written primarily by Paul McCartney and credited to the Lennon–McCartney songwriting partnership. The lyrics refer to Penny Lane, a street in Liverpool, and make mention of the sights and characters that McCartney recalled from his upbringing in the city.
The Beatles began recording ""Penny Lane"" in December 1966, intending it as a song for their album Sgt. Pepper's Lonely Hearts Club Band. Instead, after it was issued as a single to satisfy record company demand for a new release, the band adhered to their policy of omitting previously released singles from their albums. The song features numerous modulations that occur mid-verse and between its choruses. Session musician David Mason played a piccolo trumpet solo for its bridge section.
""Penny Lane"" was a top-five hit across Europe and topped the US Billboard Hot 100. In Britain, it was the first Beatles single since ""Please Please Me"" in 1963 to fail to reach number 1 on the Record Retailer chart."
"A cigar is a rolled bundle of dried and fermented tobacco leaves made to be smoked. Cigars are produced in a variety of sizes and shapes. Since the 20th century, almost all cigars are made of three distinct components: the filler, the binder leaf which holds the filler together, and a wrapper leaf, which is often the highest quality leaf used. Often there will be a cigar band printed with the cigar manufacturer's logo. Modern cigars can come with two or more bands, especially Cuban cigars, showing Limited Edition (Edición Limitada) bands displaying the year of production.
Cigar tobacco is grown in significant quantities primarily in Brazil, Central America (Costa Rica, Ecuador, Guatemala, Honduras, Mexico, Nicaragua, and Panama), and the islands of the Caribbean (Cuba, the Dominican Republic, Haiti, and Puerto Rico); it is also produced in the Eastern United States (mostly in Florida, Kentucky, Tennessee, and Virginia) and in the Mediterranean countries of Italy, Greece, Spain (in the Canary Islands), and Turkey, and to a lesser degree in Indonesia and the Philippines of Southeast Asia.
Cigar smoking carries serious health risks, including increased risk of developing various types and subtypes of cancers, respiratory diseases, cardiovascular diseases, cerebrovascular diseases, periodontal diseases, teeth decay and loss, and malignant diseases. In the United States, the tobacco industry and cigar brands have aggressively targeted African Americans and Non-Hispanic Whites with customized advertising techniques and tobacco-related lifestyle magazines since the 1990s.


== Etymology ==
The word cigar originally derives from the Mayan sikar (""to smoke rolled tobacco leaves""—from si'c, ""tobacco""). The Spanish word, ""cigarro"" spans the gap between the Mayan and modern use."
"TensorFlow is a software library for machine learning and artificial intelligence. It can be used across a range of tasks, but is used mainly for training and inference of neural networks. It is one of the most popular deep learning frameworks, alongside others such as PyTorch and PaddlePaddle. It is free and open-source software released under the Apache License 2.0.
It was developed by the Google Brain team for Google's internal use in research and production. The initial version was released under the Apache License 2.0 in 2015. Google released an updated version, TensorFlow 2.0, in September 2019.
TensorFlow can be used in a wide variety of programming languages, including Python, JavaScript, C++, and Java, facilitating its use in a range of applications in many sectors.


== History ==


=== DistBelief ===
Starting in 2011, Google Brain built DistBelief as a proprietary machine learning system based on deep learning neural networks. Its use grew rapidly across diverse Alphabet companies in both research and commercial applications."
"In statistics, mean absolute error (MAE) is a measure of errors between paired observations expressing the same phenomenon. Examples of Y versus X include comparisons of predicted versus observed, subsequent time versus initial time, and one technique of measurement versus an alternative technique of measurement. MAE is calculated as the sum of absolute errors (i.e., the Manhattan distance) divided by the sample size:
  
    
      
        
          M
          A
          E
        
        =
        
          
            
              
                ∑
                
                  i
                  =
                  1
                
                
                  n
                
              
              
                |
                
                  
                    y
                    
                      i
                    
                  
                  −
                  
                    x
                    
                      i
                    
                  
                
                |
              
            
            n
          
        
        =
        
          
            
              
                ∑
                
                  i
                  =
                  1
                
                
                  n
                
              
              
                |
                
                  e
                  
                    i
                  
                
                |
              
            
            n
          
        
        .
      
    
    {\displaystyle \mathrm {MAE} ={\frac {\sum _{i=1}^{n}\left|y_{i}-x_{i}\right|}{n}}={\frac {\sum _{i=1}^{n}\left|e_{i}\right|}{n}}.}
  
It is thus an arithmetic average of the absolute errors 
  
    
      
        
          |
        
        
          e
          
            i
          
        
        
          |
        
        =
        
          |
        
        
          y
          
            i
          
        
        −
        
          x
          
            i
          
        
        
          |
        
      
    
    {\displaystyle |e_{i}|=|y_{i}-x_{i}|}
  
, where 
  
    
      
        
          y
          
            i
          
        
      
    
    {\displaystyle y_{i}}
  
 is the prediction and 
  
    
      
        
          x
          
            i
          
        
      
    
    {\displaystyle x_{i}}
  
 the true value. Alternative formulations may include relative frequencies as weight factors. The mean absolute error uses the same scale as the data being measured. This is known as a scale-dependent accuracy measure and therefore cannot be used to make comparisons between predicted values that use different scales. The mean absolute error is a common measure of forecast error in time series analysis, sometimes used in confusion with the more standard definition of mean absolute deviation. The same confusion exists more generally.


== Quantity disagreement and allocation disagreement ==

In remote sensing the MAE is sometimes expressed as the sum of two components: quantity disagreement and allocation disagreement."
"A generative adversarial network (GAN) is a class of machine learning frameworks and a prominent framework for approaching generative artificial intelligence. The concept was initially developed by Ian Goodfellow and his colleagues in June 2014. In a GAN, two neural networks contest with each other in the form of a zero-sum game, where one agent's gain is another agent's loss.
Given a training set, this technique learns to generate new data with the same statistics as the training set. For example, a GAN trained on photographs can generate new photographs that look at least superficially authentic to human observers, having many realistic characteristics. Though originally proposed as a form of generative model for unsupervised learning, GANs have also proved useful for semi-supervised learning, fully supervised learning, and reinforcement learning.
The core idea of a GAN is based on the ""indirect"" training through the discriminator, another neural network that can tell how ""realistic"" the input seems, which itself is also being updated dynamically. This means that the generator is not trained to minimize the distance to a specific image, but rather to fool the discriminator. This enables the model to learn in an unsupervised manner.
GANs are similar to mimicry in evolutionary biology, with an evolutionary arms race between both networks."
"A feedforward neural network (FNN) is one of the two broad types of artificial neural network, characterized by direction of the flow of information between its layers. Its flow is uni-directional, meaning that the information in the model flows in only one direction—forward—from the input nodes, through the hidden nodes (if any) and to the output nodes, without any cycles or loops (in contrast to recurrent neural networks, which have a bi-directional flow). Modern feedforward networks are trained using backpropagation, and are colloquially referred to as ""vanilla"" neural networks.


== Mathematical foundations ==


=== Activation function ===
The two historically common activation functions are both sigmoids, and are described by

  
    
      
        y
        (
        
          v
          
            i
          
        
        )
        =
        tanh
        ⁡
        (
        
          v
          
            i
          
        
        )
         
         
        
          
            and
          
        
         
         
        y
        (
        
          v
          
            i
          
        
        )
        =
        (
        1
        +
        
          e
          
            −
            
              v
              
                i
              
            
          
        
        
          )
          
            −
            1
          
        
      
    
    {\displaystyle y(v_{i})=\tanh(v_{i})~~{\textrm {and}}~~y(v_{i})=(1+e^{-v_{i}})^{-1}}
  
.
The first is a hyperbolic tangent that ranges from -1 to 1, while the other is the logistic function, which is similar in shape but ranges from 0 to 1. Here 
  
    
      
        
          y
          
            i
          
        
      
    
    {\displaystyle y_{i}}
  
 is the output of the 
  
    
      
        i
      
    
    {\displaystyle i}
  
th node (neuron) and 
  
    
      
        
          v
          
            i
          
        
      
    
    {\displaystyle v_{i}}
  
 is the weighted sum of the input connections. Alternative activation functions have been proposed, including the rectifier and softplus functions. More specialized activation functions include radial basis functions (used in radial basis networks, another class of supervised neural network models).
In recent developments of deep learning the rectified linear unit (ReLU) is more frequently used as one of the possible ways to overcome the numerical problems related to the sigmoids.


=== Learning ===
Learning occurs by changing connection weights after each piece of data is processed, based on the amount of error in the output compared to the expected result."
"Attention is a machine learning method that determines the relative importance of each component in a sequence relative to the other components in that sequence. In natural language processing, importance is represented by ""soft"" weights assigned to each word in a sentence. More generally, attention encodes vectors called token embeddings across a fixed-width sequence that can range from tens to millions of tokens in size.
Unlike ""hard"" weights, which are computed during the backwards training pass, ""soft"" weights exist only in the forward pass and therefore change with every step of the input. Earlier designs implemented the attention mechanism in a serial recurrent neural network language translation system, but a more recent design, namely the transformer, removed the slower sequential RNN and relied more heavily on the faster parallel attention scheme.
Inspired by ideas about attention in humans, the attention mechanism was developed to address the weaknesses of leveraging information from the hidden layers of recurrent neural networks. Recurrent neural networks favor more recent information contained in words at the end of a sentence, while information earlier in the sentence tends to be attenuated. Attention allows a token equal access to any part of a sentence directly, rather than only through the previous state.


== History ==

Academic reviews of the history of the attention mechanism are provided in Niu et al. and Soydaner."
"A virtual assistant (VA) is a software agent that can perform a range of tasks or services for a user based on user input such as commands or questions, including verbal ones. Such technologies often incorporate chatbot capabilities to simulate human conversation, such as via online chat, to facilitate interaction with their users. The interaction may be via text, graphical interface, or voice - as some virtual assistants are able to interpret human speech and respond via synthesized voices.
In many cases, users can ask their virtual assistants questions, control home automation devices and media playback, and manage other basic tasks such as email, to-do lists, and calendars - all with verbal commands. In recent years, prominent virtual assistants for direct consumer use have included Apple's Siri, Amazon Alexa, Google Assistant, and Samsung's Bixby. Also, companies in various industries often incorporate some kind of virtual assistant technology into their customer service or support.
Into the 2020s, the emergence of artificial intelligence based chatbots, such as ChatGPT, has brought increased capability and interest to the field of virtual assistant products and services.


== History ==


=== Experimental decades: 1910s–1980s ===
Radio Rex was the first voice activated toy, patented in 1916 and released in 1922. It was a wooden toy in the shape of a dog that would come out of its house when its name is called.
In 1952, Bell Labs presented ""Audrey"", the Automatic Digit Recognition machine."
"In statistics, naive Bayes classifiers are a family of linear ""probabilistic classifiers"" which assumes that the features are conditionally independent, given the target class. The strength (naivety) of this assumption is what gives the classifier its name. These classifiers are among the simplest Bayesian network models.
Naive Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables (features/predictors) in a learning problem. Maximum-likelihood training can be done by evaluating a closed-form expression,: 718  which takes linear time, rather than by expensive iterative approximation as used for many other types of classifiers.
In the statistics literature, naive Bayes models are known under a variety of names, including simple Bayes and independence Bayes. All these names reference the use of Bayes' theorem in the classifier's decision rule, but naive Bayes is not (necessarily) a Bayesian method.


== Introduction ==
Naive Bayes is a simple technique for constructing classifiers: models that assign class labels to problem instances, represented as vectors of feature values, where the class labels are drawn from some finite set. There is not a single algorithm for training such classifiers, but a family of algorithms based on a common principle: all naive Bayes classifiers assume that the value of a particular feature is independent of the value of any other feature, given the class variable. For example, a fruit may be considered to be an apple if it is red, round, and about 10 cm in diameter."
"In computer science, Monte Carlo tree search (MCTS) is a heuristic search algorithm for some kinds of decision processes, most notably those employed in software that plays board games. In that context MCTS is used to solve the game tree.
MCTS was combined with neural networks in 2016 and has been used in multiple board games like Chess, Shogi, Checkers, Backgammon, Contract Bridge, Go, Scrabble, and Clobber as well as in turn-based-strategy video games (such as Total War: Rome II's implementation in the high level campaign AI) and applications outside of games.


== History ==


=== Monte Carlo method ===
The Monte Carlo method, which uses random sampling for deterministic problems which are difficult or impossible to solve using other approaches, dates back to the 1940s. In his 1987 PhD thesis, Bruce Abramson combined minimax search with an expected-outcome model based on random game playouts to the end, instead of the usual static evaluation function. Abramson said the expected-outcome model ""is shown to be precise, accurate, easily estimable, efficiently calculable, and domain-independent.""  He experimented in-depth with tic-tac-toe and then with machine-generated evaluation functions for Othello and chess.
Such methods were then explored and successfully applied to heuristic search in the field of automated theorem proving by W. Ertel, J. Schumann and C. Suttner in 1989, thus improving the exponential search times of uninformed search algorithms such as e.g. breadth-first search, depth-first search or iterative deepening.
In 1992, B. Brügmann employed it for the first time in a Go-playing program. In 2002, Chang et al."
"In statistics and machine learning, leakage (also known as data leakage or target leakage) is the use of information in the model training process which would not be expected to be available at prediction time, causing the predictive scores (metrics) to overestimate the model's utility when run in a production environment.
Leakage is often subtle and indirect, making it hard to detect and eliminate. Leakage can cause a statistician or modeler to select a suboptimal model, which could be outperformed by a leakage-free model.


== Leakage modes ==
Leakage can occur in many steps in the machine learning process. The leakage causes can be sub-classified into two possible sources of leakage for a model: features and training examples.


=== Feature leakage ===
Feature or column-wise leakage is caused by the inclusion of columns which are one of the following: a duplicate label, a proxy for the label, or the label itself. These features, known as anachronisms, will not be available when the model is used for predictions, and result in leakage if included when the model is trained.
For example, including a ""MonthlySalary"" column when predicting ""YearlySalary""; or ""MinutesLate"" when predicting ""IsLate"".


=== Training example leakage ===
Row-wise leakage is caused by improper sharing of information between rows of data. Types of row-wise leakage include:

Premature featurization; leaking from premature featurization before Cross-validation/Train/Test split (must fit MinMax/ngrams/etc on only the train split, then transform the test set)
Duplicate rows between train/validation/test (e.g."
"Computer vision tasks include methods for acquiring, processing, analyzing, and understanding digital images, and extraction of high-dimensional data from the real world in order to produce numerical or symbolic information, e.g. in the form of decisions. ""Understanding"" in this context signifies the transformation of visual images (the input to the retina) into descriptions of the world that make sense to thought processes and can elicit appropriate action. This image understanding can be seen as the disentangling of symbolic information from image data using models constructed with the aid of geometry, physics, statistics, and learning theory.
The scientific discipline of computer vision is concerned with the theory behind artificial systems that extract information from images. Image data can take many forms, such as video sequences, views from multiple cameras, multi-dimensional data from a 3D scanner, 3D point clouds from LiDaR sensors, or medical scanning devices. The technological discipline of computer vision seeks to apply its theories and models to the construction of computer vision systems.
Subdisciplines of computer vision include scene reconstruction, object detection, event detection, activity recognition, video tracking, object recognition, 3D pose estimation, learning, indexing, motion estimation, visual servoing, 3D scene modeling, and image restoration.


== Definition ==
Computer vision is an interdisciplinary field that deals with how computers can be made to gain high-level understanding from digital images or videos. From the perspective of engineering, it seeks to automate tasks that the human visual system can do."
"In statistics and machine learning, ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone.
Unlike a statistical ensemble in statistical mechanics, which is usually infinite, a machine learning ensemble consists of only a concrete finite set of alternative models, but typically allows for much more flexible structure to exist among those alternatives.


== Overview ==
Supervised learning algorithms search through a hypothesis space to find a suitable hypothesis that will make good predictions with a particular problem. Even if this space contains hypotheses that are very well-suited for a particular problem, it may be very difficult to find a good one. Ensembles combine multiple hypotheses to form one which should be theoretically better.
Ensemble learning trains two or more machine learning algorithms on a specific classification or regression task. The algorithms within the ensemble model are generally referred as ""base models"", ""base learners"", or ""weak learners"" in literature. These base models can be constructed using a single modelling algorithm, or several different algorithms. The idea is to train a diverse set of weak models on the same modelling task, such that the outputs of each weak learner have poor predictive ability (i.e., high bias), and among all weak learners, the outcome and error values exhibit high variance. Fundamentally, an ensemble learning model trains at least two high-bias (weak) and high-variance (diverse) models to be combined into a better-performing model."
"A neural network is a group of interconnected units called neurons that send signals to one another. Neurons can be either biological cells or mathematical models. While individual neurons are simple, many of them together in a network can perform complex tasks. There are two main types of neural network.

In neuroscience, a biological neural network is a physical structure found in brains and complex nervous systems – a population of nerve cells connected by synapses.
In machine learning, an artificial neural network is a mathematical model used to approximate nonlinear functions. Artificial neural networks are used to solve artificial intelligence problems.


== In biology ==

In the context of biology, a neural network is a population of biological neurons chemically connected to each other by synapses. A given neuron can be connected to hundreds of thousands of synapses.
Each neuron sends and receives electrochemical signals called action potentials to its connected neighbors."
"Standing, also referred to as orthostasis, is a position in which the body is held in an upright (orthostatic) position and supported only by the feet. Although seemingly static, the body rocks slightly back and forth from the ankle in the sagittal plane, which bisects the body into right and left sides. The sway of quiet standing is often likened to the motion of an inverted pendulum.
Standing at attention is a military standing posture, as is stand at ease, but these terms are also used in military-style organisations and in some professions which involve standing, such as modeling. At ease refers to the classic military position of standing with legs slightly apart, not in as formal or regimented a pose as standing at attention. In modeling, model at ease refers to the model standing with one leg straight, with the majority of the weight on it, and the other leg tucked over and slightly around. There may be a time when a person is standing, where they lose control due to an external force or lack of energy, where they accelerate to the ground due to gravity. This is known as ""falling"" and may result in injuries around the part of the body that made contact with the ground.


== Control ==
Standing posture relies on dynamic rather than static balance. The human center of mass is in front of the ankle, and unlike in tetrapods, the base of support is narrow, consisting of only two feet."
"Gradient boosting is a machine learning technique based on boosting in a functional space, where the target is pseudo-residuals instead of residuals as in traditional boosting. It gives a prediction model in the form of an ensemble of weak prediction models, i.e., models that make very few assumptions about the data, which are typically simple decision trees. When a decision tree is the weak learner, the resulting algorithm is called gradient-boosted trees; it usually outperforms random forest. As with other boosting methods, a gradient-boosted trees model is built in stages, but it generalizes the other methods by allowing optimization of an arbitrary differentiable loss function.


== History ==
The idea of gradient boosting originated in the observation by Leo Breiman that boosting can be interpreted as an optimization algorithm on a suitable cost function. Explicit regression gradient boosting algorithms were subsequently developed, by Jerome H. Friedman, (in 1999 and later in 2001) simultaneously with the more general functional gradient boosting perspective of Llew Mason, Jonathan Baxter, Peter Bartlett and Marcus Frean.
The latter two papers introduced the view of boosting algorithms as iterative functional gradient descent algorithms. That is, algorithms that optimize a cost function over function space by iteratively choosing a function (weak hypothesis) that points in the negative gradient direction. This functional gradient view of boosting has led to the development of boosting algorithms in many areas of machine learning and statistics beyond regression and classification.


== Informal introduction ==
(This section follows the exposition by Cheng Li.)
Like other boosting methods, gradient boosting combines weak ""learners"" into a single strong learner iteratively."
"Gradient descent is a method for unconstrained mathematical optimization. It is a first-order iterative algorithm for minimizing a differentiable multivariate function.
The idea is to take repeated steps in the opposite direction of the gradient (or approximate gradient) of the function at the current point, because this is the direction of steepest descent. Conversely, stepping in the direction of the gradient will lead to a trajectory that maximizes that function; the procedure is then known as gradient ascent.
It is particularly useful in machine learning for minimizing the cost or loss function. Gradient descent should not be confused with local search algorithms, although both are iterative methods for optimization.
Gradient descent is generally attributed to Augustin-Louis Cauchy, who first suggested it in 1847. Jacques Hadamard independently proposed a similar method in 1907. Its convergence properties for non-linear optimization problems were first studied by Haskell Curry in 1944, with the method becoming increasingly well-studied and used in the following decades.
A simple extension of gradient descent, stochastic gradient descent, serves as the most basic algorithm used for training most deep networks today."
"In data mining and statistics, hierarchical clustering (also called hierarchical cluster analysis or HCA) is a method of cluster analysis that seeks to build a hierarchy of clusters. Strategies for hierarchical clustering generally fall into two categories:

Agglomerative: This is a ""bottom-up"" approach: Each observation starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy.
Divisive: This is a ""top-down"" approach: All observations start in one cluster, and splits are performed recursively as one moves down the hierarchy.
In general, the merges and splits are determined in a greedy manner. The results of hierarchical clustering are usually presented in a dendrogram.
Hierarchical clustering has the distinct advantage that any valid measure of distance can be used. In fact, the observations themselves are not required: all that is used is a matrix of distances. On the other hand, except for the special case of single-linkage distance, none of the algorithms (except exhaustive search in 
  
    
      
        
          
            O
          
        
        (
        
          2
          
            n
          
        
        )
      
    
    {\displaystyle {\mathcal {O}}(2^{n})}
  
) can be guaranteed to find the optimum solution.


== Complexity ==
The standard algorithm for hierarchical agglomerative clustering (HAC) has a time complexity of 
  
    
      
        
          
            O
          
        
        (
        
          n
          
            3
          
        
        )
      
    
    {\displaystyle {\mathcal {O}}(n^{3})}
  
 and requires 
  
    
      
        Ω
        (
        
          n
          
            2
          
        
        )
      
    
    {\displaystyle \Omega (n^{2})}
  
 memory, which makes it too slow for even medium data sets. However, for some special cases, optimal efficient agglomerative methods (of complexity 
  
    
      
        
          
            O
          
        
        (
        
          n
          
            2
          
        
        )
      
    
    {\displaystyle {\mathcal {O}}(n^{2})}
  
) are known: SLINK for single-linkage and CLINK for complete-linkage clustering."
"Algorithmic bias describes systematic and repeatable errors in a computer system that create ""unfair"" outcomes, such as ""privileging"" one category over another in ways different from the intended function of the algorithm.
Bias can emerge from many factors, including but not limited to the design of the algorithm or the unintended or unanticipated use or decisions relating to the way data is coded, collected, selected or used to train the algorithm. For example, algorithmic bias has been observed in search engine results and social media platforms. This bias can have impacts ranging from inadvertent privacy violations to reinforcing social biases of race, gender, sexuality, and ethnicity. The study of algorithmic bias is most concerned with algorithms that reflect ""systematic and unfair"" discrimination. This bias has only recently been addressed in legal frameworks, such as the European Union's General Data Protection Regulation (proposed 2018) and the Artificial Intelligence Act (proposed 2021, approved 2024).
As algorithms expand their ability to organize society, politics, institutions, and behavior, sociologists have become concerned with the ways in which unanticipated output and manipulation of data can impact the physical world. Because algorithms are often considered to be neutral and unbiased, they can inaccurately project greater authority than human expertise (in part due to the psychological phenomenon of automation bias), and in some cases, reliance on algorithms can displace human responsibility for their outcomes. Bias can enter into algorithmic systems as a result of pre-existing cultural, social, or institutional expectations; by how features and labels are chosen; because of technical limitations of their design; or by being used in unanticipated contexts or by audiences who are not considered in the software's initial design.
Algorithmic bias has been cited in cases ranging from election outcomes to the spread of online hate speech."
"In time series analysis, dynamic time warping (DTW) is an algorithm for measuring similarity between two temporal sequences, which may vary in speed.  For instance, similarities in walking could be detected using DTW, even if one person was walking faster than the other, or if there were accelerations and decelerations during the course of an observation. DTW has been applied to temporal sequences of video, audio, and graphics data — indeed, any data that can be turned into a one-dimensional sequence can be analyzed with DTW. A well-known application has been automatic speech recognition, to cope with different speaking speeds. Other applications include speaker recognition and online signature recognition. It can also be used in partial shape matching applications.
In general, DTW is a method that calculates an optimal match between two given sequences (e.g. time series) with certain restriction and rules:

Every index from the first sequence must be matched with one or more indices from the other sequence, and vice versa
The first index from the first sequence must be matched with the first index from the other sequence (but it does not have to be its only match)
The last index from the first sequence must be matched with the last index from the other sequence (but it does not have to be its only match)
The mapping of the indices from the first sequence to indices from the other sequence must be monotonically increasing, and vice versa, i.e. if 
  
    
      
        j
        >
        i
      
    
    {\displaystyle j>i}
  
 are indices from the first sequence, then there must not be two indices 
  
    
      
        l
        >
        k
      
    
    {\displaystyle l>k}
  
 in the other sequence, such that index 
  
    
      
        i
      
    
    {\displaystyle i}
  
 is matched with index 
  
    
      
        l
      
    
    {\displaystyle l}
  
 and index 
  
    
      
        j
      
    
    {\displaystyle j}
  
 is matched with index 
  
    
      
        k
      
    
    {\displaystyle k}
  
, and vice versa
We can plot each match between the sequences 
  
    
      
        1
        :
        M
      
    
    {\displaystyle 1:M}
  
 and 
  
    
      
        1
        :
        N
      
    
    {\displaystyle 1:N}
  
 as a path in a 
  
    
      
        M
        ×
        N
      
    
    {\displaystyle M\times N}
  
 matrix from 
  
    
      
        (
        1
        ,
        1
        )
      
    
    {\displaystyle (1,1)}
  
 to 
  
    
      
        (
        M
        ,
        N
        )
      
    
    {\displaystyle (M,N)}
  
, such that each step is one of 
  
    
      
        (
        0
        ,
        1
        )
        ,
        (
        1
        ,
        0
        )
        ,
        (
        1
        ,
        1
        )
      
    
    {\displaystyle (0,1),(1,0),(1,1)}
  
. In this formulation, we see that the number of possible matches is the Delannoy number.
The optimal match is denoted by the match that satisfies all the restrictions and the rules and that has the minimal cost, where the cost is computed as the sum of absolute differences, for each matched pair of indices, between their values."
"Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus perform tasks without explicit instructions. Advances in the field of deep learning have allowed neural networks to surpass many previous approaches in performance.
ML finds application in many fields, including natural language processing, computer vision, speech recognition, email filtering, agriculture, and medicine. The application of ML to business problems is known as predictive analytics.
Statistics and mathematical optimization (mathematical programming) methods comprise the foundations of machine learning. Data mining is a related field of study, focusing on exploratory data analysis (EDA) via unsupervised learning. 
From a theoretical viewpoint, probably approximately correct (PAC) learning provides a framework for describing machine learning.


== History ==

The term machine learning was coined in 1959 by Arthur Samuel, an IBM employee and pioneer in the field of computer gaming and artificial intelligence. The synonym self-teaching computers was also used in this time period.
Although the earliest machine learning model was introduced in the 1950s when Arthur Samuel invented a program that calculated the winning chance in checkers for each side, the history of machine learning roots back to decades of human desire and effort to study human cognitive processes."
"Bayesian inference ( BAY-zee-ən or  BAY-zhən) is a method of statistical inference in which Bayes' theorem is used to calculate a probability of a hypothesis, given prior evidence, and update it as more information becomes available. Fundamentally, Bayesian inference uses a prior distribution to estimate posterior probabilities. Bayesian inference is an important technique in statistics, and especially in mathematical statistics. Bayesian updating is particularly important in the dynamic analysis of a sequence of data. Bayesian inference has found application in a wide range of activities, including science, engineering, philosophy, medicine, sport, and law. In the philosophy of decision theory, Bayesian inference is closely related to subjective probability, often called ""Bayesian probability"".


== Introduction to Bayes' rule ==


=== Formal explanation ===

Bayesian inference derives the posterior probability as a consequence of two antecedents: a prior probability and a ""likelihood function"" derived from a statistical model for the observed data. Bayesian inference computes the posterior probability according to Bayes' theorem:

  
    
      
        P
        (
        H
        ∣
        E
        )
        =
        
          
            
              P
              (
              E
              ∣
              H
              )
              ⋅
              P
              (
              H
              )
            
            
              P
              (
              E
              )
            
          
        
        ,
      
    
    {\displaystyle P(H\mid E)={\frac {P(E\mid H)\cdot P(H)}{P(E)}},}
  

where

  
    
      
        H
      
    
    {\displaystyle H}
  
 stands for any hypothesis whose probability may be affected by data (called evidence below). Often there are competing hypotheses, and the task is to determine which is the most probable.

  
    
      
        P
        (
        H
        )
      
    
    {\displaystyle P(H)}
  
, the prior probability, is the estimate of the probability of the hypothesis 
  
    
      
        H
      
    
    {\displaystyle H}
  
 before the data 
  
    
      
        E
      
    
    {\displaystyle E}
  
, the current evidence, is observed."
"Heroku is a cloud platform as a service (PaaS) supporting several programming languages. As one of the first cloud platforms, Heroku has been in development since June 2007, when it supported only the Ruby programming language, but now also supports Java, Node.js, Scala, Clojure, Python, PHP, and Go. For this reason, Heroku is said to be a polyglot platform as it has features for a developer to build, run and scale applications in a similar manner across most of these languages.  Heroku was acquired by Salesforce in 2010 for $212 million.


== History ==
Heroku was initially developed by James Lindenbaum, Adam Wiggins, and Orion Henry for supporting projects that were compatible with the Ruby programming platform Rack. The prototype development took around six months. Later on, Heroku faced setbacks because of a lack of proper market customers as many app developers used their own tools and environment. In January 2009, a new platform was launched which was built almost from scratch after a three-month effort. In October 2009, Byron Sebastian joined Heroku as CEO. On December 8, 2010, Salesforce.com acquired Heroku as a wholly owned subsidiary of Salesforce.com. On July 12, 2011, Yukihiro ""Matz"" Matsumoto, the chief designer of the Ruby programming language, joined the company as Chief Architect for Ruby."
"In data analysis, anomaly detection (also referred to as outlier detection and sometimes as novelty detection) is generally understood to be the identification of rare items, events or observations which deviate significantly from the majority of the data and do not conform to a well defined notion of normal behavior. Such examples may arouse suspicions of being generated by a different mechanism, or appear inconsistent with the remainder of that set of data.
Anomaly detection finds application in many domains including cybersecurity, medicine, machine vision, statistics, neuroscience, law enforcement and financial fraud to name only a few. Anomalies were initially searched for clear rejection or omission from the data to aid statistical analysis, for example to compute the mean or standard deviation. They were also removed to better predictions from models such as linear regression, and more recently their removal aids the performance of machine learning algorithms. However, in many applications anomalies themselves are of interest and are the observations most desirous in the entire data set, which need to be identified and separated from noise or irrelevant outliers.
Three broad categories of anomaly detection techniques exist. Supervised anomaly detection techniques require a data set that has been labeled as ""normal"" and ""abnormal"" and involves training a classifier. However, this approach is rarely used in anomaly detection due to the general unavailability of labelled data and the inherent unbalanced nature of the classes. Semi-supervised anomaly detection techniques assume that some portion of the data is labelled."
"Microsoft Azure, or just Azure (/ˈæʒər, ˈeɪʒər/ AZH-ər, AY-zhər, UK also /ˈæzjʊər, ˈeɪzjʊər/ AZ-ure, AY-zure), is the cloud computing platform developed by Microsoft. It has management, access and development of applications and services to individuals, companies, and governments through its global infrastructure. It also provides capabilities that are usually not included within other cloud platforms, including software as a service (SaaS), platform as a service (PaaS), and infrastructure as a service (IaaS). Microsoft Azure supports many programming languages, tools, and frameworks, including Microsoft-specific and third-party software and systems.
Azure was first introduced at the Professional Developers Conference (PDC) in October 2008 under the codename ""Project Red Dog"". It was officially launched as Windows Azure in February 2010 and later renamed to Microsoft Azure on March 25, 2014.


== Services ==
Microsoft Azure uses large-scale virtualization at Microsoft data centers worldwide and offers more than 600 services.


=== Computer services ===
Virtual machines, infrastructure as a service (IaaS), allowing users to launch general-purpose Microsoft Windows and Linux virtual machines, software as a service (SaaS), as well as preconfigured machine images for popular software packages.
Starting in 2022, these virtual machines are now powered by Ampere Cloud-native processors.
Most users run Linux on Azure, some of the many Linux distributions offered, including Microsoft's own Linux-based Azure Sphere."
"Ridge regression is a method of estimating the coefficients of multiple-regression models in scenarios where the independent variables are highly correlated. It has been used in many fields including econometrics, chemistry, and engineering. Also known as Tikhonov regularization, named for Andrey Tikhonov, it is a method of regularization of ill-posed problems. It is particularly useful to mitigate the problem of multicollinearity in linear regression, which commonly occurs in models with large numbers of parameters. In general, the method provides improved efficiency in parameter estimation problems in exchange for a tolerable amount of bias (see bias–variance tradeoff).
The theory was first introduced by Hoerl and Kennard in 1970 in their Technometrics papers ""Ridge regressions: biased estimation of nonorthogonal problems"" and ""Ridge regressions: applications in nonorthogonal problems"". 
Ridge regression was developed as a possible solution to the imprecision of least square estimators when linear regression models have some multicollinear (highly correlated) independent variables—by creating a ridge regression estimator (RR). This provides a more precise ridge parameters estimate, as its variance and mean square estimator are often smaller than the least square estimators previously derived.


== Overview ==
In the simplest case, the problem of a near-singular moment matrix 
  
    
      
        
          
            X
          
          
            
              T
            
          
        
        
          X
        
      
    
    {\displaystyle \mathbf {X} ^{\mathsf {T}}\mathbf {X} }
  
 is alleviated by adding positive elements to the diagonals, thereby decreasing its condition number. Analogous to the ordinary least squares estimator, the simple ridge estimator is then given by 

  
    
      
        
          
            
              
                β
                ^
              
            
          
          
            R
          
        
        =
        
          
            (
            
              
                
                  X
                
                
                  
                    T
                  
                
              
              
                X
              
              +
              λ
              
                I
              
            
            )
          
          
            −
            1
          
        
        
          
            X
          
          
            
              T
            
          
        
        
          y
        
      
    
    {\displaystyle {\hat {\beta }}_{R}=\left(\mathbf {X} ^{\mathsf {T}}\mathbf {X} +\lambda \mathbf {I} \right)^{-1}\mathbf {X} ^{\mathsf {T}}\mathbf {y} }
  

where 
  
    
      
        
          y
        
      
    
    {\displaystyle \mathbf {y} }
  
 is the regressand, 
  
    
      
        
          X
        
      
    
    {\displaystyle \mathbf {X} }
  
 is the design matrix, 
  
    
      
        
          I
        
      
    
    {\displaystyle \mathbf {I} }
  
 is the identity matrix, and the ridge parameter 
  
    
      
        λ
        ≥
        0
      
    
    {\displaystyle \lambda \geq 0}
  
 serves as the constant shifting the diagonals of the moment matrix."
"TensorFlow is a software library for machine learning and artificial intelligence. It can be used across a range of tasks, but is used mainly for training and inference of neural networks. It is one of the most popular deep learning frameworks, alongside others such as PyTorch and PaddlePaddle. It is free and open-source software released under the Apache License 2.0.
It was developed by the Google Brain team for Google's internal use in research and production. The initial version was released under the Apache License 2.0 in 2015. Google released an updated version, TensorFlow 2.0, in September 2019.
TensorFlow can be used in a wide variety of programming languages, including Python, JavaScript, C++, and Java, facilitating its use in a range of applications in many sectors.


== History ==


=== DistBelief ===
Starting in 2011, Google Brain built DistBelief as a proprietary machine learning system based on deep learning neural networks. Its use grew rapidly across diverse Alphabet companies in both research and commercial applications."
"Artificial intelligence (AI), in its broadest sense, is intelligence exhibited by machines, particularly computer systems. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals. Such machines may be called AIs.
Some high-profile applications of AI include advanced web search engines (e.g., Google Search); recommendation systems (used by YouTube, Amazon, and Netflix); interacting via human speech (e.g., Google Assistant, Siri, and Alexa); autonomous vehicles (e.g., Waymo); generative and creative tools (e.g., ChatGPT, and AI art); and superhuman play and analysis in strategy games (e.g., chess and Go). However, many AI applications are not perceived as AI: ""A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.""
The various subfields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include reasoning, knowledge representation, planning, learning, natural language processing, perception, and support for robotics. General intelligence—the ability to complete any task performable by a human on an at least equal level—is among the field's long-term goals. To reach these goals, AI researchers have adapted and integrated a wide range of techniques, including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, operations research, and economics. AI also draws upon psychology, linguistics, philosophy, neuroscience, and other fields.
Artificial intelligence was founded as an academic discipline in 1956, and the field went through multiple cycles of optimism, followed by periods of disappointment and loss of funding, known as AI winter."
"A receiver operating characteristic curve, or ROC curve, is a graphical plot that illustrates the performance of a binary classifier model (can be used for multi class classification as well) at varying threshold values.
The ROC curve is the plot of the true positive rate (TPR) against the false positive rate (FPR) at each threshold setting.
The ROC can also be thought of as a plot of the statistical power as a function of the Type I Error of the decision rule (when the performance is calculated from just a sample of the population, it can be thought of as estimators of these quantities). The ROC curve is thus the sensitivity as a function of false positive rate.
Given that the probability distributions for both true positive and false positive are known, the ROC curve is obtained as the cumulative distribution function (CDF, area under the probability distribution from 
  
    
      
        −
        ∞
      
    
    {\displaystyle -\infty }
  
 to the discrimination threshold) of the detection probability in the y-axis versus the CDF of the false positive probability on the x-axis.
ROC analysis provides tools to select possibly optimal models and to discard suboptimal ones independently from (and prior to specifying) the cost context or the class distribution. ROC analysis is related in a direct and natural way to the cost/benefit analysis of diagnostic decision making.


== Terminology ==
The true-positive rate is also known as sensitivity or probability of detection. The false-positive rate is also known as the probability of false alarm and equals (1 − specificity). 
The ROC is also known as a relative operating characteristic curve, because it is a comparison of two operating characteristics (TPR and FPR) as the criterion changes."
"Transfer learning (TL) is a technique in machine learning (ML) in which knowledge learned from a task is re-used in order to boost performance on a related task. For example, for image classification, knowledge gained while learning to recognize cars could be applied when trying to recognize trucks. This topic is related to the psychological literature on transfer of learning, although practical ties between the two fields are limited. Reusing/transferring information from previously learned tasks to new tasks has the potential to significantly improve learning efficiency.
Since transfer learning makes use of training with multiple objective functions it is related to cost-sensitive machine learning and multi-objective optimization.


== History ==
In 1976, Bozinovski and Fulgosi published a paper addressing transfer learning in neural network training. The paper gives a mathematical and geometrical model of the topic. In 1981, a report considered the application of transfer learning to a dataset of images representing letters of computer terminals, experimentally demonstrating positive and negative transfer learning.
In 1992, Lorien Pratt formulated the discriminability-based transfer (DBT) algorithm.
By 1998, the field had advanced to include multi-task learning, along with more formal theoretical foundations."
"Document classification or document categorization is a problem in library science, information science and computer science. The task is to assign a document to one or more classes or categories. This may be done ""manually"" (or ""intellectually"") or algorithmically. The intellectual classification of documents has mostly been the province of library science, while the algorithmic classification of documents is mainly in information science and computer science. The problems are overlapping, however, and there is therefore interdisciplinary research on document classification.
The documents to be classified may be texts, images, music, etc. Each kind of document possesses its special classification problems. When not otherwise specified, text classification is implied.
Documents may be classified according to their subjects or according to other attributes (such as document type, author, printing year etc.). In the rest of this article only subject classification is considered."
"BigQuery is a managed, serverless data warehouse product by Google, offering scalable analysis over large quantities of data. It is a Platform as a Service (PaaS) that supports querying using a dialect of SQL. It also has built-in machine learning capabilities. BigQuery was announced in May 2010 and made generally available in November 2011.


== History ==
Bigquery originated from Google's internal Dremel technology, which enabled quick queries across trillions of rows of data. The product was originally announced in May 2010 at Google I/O. Initially, it was only usable by a limited number of external early adopters due to limitations on the API. However, after the product proved its potential, it was released for limited availability in 2011 and general availability in 2012. After general availability, BigQuery found success among a broad range of customers, including airlines, insurance, and retail organizations. 


== Design ==
BigQuery requires all requests to be authenticated, supporting a number of Google-proprietary mechanisms as well as OAuth.


== Features ==
Managing data - Create and delete objects such as tables, views, and user defined functions. Import data from Google Storage in formats such as CSV, Parquet, Avro or JSON.
Query - Queries are expressed in a SQL dialect and the results are returned in JSON with a maximum reply length of approximately 128 MB, or an unlimited size when large query results are enabled.
Integration - BigQuery can be used from Google Apps Script (e.g."
"Explainable AI (XAI), often overlapping with interpretable AI, or explainable machine learning (XML), either refers to an artificial intelligence (AI) system over which it is possible for humans to retain intellectual oversight, or refers to the methods to achieve this. The main focus is usually on the reasoning behind the decisions or predictions made by the AI which are made more understandable and transparent. XAI counters the ""black box"" tendency of machine learning, where even the AI's designers cannot explain why it arrived at a specific decision.
XAI hopes to help users of AI-powered systems perform more effectively by improving their understanding of how those systems reason. XAI may be an implementation of the social right to explanation. Even if there is no such legal right or regulatory requirement, XAI can improve the user experience of a product or service by helping end users trust that the AI is making good decisions. XAI aims to explain what has been done, what is being done, and what will be done next, and to unveil which information these actions are based on. This makes it possible to confirm existing knowledge, challenge existing knowledge, and generate new assumptions.
Machine learning (ML) algorithms used in AI can be categorized as white-box or black-box. White-box models provide results that are understandable to experts in the domain."
"A decision tree  is a decision support recursive partitioning structure that uses a tree-like model of decisions and their possible consequences, including chance event outcomes, resource costs, and utility. It is one way to display an algorithm that only contains conditional control statements.
Decision trees are commonly used in operations research, specifically in decision analysis, to help identify a strategy most likely to reach a goal, but are also a popular tool in machine learning.


== Overview ==
A decision tree is a flowchart-like structure in which each internal node represents a ""test"" on an attribute (e.g. whether a coin flip comes up heads or tails), each branch represents the outcome of the test, and each leaf node represents a class label (decision taken after computing all attributes). The paths from root to leaf represent classification rules.
In decision analysis, a decision tree and the closely related influence diagram are used as a visual and analytical decision support tool, where the expected values (or expected utility) of competing alternatives are calculated.
A decision tree consists of three types of nodes:

Decision nodes – typically represented by squares
Chance nodes – typically represented by circles
End nodes – typically represented by triangles
Decision trees are commonly used in operations research and operations management. If, in practice, decisions have to be taken online with no recall under incomplete knowledge, a decision tree should be paralleled by a probability model as a best choice model or online selection model algorithm. Another use of decision trees is as a descriptive means for calculating conditional probabilities."
"Feature scaling is a method used to normalize the range of independent variables or features of data. In data processing, it is also known as data normalization and is generally performed during the data preprocessing step.


== Motivation ==
Since the range of values of raw data varies widely, in some machine learning algorithms, objective functions will not work properly without normalization. For example, many classifiers calculate the distance between two points by the Euclidean distance. If one of the features has a broad range of values, the distance will be governed by this particular feature. Therefore, the range of all features should be normalized so that each feature contributes approximately proportionately to the final distance.
Another reason why feature scaling is applied is that gradient descent converges much faster with feature scaling than without it.
It's also important to apply feature scaling if regularization is used as part of the loss function (so that coefficients are penalized appropriately).
Empirically, feature scaling can improve the convergence speed of stochastic gradient descent. In support vector machines, it can reduce the time to find support vectors."
"A graphical model or probabilistic graphical model (PGM) or structured probabilistic model is a probabilistic model for which a graph expresses the conditional dependence structure between random variables. They are commonly used in probability theory, statistics—particularly Bayesian statistics—and machine learning.


== Types of graphical models ==
Generally, probabilistic graphical models use a graph-based representation as the foundation for encoding a  distribution over a multi-dimensional space and a graph that is a compact or factorized representation of a set of independences that hold in the specific distribution. Two branches of graphical representations of distributions are commonly used, namely, Bayesian networks and Markov random fields. Both families encompass the properties of factorization and independences, but they differ in the set of independences they can encode and the factorization of the distribution that they induce.


=== Undirected Graphical Model ===

The undirected graph shown may have one of several interpretations; the common feature is that the presence of an edge implies some sort of dependence between the corresponding random variables.  From this graph we might deduce that 
  
    
      
        B
        ,
        C
        ,
        D
      
    
    {\displaystyle B,C,D}
  
 are all mutually independent, once 
  
    
      
        A
      
    
    {\displaystyle A}
  
 is known, or (equivalently in this case) that 

  
    
      
        P
        [
        A
        ,
        B
        ,
        C
        ,
        D
        ]
        =
        
          f
          
            A
            B
          
        
        [
        A
        ,
        B
        ]
        ⋅
        
          f
          
            A
            C
          
        
        [
        A
        ,
        C
        ]
        ⋅
        
          f
          
            A
            D
          
        
        [
        A
        ,
        D
        ]
      
    
    {\displaystyle P[A,B,C,D]=f_{AB}[A,B]\cdot f_{AC}[A,C]\cdot f_{AD}[A,D]}
  

for some non-negative functions  
  
    
      
        
          f
          
            A
            B
          
        
        ,
        
          f
          
            A
            C
          
        
        ,
        
          f
          
            A
            D
          
        
      
    
    {\displaystyle f_{AB},f_{AC},f_{AD}}
  
.


=== Bayesian network ===

If the network structure of the model is a directed acyclic graph, the model represents a factorization of the joint probability of all random variables.  More precisely, if the events are 
  
    
      
        
          X
          
            1
          
        
        ,
        …
        ,
        
          X
          
            n
          
        
      
    
    {\displaystyle X_{1},\ldots ,X_{n}}
  
 then the joint probability satisfies

  
    
      
        P
        [
        
          X
          
            1
          
        
        ,
        …
        ,
        
          X
          
            n
          
        
        ]
        =
        
          ∏
          
            i
            =
            1
          
          
            n
          
        
        P
        [
        
          X
          
            i
          
        
        
          |
        
        
          pa
        
        (
        
          X
          
            i
          
        
        )
        ]
      
    
    {\displaystyle P[X_{1},\ldots ,X_{n}]=\prod _{i=1}^{n}P[X_{i}|{\text{pa}}(X_{i})]}
  

where 
  
    
      
        
          pa
        
        (
        
          X
          
            i
          
        
        )
      
    
    {\displaystyle {\text{pa}}(X_{i})}
  
 is the set of parents of node 
  
    
      
        
          X
          
            i
          
        
      
    
    {\displaystyle X_{i}}
  
 (nodes with edges directed towards 
  
    
      
        
          X
          
            i
          
        
      
    
    {\displaystyle X_{i}}
  
).  In other words, the joint distribution factors into a product of conditional distributions."
"In Euclidean geometry, a square is a regular quadrilateral, which means that it has four straight sides of equal length and four equal angles (90-degree angles, π/2 radian angles, or right angles). It can also be defined as a rectangle with two equal-length adjacent sides. It is the only regular polygon whose internal angle, central angle, and external angle are all equal (90°). A square with vertices ABCD would be denoted 
  
    
      
        ◻
      
    
    {\displaystyle \square }
  
 ABCD.


== Characterizations ==
A quadrilateral is a square if and only if it is any one of the following:

A rectangle with two adjacent equal sides
A rhombus with a right vertex angle
A rhombus with all angles equal
A parallelogram with one right vertex angle and two adjacent equal sides
A quadrilateral with four equal sides and four right angles
A quadrilateral where the diagonals are equal, and are the perpendicular bisectors of each other (i.e., a rhombus with equal diagonals)
A convex quadrilateral with successive sides a, b, c, d whose area is 
  
    
      
        A
        =
        
          
            
              1
              2
            
          
        
        (
        
          a
          
            2
          
        
        +
        
          c
          
            2
          
        
        )
        =
        
          
            
              1
              2
            
          
        
        (
        
          b
          
            2
          
        
        +
        
          d
          
            2
          
        
        )
        .
      
    
    {\displaystyle A={\tfrac {1}{2}}(a^{2}+c^{2})={\tfrac {1}{2}}(b^{2}+d^{2}).}
  
: Corollary 15 


== Properties ==
A square is a special case of a rhombus (equal sides, opposite equal angles), a kite (two pairs of adjacent equal sides), a trapezoid (one pair of opposite sides parallel), a parallelogram (all opposite sides parallel), a quadrilateral or tetragon (four-sided polygon), and a rectangle (opposite sides equal, right-angles), and therefore has all the properties of all these shapes, namely:

All four internal angles of a square are equal (each being 360°/4 = 90°, a right angle).
The central angle of a square is equal to 90° (360°/4).
The external angle of a square is equal to 90°.
The diagonals of a square are equal and bisect each other, meeting at 90°.
The diagonal of a square bisects its internal angle, forming adjacent angles of 45°.
All four sides of a square are equal."
"TensorFlow is a software library for machine learning and artificial intelligence. It can be used across a range of tasks, but is used mainly for training and inference of neural networks. It is one of the most popular deep learning frameworks, alongside others such as PyTorch and PaddlePaddle. It is free and open-source software released under the Apache License 2.0.
It was developed by the Google Brain team for Google's internal use in research and production. The initial version was released under the Apache License 2.0 in 2015. Google released an updated version, TensorFlow 2.0, in September 2019.
TensorFlow can be used in a wide variety of programming languages, including Python, JavaScript, C++, and Java, facilitating its use in a range of applications in many sectors.


== History ==


=== DistBelief ===
Starting in 2011, Google Brain built DistBelief as a proprietary machine learning system based on deep learning neural networks. Its use grew rapidly across diverse Alphabet companies in both research and commercial applications."
"Data augmentation is a statistical technique which allows maximum likelihood estimation from incomplete data. Data augmentation has important applications in Bayesian analysis, and the technique is widely used in machine learning to reduce overfitting when training machine learning models, achieved by training models on several slightly-modified copies of existing data.


== Synthetic oversampling techniques for traditional machine learning ==
Synthetic Minority Over-sampling Technique (SMOTE) is a method used to address imbalanced datasets in machine learning. In such datasets, the number of samples in different classes varies significantly, leading to biased model performance. For example, in a medical diagnosis dataset with 90 samples representing healthy individuals and only 10 samples representing individuals with a particular disease, traditional algorithms may struggle to accurately classify the minority class. SMOTE rebalances the dataset by generating synthetic samples for the minority class. For instance, if there are 100 samples in the majority class and 10 in the minority class, SMOTE can create synthetic samples by randomly selecting a minority class sample and its nearest neighbors, then generating new samples along the line segments joining these neighbors. This process helps increase the representation of the minority class, improving model performance.


== Data augmentation for image classification ==
When convolutional neural networks grew larger in mid-1990s, there was a lack of data to use, especially considering that some part of the overall dataset should be spared for later testing. It was proposed to perturb existing data with affine transformations to create new examples with the same labels, which were complemented by so-called elastic distortions in 2003, and the technique was widely used as of 2010s."
"Reinforcement learning (RL) is an interdisciplinary area of machine learning and optimal control concerned with how an intelligent agent should take actions in a dynamic environment in order to maximize a reward signal. Reinforcement learning is one of the three basic machine learning paradigms, alongside supervised learning and unsupervised learning.
Q-learning at its simplest stores data in tables. This approach becomes infeasible as the number of states/actions increases (e.g., if the state space or action space were continuous), as the probability of the agent visiting a particular state and performing a particular action diminishes. 
Reinforcement learning differs from supervised learning in not needing labelled input-output pairs to be presented, and in not needing sub-optimal actions to be explicitly corrected. Instead, the focus is on finding a balance between exploration (of uncharted territory) and exploitation (of current knowledge) with the goal of maximizing the cumulative reward (the feedback of which might be incomplete or delayed). The search for this balance is known as the exploration-exploitation dilemma.

The environment is typically stated in the form of a Markov decision process (MDP), as many reinforcement learning algorithms use dynamic programming techniques. The main difference between classical dynamic programming methods and reinforcement learning algorithms is that the latter do not assume knowledge of an exact mathematical model of the Markov decision process, and they target large MDPs where exact methods become infeasible. 


== Introduction ==

Due to its generality, reinforcement learning is studied in many disciplines, such as game theory, control theory, operations research, information theory, simulation-based optimization, multi-agent systems, swarm intelligence, and statistics."
"In statistics, the mean squared error (MSE) or mean squared deviation (MSD) of an estimator (of a procedure for estimating an unobserved quantity) measures the average of the squares of the errors—that is, the average squared difference between the estimated values and the actual value. MSE is a risk function, corresponding to the expected value of the squared error loss. The fact that MSE is almost always strictly positive (and not zero) is because of randomness or because the estimator does not account for information that could produce a more accurate estimate. In machine learning, specifically empirical risk minimization, MSE may refer to the empirical risk (the average loss on an observed data set), as an estimate of the true MSE (the true risk: the average loss on the actual population distribution).
The MSE is a measure of the quality of an estimator.  As it is derived from the square of Euclidean distance, it is always a positive value that decreases as the error approaches zero.
The MSE is the second moment (about the origin) of the error, and thus incorporates both the variance of the estimator (how widely spread the estimates are from one data sample to another) and its bias (how far off the average estimated value is from the true value). For an unbiased estimator, the MSE is the variance of the estimator. Like the variance, MSE has the same units of measurement as the square of the quantity being estimated. In an analogy to standard deviation, taking the square root of MSE yields the root-mean-square error or root-mean-square deviation (RMSE or RMSD), which has the same units as the quantity being estimated; for an unbiased estimator, the RMSE is the square root of the variance, known as the standard error."
"Video content analysis or video content analytics (VCA), also known as video analysis or video analytics (VA), is the capability of automatically analyzing video to detect and determine temporal and spatial events.
This technical capability is used in a wide range of domains including entertainment, video retrieval and video browsing, health-care, retail, automotive, transport, home automation, flame and smoke detection, safety, and security. The algorithms can be implemented as software on general-purpose machines, or as hardware in specialized video processing units.
Many different functionalities can be implemented in VCA. Video Motion Detection is one of the simpler forms where motion is detected with regard to a fixed background scene. More advanced functionalities include video tracking and egomotion estimation.
Based on the internal representation that VCA generates in the machine, it is possible to build other functionalities, such as video summarization, identification, behavior analysis, or other forms of situation awareness.
VCA relies on good input video, so it is often combined with video enhancement technologies such as video denoising, image stabilization, unsharp masking, and super-resolution.


== Functionalities ==
Several articles provide an overview of the modules involved in the development of video analytic applications. This is a list of known functionalities and a short description.


== Commercial applications ==
VCA is a relatively new technology, with numerous companies releasing VCA-enhanced products in the mid-2000s."
"Emo  is a music genre characterized by emotional, often confessional lyrics. It emerged as a style of hardcore punk and post-hardcore from the mid-1980s Washington, D.C. hardcore scene, where it was known as emotional hardcore or emocore. The bands Rites of Spring and Embrace, among others, pioneered the genre. In the early-to-mid 1990s, emo was adopted and reinvented by alternative rock, indie rock, punk rock, and pop-punk bands, including Sunny Day Real Estate, Jawbreaker, Cap'n Jazz, and Jimmy Eat World. By the mid-1990s, Braid, the Promise Ring, and the Get Up Kids emerged from Midwest emo, and several independent record labels began to specialize in the genre. Meanwhile, screamo, a more aggressive style of emo using screamed vocals, also emerged, pioneered by the San Diego bands Heroin and Antioch Arrow. Screamo achieved mainstream success in the 2000s with bands like Hawthorne Heights, Silverstein, Story of the Year, Thursday, the Used, and Underoath.
Often seen as a subculture, emo also signifies a specific relationship between fans and artists and certain aspects of fashion, culture, and behavior. Emo fashion includes skinny jeans, black eyeliner, tight t-shirts with band names, studded belts, and flat, straight, jet-black hair with long bangs. Since the early-to-mid 2000s, fans of emo music who dress like this are referred to as ""emo kids"" or ""emos""."
"A convolutional neural network (CNN) is a regularized type of feed-forward neural network that learns features by itself via filter (or kernel) optimization. This type of deep learning network has been applied to process and make predictions from many different types of data including text, images and audio. Convolution-based networks are the de-facto standard in deep learning-based approaches to computer vision and image processing, and have only recently have been replaced -- in some cases -- by newer deep learning architectures such as the transformer. Vanishing gradients and exploding gradients, seen during backpropagation in earlier neural networks, are prevented by using regularized weights over fewer connections. For example, for each neuron in the fully-connected layer, 10,000 weights would be required for processing an image sized 100 × 100 pixels. However, applying cascaded convolution (or cross-correlation) kernels, only 25 neurons are required to process 5x5-sized tiles. Higher-layer features are extracted from wider context windows, compared to lower-layer features.
Some applications of CNNs include: 

image and video recognition,
recommender systems,
image classification,
image segmentation,
medical image analysis,
natural language processing,
brain–computer interfaces, and
financial time series.
CNNs are also known as shift invariant or space invariant artificial neural networks, based on the shared-weight architecture of the convolution kernels or filters that slide along input features and provide translation-equivariant responses known as feature maps. Counter-intuitively, most convolutional neural networks are not invariant to translation, due to the downsampling operation they apply to the input."
"Monte Carlo methods, or Monte Carlo experiments, are a broad class of computational algorithms that rely on repeated random sampling to obtain numerical results. The underlying concept is to use randomness to solve problems that might be deterministic in principle. The name comes from the Monte Carlo Casino in Monaco, where the primary developer of the method, mathematician Stanislaw Ulam, was inspired by his uncle's gambling habits.
Monte Carlo methods are mainly used in three distinct problem classes: optimization, numerical integration, and generating draws from a probability distribution. They can also be used to model phenomena with significant uncertainty in inputs, such as calculating the risk of a nuclear power plant failure. Monte Carlo methods are often implemented using computer simulations, and they can provide approximate solutions to problems that are otherwise intractable or too complex to analyze mathematically.
Monte Carlo methods are widely used in various fields of science, engineering, and mathematics, such as physics, chemistry, biology, statistics, artificial intelligence, finance, and cryptography. They have also been applied to social sciences, such as sociology, psychology, and political science. Monte Carlo methods have been recognized as one of the most important and influential ideas of the 20th century, and they have enabled many scientific and technological breakthroughs.
Monte Carlo methods also have some limitations and challenges, such as the trade-off between accuracy and computational cost, the curse of dimensionality, the reliability of random number generators, and the verification and validation of the results."
"Cross-validation, sometimes called rotation estimation or out-of-sample testing, is any of various similar model validation techniques for assessing how the results of a statistical analysis will generalize to an independent data set.
Cross-validation includes resampling and sample splitting methods that use different portions of the data to test and train a model on different iterations. It is often used in settings where the goal is prediction, and one wants to estimate how accurately a predictive model will perform in practice. It can also be used to assess the quality of a fitted model and the stability of its parameters.
In a prediction problem, a model is usually given a dataset of known data on which training is run (training dataset), and a dataset of unknown data (or first seen data) against which the model is tested (called the validation dataset or testing set). The goal of cross-validation is to test the model's ability to predict new data that was not used in estimating it, in order to flag problems like overfitting or selection bias and to give an insight on how the model will generalize to an independent dataset (i.e., an unknown dataset, for instance from a real problem).
One round of cross-validation involves partitioning a sample of data into complementary subsets, performing the analysis on one subset (called the training set), and validating the analysis on the other subset (called the validation set or testing set). To reduce variability, in most methods multiple rounds of cross-validation are performed using different partitions, and the validation results are combined (e.g. averaged) over the rounds to give an estimate of the model's predictive performance.
In summary, cross-validation combines (averages) measures of fitness in prediction to derive a more accurate estimate of model prediction performance."
"A covert listening device, more commonly known as a bug or a wire, is usually a combination of a miniature radio transmitter with a microphone. The use of bugs, called bugging, or wiretapping is a common technique in surveillance, espionage and police investigations.
Self-contained electronic covert listening devices came into common use with intelligence agencies in the 1950s, when technology allowed for a suitable transmitter to be built into a relatively small package. By 1956, the US Central Intelligence Agency was designing and building ""Surveillance Transmitters"" that employed transistors, which greatly reduced the size and power consumption. With no moving parts and greater power efficiency, these solid-state devices could be operated by small batteries, which revolutionized the process of covert listening.
A bug does not have to be a device specifically designed for the purpose of eavesdropping. For instance, with the right equipment, it is possible to remotely activate the microphone of cellular phones, even when a call is not being made, to listen to conversations in the vicinity of the phone.


== Dictograph ==
Among the earliest covert listening devices used in the United States of America was the dictograph, an invention of Kelley M. Turner patented in 1906 (US Patent US843186A). It consisted of a microphone in one location and a remote listening post with a speaker that could also be recorded using a phonograph. While also marketed as a device that allowed broadcasting of sounds, or dictating text from one room to a typist in another, it was used in several criminal investigations."
"Attention is a machine learning method that determines the relative importance of each component in a sequence relative to the other components in that sequence. In natural language processing, importance is represented by ""soft"" weights assigned to each word in a sentence. More generally, attention encodes vectors called token embeddings across a fixed-width sequence that can range from tens to millions of tokens in size.
Unlike ""hard"" weights, which are computed during the backwards training pass, ""soft"" weights exist only in the forward pass and therefore change with every step of the input. Earlier designs implemented the attention mechanism in a serial recurrent neural network language translation system, but a more recent design, namely the transformer, removed the slower sequential RNN and relied more heavily on the faster parallel attention scheme.
Inspired by ideas about attention in humans, the attention mechanism was developed to address the weaknesses of leveraging information from the hidden layers of recurrent neural networks. Recurrent neural networks favor more recent information contained in words at the end of a sentence, while information earlier in the sentence tends to be attenuated. Attention allows a token equal access to any part of a sentence directly, rather than only through the previous state.


== History ==

Academic reviews of the history of the attention mechanism are provided in Niu et al. and Soydaner."
"In machine learning and statistics, the learning rate is a tuning parameter in an optimization algorithm that determines the step size at each iteration while moving toward a minimum of a loss function. Since it influences to what extent newly acquired information overrides old information, it metaphorically represents the speed at which a machine learning model ""learns"". In the adaptive control literature, the learning rate is commonly referred to as gain.
In setting a learning rate, there is a trade-off between the rate of convergence and overshooting. While the descent direction is usually determined from the gradient of the loss function, the learning rate determines how big a step is taken in that direction. A too high learning rate will make the learning jump over minima but a too low learning rate will either take too long to converge or get stuck in an undesirable local minimum.
In order to achieve faster convergence, prevent oscillations and getting stuck in undesirable local minima the learning rate is often varied during training either in accordance to a learning rate schedule or by using an adaptive learning rate. The learning rate and its adjustments may also differ per parameter, in which case it is a diagonal matrix that can be interpreted as an approximation to the inverse of the Hessian matrix in Newton's method. The learning rate is related to the step length determined by inexact line search in quasi-Newton methods and related optimization algorithms. 


== Learning rate schedule ==
Initial rate can be left as system default or can be selected using a range of techniques."
"In law, fraud is intentional deception to deprive a victim of a legal right or to gain from a victim unlawfully or unfairly. Fraud can violate civil law (e.g., a fraud victim may sue the fraud perpetrator to avoid the fraud or recover monetary compensation) or criminal law (e.g., a fraud perpetrator may be prosecuted and imprisoned by governmental authorities), or it may cause no loss of money, property, or legal right but still be an element of another civil or criminal wrong. The purpose of fraud may be monetary gain or other benefits, for example by obtaining a passport, travel document, or driver's license, or mortgage fraud, where the perpetrator may attempt to qualify for a mortgage by way of false statements.


== Terminology ==
Fraud can be defined as either a civil wrong or a criminal act. For civil fraud, a government agency or person or entity harmed by fraud may bring litigation to stop the fraud, seek monetary damages, or both. For criminal fraud, a person may be prosecuted for the fraud and potentially face fines, incarceration, or both.


=== Civil law ===
In common law jurisdictions, as a civil wrong, fraud is a tort. While the precise definitions and requirements of proof vary among jurisdictions, the requisite elements of fraud as a tort generally are the intentional misrepresentation or concealment of an important fact upon which the victim is meant to rely, and in fact does rely, to the harm of the victim. Proving fraud in a court of law is often said to be difficult as the intention to defraud is the key element in question. As such, proving fraud comes with a ""greater evidentiary burden than other civil claims""."
"These datasets are used in machine learning (ML) research and have been cited in peer-reviewed academic journals. Datasets are an integral part of the field of machine learning. Major advances in this field can result from advances in learning algorithms (such as deep learning), computer hardware, and, less-intuitively, the availability of high-quality training datasets. High-quality labeled training datasets for supervised and semi-supervised machine learning algorithms are usually difficult and expensive to produce because of the large amount of time needed to label the data. Although they do not need to be labeled, high-quality datasets for unsupervised learning can also be difficult and costly to produce.
Many organizations, including governments, publish and share their datasets. The datasets are classified, based on the licenses, as Open data and Non-Open data.
The datasets from various governmental-bodies are presented in List of open government data sites. The datasets are ported on open data portals. They are made available for searching, depositing and accessing through interfaces like Open API.  The datasets are made available as various sorted types and subtypes."
"In mathematics, statistics, finance, and computer science, particularly in machine learning and inverse problems, regularization is a process that converts the answer of a problem to a simpler one. It is often used in solving ill-posed problems or to prevent overfitting.
Although regularization procedures can be divided in many ways, the following delineation is particularly helpful:

Explicit regularization is regularization whenever one explicitly adds a term to the optimization problem.  These terms could be priors, penalties, or constraints. Explicit regularization is commonly employed with ill-posed optimization problems. The regularization term, or penalty, imposes a cost on the optimization function to make the optimal solution unique.
Implicit regularization is all other forms of regularization.  This includes, for example, early stopping, using a robust loss function, and discarding outliers. Implicit regularization is essentially ubiquitous in modern machine learning approaches, including stochastic gradient descent for training deep neural networks, and ensemble methods (such as random forests and gradient boosted trees).
In explicit regularization, independent of the problem or model, there is always a data term, that corresponds to a likelihood of the measurement and a regularization term that corresponds to a prior."
"Deep learning is a subset of machine learning that focuses on utilizing neural networks to perform tasks such as classification, regression, and representation learning. The field takes inspiration from biological neuroscience and is centered around stacking artificial neurons into layers and ""training"" them to process data. The adjective ""deep"" refers to the use of multiple layers (ranging from three to several hundred or thousands) in the network. Methods used can be either supervised, semi-supervised or unsupervised.
Some common deep learning network architectures include fully connected networks, deep belief networks, recurrent neural networks, convolutional neural networks, generative adversarial networks, transformers, and neural radiance fields. These architectures have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.
Early forms of neural networks were inspired by information processing and distributed communication nodes in biological systems, particularly the human brain. However, current neural networks do not intend to model the brain function of organisms, and are generally seen as low-quality models for that purpose.


== Overview ==
Most modern deep learning models are based on multi-layered neural networks such as convolutional neural networks and transformers, although they can also include propositional formulas or latent variables organized layer-wise in deep generative models such as the nodes in deep belief networks and deep Boltzmann machines.
Fundamentally, deep learning refers to a class of machine learning algorithms in which a hierarchy of layers is used to transform input data into a slightly more abstract and composite representation."
"Spiking neural networks (SNNs) are artificial neural networks (ANN) that more closely mimic natural neural networks. These models leverage timing of discrete spikes as the main information carrier.
In addition to neuronal and synaptic state, SNNs incorporate the concept of time into their operating model. The idea is that neurons in the SNN do not transmit information at each propagation cycle (as it happens with typical multi-layer perceptron networks), but rather transmit information only when a membrane potential—an intrinsic quality of the neuron related to its membrane electrical charge—reaches a specific value, called the threshold. When the membrane potential reaches the threshold, the neuron fires, and generates a signal that travels to other neurons which, in turn, increase or decrease their potentials in response to this signal. A neuron model that fires at the moment of threshold crossing is also called a spiking neuron model.
Although it was previously believed that the brain encoded information through spike rates, which can be considered as the analogue variable output of a traditional ANN, research in the field of neurobiology has indicated that high speed processing cannot solely be performed through a rate based scheme. For example humans can perform an image recognition task at rate requiring no more than 10ms of processing time per neuron through the successive layers (going from the retina to the temporal lobe). This time window is too short for a rate based encoding. The precise spike timings in a small set of spiking neurons also has a higher information coding capacity compared with a rate based approach."
"A hidden Markov model (HMM) is a Markov model in which the observations are dependent on a latent (or hidden) Markov process (referred to as 
  
    
      
        X
      
    
    {\displaystyle X}
  
). An HMM requires that there be an observable process 
  
    
      
        Y
      
    
    {\displaystyle Y}
  
 whose outcomes depend on the outcomes of 
  
    
      
        X
      
    
    {\displaystyle X}
  
 in a known way. Since 
  
    
      
        X
      
    
    {\displaystyle X}
  
 cannot be observed directly, the goal is to learn about state of 
  
    
      
        X
      
    
    {\displaystyle X}
  
 by observing 
  
    
      
        Y
      
    
    {\displaystyle Y}
  
. By definition of being a Markov model, an HMM has an additional requirement that the outcome of 
  
    
      
        Y
      
    
    {\displaystyle Y}
  
 at time 
  
    
      
        t
        =
        
          t
          
            0
          
        
      
    
    {\displaystyle t=t_{0}}
  
 must be ""influenced"" exclusively by the outcome of 
  
    
      
        X
      
    
    {\displaystyle X}
  
 at 
  
    
      
        t
        =
        
          t
          
            0
          
        
      
    
    {\displaystyle t=t_{0}}
  
 and that the outcomes of 
  
    
      
        X
      
    
    {\displaystyle X}
  
 and 
  
    
      
        Y
      
    
    {\displaystyle Y}
  
 at 
  
    
      
        t
        <
        
          t
          
            0
          
        
      
    
    {\displaystyle t<t_{0}}
  
 must be conditionally independent of 
  
    
      
        Y
      
    
    {\displaystyle Y}
  
 at 
  
    
      
        t
        =
        
          t
          
            0
          
        
      
    
    {\displaystyle t=t_{0}}
  
 given 
  
    
      
        X
      
    
    {\displaystyle X}
  
 at time 
  
    
      
        t
        =
        
          t
          
            0
          
        
      
    
    {\displaystyle t=t_{0}}
  
. Estimation of the parameters in an HMM can be performed using maximum likelihood estimation. For linear chain HMMs, the Baum–Welch algorithm can be used to estimate parameters.
Hidden Markov models are known for their applications to thermodynamics, statistical mechanics, physics, chemistry, economics, finance, signal processing, information theory, pattern recognition—such as speech, handwriting, gesture recognition, part-of-speech tagging, musical score following, partial discharges and bioinformatics.


== Definition ==
Let 
  
    
      
        
          X
          
            n
          
        
      
    
    {\displaystyle X_{n}}
  
 and 
  
    
      
        
          Y
          
            n
          
        
      
    
    {\displaystyle Y_{n}}
  
 be discrete-time stochastic processes and 
  
    
      
        n
        ≥
        1
      
    
    {\displaystyle n\geq 1}
  
. The pair 
  
    
      
        (
        
          X
          
            n
          
        
        ,
        
          Y
          
            n
          
        
        )
      
    
    {\displaystyle (X_{n},Y_{n})}
  
 is a hidden Markov model if

  
    
      
        
          X
          
            n
          
        
      
    
    {\displaystyle X_{n}}
  
 is a Markov process whose behavior is not directly observable (""hidden"");

  
    
      
        
          
            P
          
        
        ⁡
        
          
            (
          
        
        
          Y
          
            n
          
        
        ∈
        A
         
        
          
            |
          
        
         
        
          X
          
            1
          
        
        =
        
          x
          
            1
          
        
        ,
        …
        ,
        
          X
          
            n
          
        
        =
        
          x
          
            n
          
        
        
          
            )
          
        
        =
        
          
            P
          
        
        ⁡
        
          
            (
          
        
        
          Y
          
            n
          
        
        ∈
        A
         
        
          
            |
          
        
         
        
          X
          
            n
          
        
        =
        
          x
          
            n
          
        
        
          
            )
          
        
      
    
    {\displaystyle \operatorname {\mathbf {P} } {\bigl (}Y_{n}\in A\ {\bigl |}\ X_{1}=x_{1},\ldots ,X_{n}=x_{n}{\bigr )}=\operatorname {\mathbf {P} } {\bigl (}Y_{n}\in A\ {\bigl |}\ X_{n}=x_{n}{\bigr )}}
  
,
for every 
  
    
      
        n
        ≥
        1
      
    
    {\displaystyle n\geq 1}
  
, 
  
    
      
        
          x
          
            1
          
        
        ,
        …
        ,
        
          x
          
            n
          
        
      
    
    {\displaystyle x_{1},\ldots ,x_{n}}
  
, and every Borel set 
  
    
      
        A
      
    
    {\displaystyle A}
  
.
Let 
  
    
      
        
          X
          
            t
          
        
      
    
    {\displaystyle X_{t}}
  
 and 
  
    
      
        
          Y
          
            t
          
        
      
    
    {\displaystyle Y_{t}}
  
 be continuous-time stochastic processes."
"Stochastic gradient descent (often abbreviated SGD) is an iterative method for optimizing an objective function with suitable smoothness properties (e.g. differentiable or subdifferentiable). It can be regarded as a stochastic approximation of gradient descent optimization, since it replaces the actual gradient (calculated from the entire data set) by an estimate thereof (calculated from a randomly selected subset of the data). Especially in high-dimensional optimization problems this reduces the very high computational burden, achieving faster iterations in exchange for a lower convergence rate.
The basic idea behind stochastic approximation can be traced back to the Robbins–Monro algorithm of the 1950s. Today, stochastic gradient descent has become an important optimization method in machine learning.


== Background ==

Both statistical estimation and machine learning consider the problem of minimizing an objective function that has the form of a sum:

  
    
      
        Q
        (
        w
        )
        =
        
          
            1
            n
          
        
        
          ∑
          
            i
            =
            1
          
          
            n
          
        
        
          Q
          
            i
          
        
        (
        w
        )
        ,
      
    
    {\displaystyle Q(w)={\frac {1}{n}}\sum _{i=1}^{n}Q_{i}(w),}
  

where the parameter 
  
    
      
        w
      
    
    {\displaystyle w}
  
 that minimizes 
  
    
      
        Q
        (
        w
        )
      
    
    {\displaystyle Q(w)}
  
 is to be estimated. Each summand function 
  
    
      
        
          Q
          
            i
          
        
      
    
    {\displaystyle Q_{i}}
  
 is typically associated with the 
  
    
      
        i
      
    
    {\displaystyle i}
  
-th observation in the data set (used for training).
In classical statistics, sum-minimization problems arise in least squares and in maximum-likelihood estimation (for independent observations).  The general class of estimators that arise as minimizers of sums are called M-estimators."
"In mathematical logic, interpretability is a relation between formal theories that expresses the possibility of interpreting or translating one into the other.


== Informal definition ==
Assume T and S are formal theories. Slightly simplified, T is said to be interpretable in S if and only if the language of T can be translated into the language of S in such a way that S proves the translation of every theorem of T. Of course, there are some natural conditions on admissible translations here, such as the necessity for a translation to preserve the logical structure of formulas.
This concept, together with weak interpretability, was introduced by Alfred Tarski in 1953. Three other related concepts are cointerpretability, logical tolerance, and cotolerance, introduced by Giorgi Japaridze in 1992–93.


== See also ==
Interpretation (logic)
Interpretation (model theory)
Interpretability logic


== References ==
Japaridze, G., and De Jongh, D. (1998) ""The logic of provability"" in Buss, S., ed., Handbook of Proof Theory. North-Holland: 476–546.
Alfred Tarski, Andrzej Mostowski, and Raphael Robinson (1953) Undecidable Theories. North-Holland."
"Random forests or random decision forests is an ensemble learning method for classification, regression and other tasks that works by creating a multitude of decision trees during training. For classification tasks, the output of the random forest is the class selected by most trees. For regression tasks, the output is the average of the predictions of the trees. Random forests correct for decision trees' habit of overfitting to their training set.: 587–588 
The first algorithm for random decision forests was created in 1995 by Tin Kam Ho using the random subspace method, which, in Ho's formulation, is a way to implement the ""stochastic discrimination"" approach to classification proposed by Eugene Kleinberg.
An extension of the algorithm was developed by Leo Breiman and Adele Cutler, who registered ""Random Forests"" as a trademark in 2006 (as of 2019, owned by Minitab, Inc.). The extension combines Breiman's ""bagging"" idea and random selection of features, introduced first by Ho and later independently by Amit and Geman in order to construct a collection of decision trees with controlled variance.


== History ==
The general method of random decision forests was first proposed by Salzberg and Heath in 1993, with a method that used a randomized decision tree algorithm to create multiple trees and then combine them using majority voting.  This idea was developed further by Ho in 1995.  Ho established that forests of trees splitting with oblique hyperplanes can gain accuracy as they grow without suffering from overtraining, as long as the forests are randomly restricted to be sensitive to only selected feature dimensions.  A subsequent work along the same lines concluded that other splitting methods behave similarly, as long as they are randomly forced to be insensitive to some feature dimensions."
"The bag-of-words model (BoW) is a model of text which uses a representation of text that is based on an unordered collection (a ""bag"") of words. It is used in natural language processing and information retrieval (IR). It disregards word order (and thus most of syntax or grammar) but captures multiplicity.
The bag-of-words model is commonly used in methods of document classification where, for example, the (frequency of) occurrence of each word is used as a feature for training a classifier. It has also been used for computer vision.
An early reference to ""bag of words"" in a linguistic context can be found in Zellig Harris's 1954 article on Distributional Structure.


== Definition ==
The following models a text document using bag-of-words. Here are two simple text documents:

Based on these two text documents, a list is constructed as follows for each document:

Representing each bag-of-words as a JSON object, and attributing to the respective JavaScript variable:

Each key is the word, and each value is the number of occurrences of that word in the given text document.
The order of elements is free, so, for example {""too"":1,""Mary"":1,""movies"":2,""John"":1,""watch"":1,""likes"":2,""to"":1} is also equivalent to BoW1. It is also what we expect from a strict JSON object representation."
"Albumentations is an open-source image augmentation library created in June 2018 by a group of researchers and engineers, including Alexander Buslaev, Vladimir Iglovikov, and Alex Parinov. The library was designed to provide a flexible and efficient framework for data augmentation in computer vision tasks.
Data augmentation is a technique that involves artificially expanding the size of a dataset by creating new images through various transformations such as rotation, scaling, flipping, and color adjustments. This process helps improve the performance of machine learning models by providing a more diverse set of training examples.
Built on top of OpenCV, a widely used computer vision library, Albumentations provides high-performance implementations of various image processing functions. It also offers a rich set of image transformation functions and a simple API for combining them, allowing users to create custom augmentation pipelines tailored to their specific needs.


== Adoption ==
Albumentations has gained significant popularity and recognition in the computer vision and deep learning community since its introduction in 2018. The library was designed to provide a flexible and efficient framework for data augmentation in computer vision tasks, and has been widely adopted in academic research, open-source projects, and machine learning competitions.
The library's research paper, ""Albumentations: Fast and Flexible Image Augmentations,"" has received over 1000 citations, highlighting its importance and impact in the field of computer vision. The library has also been widely adopted in computer vision and deep learning projects, with over 12,000 packages depending on it as listed on its GitHub dependents page."
"A transformer is a deep learning architecture developed by researchers at Google and based on the multi-head attention mechanism, proposed in the 2017 paper ""Attention Is All You Need"". Text is converted to numerical representations called tokens, and each token is converted into a vector via lookup from a word embedding table. At each layer, each token is then contextualized within the scope of the context window with other (unmasked) tokens via a parallel multi-head attention mechanism, allowing the signal for key tokens to be amplified and less important tokens to be diminished.
Transformers have the advantage of having no recurrent units, therefore requiring less training time than earlier recurrent neural architectures (RNNs) such as long short-term memory (LSTM). Later variations have been widely adopted for training large language models (LLM) on large (language) datasets, such as the Wikipedia corpus and Common Crawl.

Transformers were first developed as an improvement over previous architectures for machine translation, but have found many applications since. They are used in large-scale natural language processing, computer vision (vision transformers), reinforcement learning, audio, multimodal learning, robotics, and even playing chess. It has also led to the development of pre-trained systems, such as generative pre-trained transformers (GPTs) and BERT (bidirectional encoder representations from transformers).


== History ==


=== Predecessors ===
For many years, sequence modelling and generation was done by using plain recurrent neural networks (RNNs). A well-cited early example was the Elman network (1990)."
"k-means clustering is a method of vector quantization, originally from signal processing, that aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean (cluster centers or cluster centroid), serving as a prototype of the cluster. This results in a partitioning of the data space into Voronoi cells. k-means clustering minimizes within-cluster variances (squared Euclidean distances), but not regular Euclidean distances, which would be the more difficult Weber problem: the mean optimizes squared errors, whereas only the geometric median minimizes Euclidean distances. For instance, better Euclidean solutions can be found using k-medians and k-medoids.
The problem is computationally difficult (NP-hard); however, efficient heuristic algorithms converge quickly to a local optimum. These are usually similar to the expectation–maximization algorithm for mixtures of Gaussian distributions via an iterative refinement approach employed by both k-means and Gaussian mixture modeling. They both use cluster centers to model the data; however, k-means clustering tends to find clusters of comparable spatial extent, while the Gaussian mixture model allows clusters to have different shapes.
The unsupervised k-means algorithm has a loose relationship to the k-nearest neighbor classifier, a popular supervised machine learning technique for classification that is often confused with k-means due to the name. Applying the 1-nearest neighbor classifier to the cluster centers obtained by k-means classifies new data into the existing clusters. This is known as nearest centroid classifier or Rocchio algorithm."
"Data cleansing or data cleaning is the process of identifying and correcting (or removing) corrupt, inaccurate, or irrelevant records from a dataset, table, or database. It involves detecting incomplete, incorrect, or inaccurate parts of the data and then replacing, modifying, or deleting the affected data. Data cleansing can be performed interactively using data wrangling tools, or through batch processing often via scripts or a data quality firewall.
After cleansing, a data set should be consistent with other similar data sets in the system. The inconsistencies detected or removed may have been originally caused by user entry errors, by corruption in transmission or storage, or by different data dictionary definitions of similar entities in different stores. Data cleaning differs from data validation in that validation almost invariably means data is rejected from the system at entry and is performed at the time of entry, rather than on batches of data.
The actual process of data cleansing may involve removing typographical errors or validating and correcting values against a known list of entities. The validation may be strict (such as rejecting any address that does not have a valid postal code), or with fuzzy or approximate string matching (such as correcting records that partially match existing, known records). Some data cleansing solutions will clean data by cross-checking with a validated data set. A common data cleansing practice is data enhancement, where data is made more complete by adding related information."
"A graph neural network (GNN) belongs to a class of artificial neural networks for processing data that can be represented as graphs.

In the more general subject of ""geometric deep learning"", certain existing neural network architectures can be interpreted as GNNs operating on suitably defined graphs. A convolutional neural network layer, in the context of computer vision, can be considered a GNN applied to graphs whose nodes are pixels and only adjacent pixels are connected by edges in the graph. A transformer layer, in natural language processing, can be considered a GNN applied to complete graphs whose nodes are words or tokens in a passage of natural language text.
The key design element of GNNs is the use of pairwise message passing, such that graph nodes iteratively update their representations by exchanging information with their neighbors. Several GNN architectures have been proposed, which implement different flavors of message passing,  started by recursive or convolutional constructive approaches. As of 2022, it is an open question whether it is possible to define GNN architectures ""going beyond"" message passing, or instead every GNN can be built on message passing over suitably defined graphs.
Relevant application domains for GNNs include natural language processing, social networks, citation networks, molecular biology, chemistry, physics and NP-hard combinatorial optimization problems.
Open source libraries implementing GNNs include PyTorch Geometric (PyTorch), TensorFlow GNN (TensorFlow), Deep Graph Library (framework agnostic), jraph (Google JAX), and GraphNeuralNetworks.jl/GeometricFlux.jl (Julia, Flux).


== Architecture ==
The architecture of a generic GNN implements the following fundamental layers:

Permutation equivariant: a permutation equivariant layer maps a representation of a graph into an updated representation of the same graph."
"In corpus linguistics, part-of-speech tagging (POS tagging or PoS tagging or POST), also called grammatical tagging is the process of marking up a word in a text (corpus) as corresponding to a particular part of speech, based on both its definition and its context.
A simplified form of this is commonly taught to school-age children, in the identification of words as nouns, verbs, adjectives, adverbs, etc.
Once performed by hand, POS tagging is now done in the context of computational linguistics, using algorithms which associate discrete terms, as well as hidden parts of speech, by a set of descriptive tags. POS-tagging algorithms fall into two distinctive groups: rule-based and stochastic. E. Brill's tagger, one of the first and most widely used English POS-taggers, employs rule-based algorithms.


== Principle ==

Part-of-speech tagging is harder than just having a list of words and their parts of speech, because some words can represent more than one part of speech at different times, and because some parts of speech are complex. This is not rare—in natural languages (as opposed to many artificial languages), a large percentage of word-forms are ambiguous. For example, even ""dogs"", which is usually thought of as just a plural noun, can also be a verb:

The sailor dogs the hatch.
Correct grammatical tagging will reflect that ""dogs"" is here used as a verb, not as the more common plural noun. Grammatical context is one way to determine this; semantic analysis can also be used to infer that ""sailor"" and ""hatch"" implicate ""dogs"" as 1) in the nautical context and 2) an action applied to the object ""hatch"" (in this context, ""dogs"" is a nautical term meaning ""fastens (a watertight door) securely"")."
"A recommender system (RecSys), or a recommendation system (sometimes replacing system with terms such as platform, engine, or algorithm), is a subclass of information filtering system that provides suggestions for items that are most pertinent to a particular user. Recommender systems are particularly useful when an individual needs to choose an item from a potentially overwhelming number of items that a service may offer.
Typically, the suggestions refer to various decision-making processes, such as what product to purchase, what music to listen to, or what online news to read.
Recommender systems are used in a variety of areas, with commonly recognised examples taking the form of playlist generators for video and music services, product recommenders for online stores, or content recommenders for social media platforms and open web content recommenders. These systems can operate using a single type of input, like music, or multiple inputs within and across platforms like news, books and search queries. There are also popular recommender systems for specific topics like restaurants and online dating. Recommender systems have also been developed to explore research articles and experts, collaborators, and financial services.
A content discovery platform is an implemented software recommendation platform which uses recommender system tools. It utilizes user metadata in order to discover and recommend appropriate content, whilst reducing ongoing maintenance and development costs. A content discovery platform delivers personalized content to websites, mobile devices and set-top boxes."
"Recurrent neural networks (RNNs) are a class of artificial neural network commonly used for sequential data processing. Unlike feedforward neural networks, which process data in a single pass, RNNs process data across multiple time steps, making them well-adapted for modelling and processing text, speech, and time series.
The building block of RNNs is the recurrent unit. This unit maintains a hidden state, essentially a form of memory, which is updated at each time step based on the current input and the previous hidden state. This feedback loop allows the network to learn from past inputs, and incorporate that knowledge into its current processing.
Early RNNs suffered from the vanishing gradient problem, limiting their ability to learn long-range dependencies. This was solved by the long short-term memory (LSTM) variant in 1997, thus making it the standard architecture for RNN.
RNNs have been applied to tasks such as unsegmented, connected handwriting recognition, speech recognition, natural language processing, and neural machine translation.


== History ==


=== Before modern ===
One origin of RNN was neuroscience. The word ""recurrent"" is used to describe loop-like structures in anatomy. In 1901, Cajal observed ""recurrent semicircles"" in the cerebellar cortex formed by parallel fiber, Purkinje cells, and granule cells."
"Neural style transfer (NST) refers to a class of software algorithms that manipulate digital images, or videos, in order to adopt the appearance or visual style of another image. NST algorithms are characterized by their use of deep neural networks for the sake of image transformation. Common uses for NST are the creation of artificial artwork from photographs, for example by transferring the appearance of famous paintings to user-supplied photographs.  Several notable mobile apps use NST techniques for this purpose, including DeepArt and Prisma. This method has been used by artists and designers around the globe to develop new artwork based on existent style(s).


== History ==
NST is an example of image stylization, a problem studied for over two decades within the field of non-photorealistic rendering.  The first two example-based style transfer algorithms were image analogies and image quilting. Both of these methods were based on patch-based texture synthesis algorithms.
Given a training pair of images–a photo and an artwork depicting that photo–a transformation could be learned and then applied to create new artwork from a new photo, by analogy. If no training photo was available, it would need to be produced by processing the input artwork; image quilting did not require this processing step, though it was demonstrated on only one style."
"Optical character recognition or optical character reader (OCR) is the electronic or mechanical conversion of images of typed, handwritten or printed text into machine-encoded text, whether from a scanned document, a photo of a document, a scene photo (for example the text on signs and billboards in a landscape photo) or from subtitle text superimposed on an image (for example: from a television broadcast).
Widely used as a form of data entry from printed paper data records – whether passport documents, invoices, bank statements, computerized receipts, business cards, mail, printed data, or any suitable documentation – it is a common method of digitizing printed texts so that they can be electronically edited, searched, stored more compactly, displayed online, and used in machine processes such as cognitive computing, machine translation, (extracted) text-to-speech, key data and text mining. OCR is a field of research in pattern recognition, artificial intelligence and computer vision.
Early versions needed to be trained with images of each character, and worked on one font at a time. Advanced systems capable of producing a high degree of accuracy for most fonts are now common, and with support for a variety of image file format inputs. Some systems are capable of reproducing formatted output that closely approximates the original page including images, columns, and other non-textual components.


== History ==

Early optical character recognition may be traced to technologies involving telegraphy and creating reading devices for the blind. In 1914, Emanuel Goldberg developed a machine that read characters and converted them into standard telegraph code. Concurrently, Edmund Fournier d'Albe developed the Optophone, a handheld scanner that when moved across a printed page, produced tones that corresponded to specific letters or characters.
In the late 1920s and into the 1930s, Emanuel Goldberg developed what he called a ""Statistical Machine"" for searching microfilm archives using an optical code recognition system."
"Question answering (QA) is a computer science discipline within the fields of information retrieval and natural language processing (NLP) that is concerned with building systems that automatically answer questions that are posed by humans in a natural language.


== Overview ==
A question-answering implementation, usually a computer program, may construct its answers by querying a structured database of knowledge or information, usually a knowledge base. More commonly, question-answering systems can pull answers from an unstructured collection of natural language documents.
Some examples of natural language document collections used for question answering systems include:

a local collection of reference texts
internal organization documents and web pages
compiled newswire reports
a set of Wikipedia pages
a subset of World Wide Web pages


== Types of question answering ==
Question-answering research attempts to develop ways of answering a wide range of question types, including fact, list, definition, how, why, hypothetical, semantically constrained, and cross-lingual questions.

Answering questions related to an article in order to evaluate reading comprehension is one of the simpler form of question answering, since a given article is relatively short compared to the domains of other types of question-answering problems. An example of such a question is ""What did Albert Einstein win the Nobel Prize for?"" after an article about this subject is given to the system.
Closed-book question answering is when a system has memorized some facts during training and can answer questions without explicitly being given a context. This is similar to humans taking closed-book exams.
Closed-domain question answering deals with questions under a specific domain (for example, medicine or automotive maintenance) and can exploit domain-specific knowledge frequently formalized in ontologies. Alternatively, ""closed-domain"" might refer to a situation where only a limited type of questions are accepted, such as questions asking for descriptive rather than procedural information."
"In statistics, the mean squared error (MSE) or mean squared deviation (MSD) of an estimator (of a procedure for estimating an unobserved quantity) measures the average of the squares of the errors—that is, the average squared difference between the estimated values and the actual value. MSE is a risk function, corresponding to the expected value of the squared error loss. The fact that MSE is almost always strictly positive (and not zero) is because of randomness or because the estimator does not account for information that could produce a more accurate estimate. In machine learning, specifically empirical risk minimization, MSE may refer to the empirical risk (the average loss on an observed data set), as an estimate of the true MSE (the true risk: the average loss on the actual population distribution).
The MSE is a measure of the quality of an estimator.  As it is derived from the square of Euclidean distance, it is always a positive value that decreases as the error approaches zero.
The MSE is the second moment (about the origin) of the error, and thus incorporates both the variance of the estimator (how widely spread the estimates are from one data sample to another) and its bias (how far off the average estimated value is from the true value). For an unbiased estimator, the MSE is the variance of the estimator. Like the variance, MSE has the same units of measurement as the square of the quantity being estimated. In an analogy to standard deviation, taking the square root of MSE yields the root-mean-square error or root-mean-square deviation (RMSE or RMSD), which has the same units as the quantity being estimated; for an unbiased estimator, the RMSE is the square root of the variance, known as the standard error."
"A globe is a spherical model of Earth, of some other celestial body, or of the celestial sphere. Globes serve purposes similar to maps, but, unlike maps, they do not distort the surface that they portray except to scale it down. A model globe of Earth is called a terrestrial globe. A model globe of the celestial sphere is called a celestial globe.
A globe shows details of its subject. A terrestrial globe shows landmasses and water bodies. It might show nations and major cities and the network of latitude and longitude lines. Some have raised relief to show mountains and other large landforms. A celestial globe shows notable stars, and may also show positions of other prominent astronomical objects. Typically, it will also divide the celestial sphere into constellations."
"In machine learning, diffusion models, also known as diffusion probabilistic models or score-based generative models, are a class of latent variable generative models. A diffusion model consists of three major components: the forward process, the reverse process, and the sampling procedure. The goal of diffusion models is to learn a diffusion process for a given dataset, such that the process can generate new elements that are distributed similarly as the original dataset. A diffusion model models data as generated by a diffusion process, whereby a new datum performs a random walk with drift through the space of all possible data. A trained diffusion model can be sampled in many ways, with different efficiency and quality.
There are various equivalent formalisms, including Markov chains, denoising diffusion probabilistic models, noise conditioned score networks, and stochastic differential equations. They are typically trained using variational inference. The model responsible for denoising is typically called its ""backbone"". The backbone may be of any kind, but they are typically U-nets or transformers.
As of 2024, diffusion models are mainly used for computer vision tasks, including image denoising, inpainting, super-resolution, image generation, and video generation."
"In digital image processing and computer vision, image segmentation is the process of partitioning a digital image into multiple image segments, also known as image regions or image objects (sets of pixels). The goal of segmentation is to simplify and/or change the representation of an image into something that is more meaningful and easier to analyze. Image segmentation is typically used to locate objects and boundaries (lines, curves, etc.) in images. More precisely, image segmentation is the process of assigning a label to every pixel in an image such that pixels with the same label share certain characteristics.
The result of image segmentation is a set of segments that collectively cover the entire image, or a set of contours extracted from the image (see edge detection). Each of the pixels in a region are similar with respect to some characteristic or computed property, such as color, intensity, or texture. Adjacent regions are significantly different with respect to the same characteristic(s). When applied to a stack of images, typical in medical imaging, the resulting contours after image segmentation can be used to create 3D reconstructions with the help of geometry reconstruction algorithms like marching cubes.


== Applications ==

Some of the practical applications of image segmentation are:

Content-based image retrieval
Machine vision
Medical imaging, and imaging studies in biomedical research, including volume rendered images from computed tomography, magnetic resonance imaging, as well as volume electron microscopy techniques such as FIB-SEM.
Locate tumors and other pathologies
Measure tissue volumes
Diagnosis, study of anatomical structure
Surgery planning
Virtual surgery simulation
Intra-surgery navigation
Radiotherapy
Object detection
Pedestrian detection
Face detection
Brake light detection
Locate objects in satellite images (roads, forests, crops, etc.)
Recognition Tasks
Face recognition
Fingerprint recognition
Iris recognition
Prohibited Item at Airport security checkpoints
Traffic control systems
Video surveillance
Video object co-segmentation and action localization
Several general-purpose algorithms and techniques have been developed for image segmentation. To be useful, these techniques must typically be combined with a domain's specific knowledge in order to effectively solve the domain's segmentation problems."
"In statistics, missing data, or missing values, occur when no data value is stored for the variable in an observation.  Missing data are a common occurrence and can have a significant effect on the conclusions that can be drawn from the data.
Missing data can occur because of nonresponse: no information is provided for one or more items or for a whole unit (""subject""). Some items are more likely to generate a nonresponse than others: for example items about private subjects such as income. Attrition is a type of missingness that can occur in longitudinal studies—for instance studying development where a measurement is repeated after a certain period of time. Missingness occurs when participants drop out before the test ends and one or more measurements are missing.
Data often are missing in research in economics, sociology, and political science because governments or private entities choose not to, or fail to, report critical statistics, or because the information is not available. Sometimes missing values are caused by the researcher—for example, when data collection is done improperly or mistakes are made in data entry.
These forms of missingness take different types, with different impacts on the validity of conclusions from research: Missing completely at random, missing at random, and missing not at random.  Missing data can be handled similarly as censored data."
"k-means clustering is a method of vector quantization, originally from signal processing, that aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean (cluster centers or cluster centroid), serving as a prototype of the cluster. This results in a partitioning of the data space into Voronoi cells. k-means clustering minimizes within-cluster variances (squared Euclidean distances), but not regular Euclidean distances, which would be the more difficult Weber problem: the mean optimizes squared errors, whereas only the geometric median minimizes Euclidean distances. For instance, better Euclidean solutions can be found using k-medians and k-medoids.
The problem is computationally difficult (NP-hard); however, efficient heuristic algorithms converge quickly to a local optimum. These are usually similar to the expectation–maximization algorithm for mixtures of Gaussian distributions via an iterative refinement approach employed by both k-means and Gaussian mixture modeling. They both use cluster centers to model the data; however, k-means clustering tends to find clusters of comparable spatial extent, while the Gaussian mixture model allows clusters to have different shapes.
The unsupervised k-means algorithm has a loose relationship to the k-nearest neighbor classifier, a popular supervised machine learning technique for classification that is often confused with k-means due to the name. Applying the 1-nearest neighbor classifier to the cluster centers obtained by k-means classifies new data into the existing clusters. This is known as nearest centroid classifier or Rocchio algorithm."
"Social stigma of obesity is bias or discriminatory behaviors targeted at overweight and obese individuals because of their weight and high body fat percentage. Such social stigmas can span one's entire life as long as excess weight is present, starting from a young age and lasting into adulthood. Studies also indicate overweight and obese individuals experience higher levels of stigma compared to other people. Stigmatization of obesity is usually associated with increased health risks (morbidity) of being overweight or obese and the possibility of a shorter lifespan (mortality).
Obese people marry less often, experience fewer educational and career opportunities, and on average earn a lesser income than normal weight individuals. Although public support regarding disability services, civil rights, and anti-workplace discrimination laws for obese individuals have gained support across the years, overweight and obese individuals still experience discrimination, which may have detrimental implications in relation to both physiological and psychological health. These issues are compounded by the significant negative physiological effects that are already associated with obesity, which some have proposed may be caused in part by stress from the social stigma of obesity (or which may be made more pronounced as a result of that stress). 
Anti-fat bias refers to prejudicial assumptions that are based on an assessment of a person as being overweight or obese. It is also known as ""fat shaming"" or ""fatphobia"". Anti-fat bias can be found in many facets of society, and fat activists commonly cite examples of mass media and popular culture that pervade this phenomenon."
"Common Crawl is a nonprofit 501(c)(3) organization that crawls the web and freely provides its archives and datasets to the public. Common Crawl's web archive consists of petabytes of data collected since 2008. It completes crawls generally every month.
Common Crawl was founded by Gil Elbaz. Advisors to the non-profit include Peter Norvig and Joi Ito. The organization's crawlers respect nofollow and robots.txt policies. Open source code for processing Common Crawl's data set is publicly available.
The Common Crawl dataset includes copyrighted work and is distributed from the US under fair use claims. Researchers in other countries have made use of techniques such as shuffling sentences or referencing the common crawl dataset to work around copyright law in other legal jurisdictions.
English is the primary language for 46% of documents in the March 2023 version of the Common Crawl dataset."
"In computer science and operations research, a genetic algorithm (GA) is a metaheuristic inspired by the process of natural selection that belongs to the larger class of evolutionary algorithms (EA). Genetic algorithms are commonly used to generate high-quality solutions to optimization and search problems via biologically inspired operators such as selection, crossover, and mutation. Some examples of GA applications include optimizing decision trees for better performance, solving sudoku puzzles, hyperparameter optimization, and causal inference.


== Methodology ==


=== Optimization problems ===
In a genetic algorithm, a population of candidate solutions (called individuals, creatures, organisms, or phenotypes) to an optimization problem is evolved toward better solutions. Each candidate solution has a set of properties (its chromosomes or genotype) which can be mutated and altered; traditionally, solutions are represented in binary as strings of 0s and 1s, but other encodings are also possible.
The evolution usually starts from a population of randomly generated individuals, and is an iterative process, with the population in each iteration called a generation. In each generation, the fitness of every individual in the population is evaluated; the fitness is usually the value of the objective function in the optimization problem being solved. The more fit individuals are stochastically selected from the current population, and each individual's genome is modified (recombined and possibly randomly mutated) to form a new generation. The new generation of candidate solutions is then used in the next iteration of the algorithm. Commonly, the algorithm terminates when either a maximum number of generations has been produced, or a satisfactory fitness level has been reached for the population."
"Named-entity recognition (NER) (also known as (named) entity identification, entity chunking, and entity extraction) is a subtask of information extraction that seeks to locate and classify named entities mentioned in unstructured text into pre-defined categories such as person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc.
Most research on NER/NEE systems has been structured as taking an unannotated block of text, such as this one:

Jim bought 300 shares of Acme Corp. in 2006.
And producing an annotated block of text that highlights the names of entities:

[Jim]Person bought 300 shares of [Acme Corp.]Organization in [2006]Time.
In this example, a person name consisting of one token, a two-token company name and a temporal expression have been detected and classified.
State-of-the-art NER systems for English produce near-human performance. For example, the best system entering MUC-7 scored 93.39% of F-measure while human annotators scored 97.60% and 96.95%.


== Named-entity recognition platforms ==
Notable NER platforms include:

GATE supports NER across many languages and domains out of the box, usable via a graphical interface and a Java API.
OpenNLP includes rule-based and statistical named-entity recognition.
SpaCy features fast statistical NER as well as an open-source named-entity visualizer.
Transformers features token classification using deep learning models."
"A stream is a continuous body of surface water flowing within the bed and banks of a channel. Depending on its location or certain characteristics, a stream may be referred to by a variety of local or regional names. Long, large streams are usually called rivers, while smaller, less voluminous and more intermittent streams are known as streamlets, brooks or creeks.
The flow of a stream is controlled by three inputs – surface runoff (from precipitation or meltwater), daylighted subterranean water, and surfaced groundwater (spring water). The surface and subterranean water are highly variable between periods of rainfall. Groundwater, on the other hand, has a relatively constant input and is controlled more by long-term patterns of precipitation. The stream encompasses surface, subsurface and groundwater fluxes that respond to geological, geomorphological, hydrological and biotic controls.
Streams are important as conduits in the water cycle, instruments in groundwater recharge, and corridors for fish and wildlife migration. The biological habitat in the immediate vicinity of a stream is called a riparian zone. Given the status of the ongoing Holocene extinction, streams play an important corridor role in connecting fragmented habitats and thus in conserving biodiversity."
"In statistics, naive Bayes classifiers are a family of linear ""probabilistic classifiers"" which assumes that the features are conditionally independent, given the target class. The strength (naivety) of this assumption is what gives the classifier its name. These classifiers are among the simplest Bayesian network models.
Naive Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables (features/predictors) in a learning problem. Maximum-likelihood training can be done by evaluating a closed-form expression,: 718  which takes linear time, rather than by expensive iterative approximation as used for many other types of classifiers.
In the statistics literature, naive Bayes models are known under a variety of names, including simple Bayes and independence Bayes. All these names reference the use of Bayes' theorem in the classifier's decision rule, but naive Bayes is not (necessarily) a Bayesian method.


== Introduction ==
Naive Bayes is a simple technique for constructing classifiers: models that assign class labels to problem instances, represented as vectors of feature values, where the class labels are drawn from some finite set. There is not a single algorithm for training such classifiers, but a family of algorithms based on a common principle: all naive Bayes classifiers assume that the value of a particular feature is independent of the value of any other feature, given the class variable. For example, a fruit may be considered to be an apple if it is red, round, and about 10 cm in diameter."
"Data wrangling, sometimes referred to as data munging, is the process of transforming and mapping data from one ""raw"" data form into another format with the intent of making it more appropriate and valuable for a variety of downstream purposes such as analytics. The goal of data wrangling is to assure quality and useful data. Data analysts typically spend the majority of their time in the process of data wrangling compared to the actual analysis of the data.
The process of data wrangling may include further munging, data visualization, data aggregation, training a statistical model, as well as many other potential uses. Data wrangling typically follows a set of general steps which begin with extracting the data in a raw form from the data source, ""munging"" the raw data (e.g. sorting) or parsing the data into predefined data structures, and finally depositing the resulting content into a data sink for storage and future use. It is closely aligned with the ETL process. 


== Background ==
The ""wrangler"" non-technical term is often said to derive from work done by the United States Library of Congress's National Digital Information Infrastructure and Preservation Program (NDIIPP) and their program partner the Emory University Libraries based MetaArchive Partnership. The term ""mung"" has roots in munging as described in the Jargon File. The term ""data wrangler"" was also suggested as the best analogy to describe someone working with data."
"The activation function of a node in an artificial neural network is a function that calculates the output of the node based on its individual inputs and their weights. Nontrivial problems can be solved using only a few nodes if the activation function is nonlinear. Modern activation functions include the smooth version of the ReLU, the GELU, which was used in the 2018 BERT model, the logistic (sigmoid) function used in the 2012 speech recognition model developed by Hinton et al, the ReLU used in the 2012 AlexNet computer vision model and in the 2015 ResNet model. 


== Comparison of activation functions ==
Aside from their empirical performance, activation functions also have different mathematical properties:

Nonlinear
When the activation function is non-linear, then a two-layer neural network can be proven to be a universal function approximator. This is known as the Universal Approximation Theorem. The identity activation function does not satisfy this property. When multiple layers use the identity activation function, the entire network is equivalent to a single-layer model.
Range
When the range of the activation function is finite, gradient-based training methods tend to be more stable, because pattern presentations significantly affect only limited weights. When the range is infinite, training is generally more efficient because pattern presentations significantly affect most of the weights. In the latter case, smaller learning rates are typically necessary."
"In deep learning, fine-tuning is an approach to transfer learning in which the parameters of a pre-trained neural network model are trained on new data. Fine-tuning can be done on the entire neural network, or on only a subset of its layers, in which case the layers that are not being fine-tuned are ""frozen"" (i.e., not changed during backpropagation). A model may also be augmented with ""adapters"" that consist of far fewer parameters than the original model, and fine-tuned in a parameter-efficient way by tuning the weights of the adapters and leaving the rest of the model's weights frozen.
For some architectures, such as convolutional neural networks, it is common to keep the earlier layers (those closest to the input layer) frozen, as they capture lower-level features, while later layers often discern high-level features that can be more related to the task that the model is trained on.
Models that are pre-trained on large, general corpora are usually fine-tuned by reusing their parameters as a starting point and adding a task-specific layer trained from scratch. Fine-tuning the full model is also common and often yields better results, but is more computationally expensive.
Fine-tuning is typically accomplished via supervised learning, but there are also techniques to fine-tune a model using weak supervision. Fine-tuning can be combined with a reinforcement learning from human feedback-based objective to produce language models such as ChatGPT (a fine-tuned version of GPT-3) and Sparrow.


== Robustness ==
Fine-tuning can degrade a model's robustness to distribution shifts. One mitigation is to linearly interpolate a fine-tuned model's weights with the weights of the original model, which can greatly increase out-of-distribution performance while largely retaining the in-distribution performance of the fine-tuned model."
"LightGBM, short for Light Gradient-Boosting Machine, is a free and open-source distributed gradient-boosting framework for machine learning, originally developed by Microsoft. It is based on decision tree algorithms and used for ranking, classification and other machine learning tasks. The development focus is on performance and scalability.


== Overview ==
The LightGBM framework supports different algorithms including GBT, GBDT, GBRT, GBM, MART and RF. LightGBM has many of XGBoost's advantages, including sparse optimization, parallel training, multiple loss functions, regularization, bagging, and early stopping. A major difference between the two lies in the construction of trees. LightGBM does not grow a tree level-wise — row by row — as most other implementations do. Instead it grows trees leaf-wise. It will choose the leaf with max delta loss to grow.  Besides, LightGBM does not use the widely used sorted-based decision tree learning algorithm, which searches the best split point on sorted feature values, as XGBoost or other implementations do. Instead, LightGBM implements a highly optimized histogram-based decision tree learning algorithm, which yields great advantages on both efficiency and memory consumption."
"Federated learning (also known as collaborative learning) is a machine learning technique focusing on settings in which multiple entities (often referred to as clients) collaboratively train a model while ensuring that their data remains decentralized. This stands in contrast to machine learning settings in which data is centrally stored. One of the primary defining characteristics of federated learning is data heterogeneity. Due to the decentralized nature of the clients' data, there is no guarantee that data samples held by each client are independently and identically distributed.
Federated learning is generally concerned with and motivated by issues such as data privacy, data minimization, and data access rights. Its applications involve a variety of research areas including defence, telecommunications, the Internet of things, and pharmaceuticals.


== Definition ==
Federated learning aims at training a machine learning algorithm, for instance deep neural networks, on multiple local datasets contained in local nodes without explicitly exchanging data samples. The general principle consists in training local models on local data samples and exchanging parameters (e.g. the weights and biases of a deep neural network) between these local nodes at some frequency to generate a global model shared by all nodes.
The main difference between federated learning and distributed learning lies in the assumptions made on the properties of the local datasets, as distributed learning originally aims at parallelizing computing power where federated learning originally aims at training on heterogeneous datasets."
"A keep is a type of fortified tower built within castles during the Middle Ages by European nobility. Scholars have debated the scope of the word keep, but usually consider it to refer to large towers in castles that were fortified residences, used as a refuge of last resort should the rest of the castle fall to an adversary. The first keeps were made of timber and formed a key part of the motte-and-bailey castles that emerged in Normandy and Anjou during the 10th century; the design spread to England, Portugal, south Italy and Sicily. As a result of the Norman Conquest of England in 1066, use spread into Wales during the second half of the 11th century and into Ireland in the 1170s. The Anglo-Normans and French rulers began to build stone keeps during the 10th and 11th centuries, including Norman keeps, with a square or rectangular design, and circular shell keeps. Stone keeps carried considerable political as well as military importance and could take a decade or more to build.
During the 12th century, new designs began to be introduced – in France, quatrefoil-shaped keeps were introduced, while in England polygonal towers were built. By the end of the century, French and English keep designs began to diverge: Philip II of France built a sequence of circular keeps as part of his bid to stamp his royal authority on his new territories, while in England castles were built without keeps. In Spain, keeps were increasingly incorporated into both Christian and Islamic castles, although in Germany tall fighting towers called bergfriede were preferred to keeps in the western fashion. In the second half of the 14th century, there was a resurgence in the building of keeps."
"In particle physics, a hadron ( ; from Ancient Greek  ἁδρός (hadrós) 'stout, thick') is a composite subatomic particle made of two or more quarks held together by the strong interaction. They are analogous to molecules, which are held together by the electric force. Most of the mass of ordinary matter comes from two hadrons: the proton and the neutron, while most of the mass of the protons and neutrons is in turn due to the binding energy of their constituent quarks, due to the strong force.
Hadrons are categorized into two broad families: baryons, made of an odd number of quarks (usually three) and mesons, made of an even number of quarks (usually two: one quark and one antiquark). Protons and neutrons (which make the majority of the mass of an atom) are examples of baryons; pions are an example of a meson. ""Exotic"" hadrons, containing more than three valence quarks, have been discovered in recent years. A tetraquark state (an exotic meson), named the Z(4430)−, was discovered in 2007 by the Belle Collaboration and confirmed as a resonance in 2014 by the LHCb collaboration. Two pentaquark states (exotic baryons), named P+c(4380) and P+c(4450), were discovered in 2015 by the LHCb collaboration. There are several more exotic hadron candidates and other colour-singlet quark combinations that may also exist.
Almost all ""free"" hadrons and antihadrons (meaning, in isolation and not bound within an atomic nucleus) are believed to be unstable and eventually decay into other particles."
"Gated recurrent units (GRUs) are a gating mechanism in recurrent neural networks, introduced in 2014 by Kyunghyun Cho et al. The GRU is like a long short-term memory (LSTM) with a gating mechanism to input or forget certain features, but lacks a context vector or output gate, resulting in fewer parameters than LSTM. 
GRU's performance on certain tasks of polyphonic music modeling, speech signal modeling and natural language processing was found to be similar to that of LSTM. GRUs showed that gating is indeed helpful in general, and Bengio's team came to no concrete conclusion on which of the two gating units was better.


== Architecture ==
There are several variations on the full gated unit, with gating done using the previous hidden state and the bias in various combinations, and a simplified form called minimal gated unit.
The operator 
  
    
      
        ⊙
      
    
    {\displaystyle \odot }
  
 denotes the Hadamard product in the following.


=== Fully gated unit ===

Initially, for 
  
    
      
        t
        =
        0
      
    
    {\displaystyle t=0}
  
, the output vector is 
  
    
      
        
          h
          
            0
          
        
        =
        0
      
    
    {\displaystyle h_{0}=0}
  
. 

  
    
      
        
          
            
              
                
                  z
                  
                    t
                  
                
              
              
                
                =
                σ
                (
                
                  W
                  
                    z
                  
                
                
                  x
                  
                    t
                  
                
                +
                
                  U
                  
                    z
                  
                
                
                  h
                  
                    t
                    −
                    1
                  
                
                +
                
                  b
                  
                    z
                  
                
                )
              
            
            
              
                
                  r
                  
                    t
                  
                
              
              
                
                =
                σ
                (
                
                  W
                  
                    r
                  
                
                
                  x
                  
                    t
                  
                
                +
                
                  U
                  
                    r
                  
                
                
                  h
                  
                    t
                    −
                    1
                  
                
                +
                
                  b
                  
                    r
                  
                
                )
              
            
            
              
                
                  
                    
                      
                        h
                        ^
                      
                    
                  
                  
                    t
                  
                
              
              
                
                =
                ϕ
                (
                
                  W
                  
                    h
                  
                
                
                  x
                  
                    t
                  
                
                +
                
                  U
                  
                    h
                  
                
                (
                
                  r
                  
                    t
                  
                
                ⊙
                
                  h
                  
                    t
                    −
                    1
                  
                
                )
                +
                
                  b
                  
                    h
                  
                
                )
              
            
            
              
                
                  h
                  
                    t
                  
                
              
              
                
                =
                (
                1
                −
                
                  z
                  
                    t
                  
                
                )
                ⊙
                
                  h
                  
                    t
                    −
                    1
                  
                
                +
                
                  z
                  
                    t
                  
                
                ⊙
                
                  
                    
                      
                        h
                        ^
                      
                    
                  
                  
                    t
                  
                
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}z_{t}&=\sigma (W_{z}x_{t}+U_{z}h_{t-1}+b_{z})\\r_{t}&=\sigma (W_{r}x_{t}+U_{r}h_{t-1}+b_{r})\\{\hat {h}}_{t}&=\phi (W_{h}x_{t}+U_{h}(r_{t}\odot h_{t-1})+b_{h})\\h_{t}&=(1-z_{t})\odot h_{t-1}+z_{t}\odot {\hat {h}}_{t}\end{aligned}}}
  

Variables (
  
    
      
        d
      
    
    {\displaystyle d}
  
 denotes the number of input features and 
  
    
      
        e
      
    
    {\displaystyle e}
  
 the number of output features):

  
    
      
        
          x
          
            t
          
        
        ∈
        
          
            R
          
          
            d
          
        
      
    
    {\displaystyle x_{t}\in \mathbb {R} ^{d}}
  
: input vector

  
    
      
        
          h
          
            t
          
        
        ∈
        
          
            R
          
          
            e
          
        
      
    
    {\displaystyle h_{t}\in \mathbb {R} ^{e}}
  
: output vector

  
    
      
        
          
            
              
                h
                ^
              
            
          
          
            t
          
        
        ∈
        
          
            R
          
          
            e
          
        
      
    
    {\displaystyle {\hat {h}}_{t}\in \mathbb {R} ^{e}}
  
: candidate activation vector

  
    
      
        
          z
          
            t
          
        
        ∈
        (
        0
        ,
        1
        
          )
          
            e
          
        
      
    
    {\displaystyle z_{t}\in (0,1)^{e}}
  
: update gate vector

  
    
      
        
          r
          
            t
          
        
        ∈
        (
        0
        ,
        1
        
          )
          
            e
          
        
      
    
    {\displaystyle r_{t}\in (0,1)^{e}}
  
: reset gate vector

  
    
      
        W
        ∈
        
          
            R
          
          
            e
            ×
            d
          
        
      
    
    {\displaystyle W\in \mathbb {R} ^{e\times d}}
  
, 
  
    
      
        U
        ∈
        
          
            R
          
          
            e
            ×
            e
          
        
      
    
    {\displaystyle U\in \mathbb {R} ^{e\times e}}
  
 and 
  
    
      
        b
        ∈
        
          
            R
          
          
            e
          
        
      
    
    {\displaystyle b\in \mathbb {R} ^{e}}
  
: parameter matrices and vector which need to be learned during training
Activation functions

  
    
      
        σ
      
    
    {\displaystyle \sigma }
  
: The original is a logistic function.

  
    
      
        ϕ
      
    
    {\displaystyle \phi }
  
: The original is a hyperbolic tangent.
Alternative activation functions are possible, provided that 
  
    
      
        σ
        (
        x
        )
        ∈
        [
        0
        ,
        1
        ]
      
    
    {\displaystyle \sigma (x)\in [0,1]}
  
.

Alternate forms can be created by changing 
  
    
      
        
          z
          
            t
          
        
      
    
    {\displaystyle z_{t}}
  
 and 
  
    
      
        
          r
          
            t
          
        
      
    
    {\displaystyle r_{t}}
  

Type 1, each gate depends only on the previous hidden state and the bias.

  
    
      
        
          
            
              
                
                  z
                  
                    t
                  
                
              
              
                
                =
                σ
                (
                
                  U
                  
                    z
                  
                
                
                  h
                  
                    t
                    −
                    1
                  
                
                +
                
                  b
                  
                    z
                  
                
                )
              
            
            
              
                
                  r
                  
                    t
                  
                
              
              
                
                =
                σ
                (
                
                  U
                  
                    r
                  
                
                
                  h
                  
                    t
                    −
                    1
                  
                
                +
                
                  b
                  
                    r
                  
                
                )
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}z_{t}&=\sigma (U_{z}h_{t-1}+b_{z})\\r_{t}&=\sigma (U_{r}h_{t-1}+b_{r})\\\end{aligned}}}
  

Type 2, each gate depends only on  the previous hidden state."
"Qiskit is an open-source software development kit (SDK) for working with quantum computers at the level of circuits, pulses, and algorithms. It provides tools for creating and manipulating quantum programs and running them on prototype quantum devices on IBM Quantum Platform or on simulators on a local computer. It follows the circuit model for universal quantum computation, and can be used for any quantum hardware (currently supports superconducting qubits and trapped ions) that follows this model.
Qiskit was founded by IBM Research to allow software development for their cloud quantum computing service, IBM Quantum Experience. Contributions are also made by external supporters, typically from academic institutions.
The primary version of Qiskit uses the Python programming language. Versions for Swift and JavaScript were initially explored, though the development for these versions has halted. Instead, a minimal re-implementation of basic features is available as MicroQiskit, which is made to be easy to port to alternative platforms.
A range of Jupyter notebooks are provided with examples of quantum computing being used. Examples include the source code behind scientific studies that use Qiskit, as well as a set of exercises to help people to learn the basics of quantum programming."
"A star is a luminous spheroid of plasma held together by self-gravity. The nearest star to Earth is the Sun. Many other stars are visible to the naked eye at night; their immense distances from Earth make them appear as fixed points of light. The most prominent stars have been categorised into constellations and asterisms, and many of the brightest stars have proper names. Astronomers have assembled star catalogues that identify the known stars and provide standardized stellar designations. The observable universe contains an estimated 1022 to 1024 stars. Only about 4,000 of these stars are visible to the naked eye—all within the Milky Way galaxy.
A star's life begins with the gravitational collapse of a gaseous nebula of material largely comprising hydrogen, helium, and trace heavier elements. Its total mass mainly determines its evolution and eventual fate. A star shines for most of its active life due to the thermonuclear fusion of hydrogen into helium in its core."
"In statistics and control theory, Kalman filtering (also known as linear quadratic estimation) is an algorithm that uses a series of measurements observed over time, including statistical noise and other inaccuracies, to produce estimates of unknown variables that tend to be more accurate than those based on a single measurement, by estimating a joint probability distribution over the variables for each time-step. The filter is constructed as a mean squared error minimiser, but an alternative derivation of the filter is also provided showing how the filter relates to maximum likelihood statistics. The filter is named after Rudolf E. Kálmán.
Kalman filtering has numerous technological applications. A common application is for guidance, navigation, and control of vehicles, particularly aircraft, spacecraft and ships positioned dynamically. Furthermore, Kalman filtering is much applied in time series analysis tasks such as signal processing and econometrics. Kalman filtering is also important for robotic motion planning and control, and can be used for trajectory optimization. Kalman filtering also works for modeling the central nervous system's control of movement. Due to the time delay between issuing motor commands and receiving sensory feedback, the use of Kalman filters provides a realistic model for making estimates of the current state of a motor system and issuing updated commands.
The algorithm works via a two-phase process: a prediction phase and an update phase."
"Proximal policy optimization (PPO) is a reinforcement learning (RL) algorithm for training an intelligent agent's decision function to accomplish difficult tasks. PPO was developed by John Schulman in 2017, and had become the default RL algorithm at the US artificial intelligence company OpenAI. Since 2018, PPO has seen success in a wide variety of applications, such as controlling a robotic arm, beating professional players at Dota 2, and excelling in Atari games. Many experts called PPO the state of the art, due to its ability to strike a balance between performance and comprehension. Compared with other algorithms, the three main advantages of PPO are simplicity, stability, and sample efficiency.
PPO is classified as a policy gradient method for training an agent's policy network. This network approximates a policy function, used by the agent to make decisions. Essentially, to train the policy function, PPO takes a small policy update step, so the agent can reliably reach the optimal solution. A step size that is too big may direct the policy in a suboptimal direction, thus having little possibility of recovery; a step size that is too small lowers the overall efficiency. Consequently, PPO implements a clip function that constrains the policy update of an agent from being too large, so that larger step sizes may be used without negatively affecting the gradient ascent process.


== Development ==
In 2015, John Schulman introduced Trust Region Policy Optimization (TRPO), which was a predecessor of PPO. TRPO addressed the instability issue of another algorithm, the Deep Q-Network (DQN), by using the trust region constraint to regulate the KL divergence between the old and new policies."
"Differentiable programming is a programming paradigm in which a numeric computer program can be differentiated throughout via automatic differentiation. This allows for gradient-based optimization of parameters in the program, often via gradient descent, as well as other learning approaches that are based on higher order derivative information. Differentiable programming has found use in a wide variety of areas, particularly scientific computing and machine learning. One of the early proposals to adopt such a framework in a systematic fashion to improve upon learning algorithms was made by the Advanced Concepts Team at the European Space Agency in early 2016.


== Approaches ==
Most differentiable programming frameworks work by constructing a graph containing the control flow and data structures in the program. Attempts generally fall into two groups:

 Static, compiled graph-based approaches such as TensorFlow, Theano, and MXNet. They tend to allow for good compiler optimization and easier scaling to large systems, but their static nature limits interactivity and the types of programs that can be created easily (e.g. those involving loops or recursion), as well as making it harder for users to reason effectively about their programs. A proof of concept compiler toolchain called Myia uses a subset of Python as a front end and supports higher-order functions, recursion, and higher-order derivatives.
Operator overloading, dynamic graph based approaches such as PyTorch, NumPy's autograd package as well as Pyaudi."
"Dimensionality reduction, or dimension reduction, is the transformation of data from a high-dimensional space into a low-dimensional space so that the low-dimensional representation retains some meaningful properties of the original data, ideally close to its intrinsic dimension. Working in high-dimensional spaces can be undesirable for many reasons; raw data are often sparse as a consequence of the curse of dimensionality, and analyzing the data is usually computationally intractable. Dimensionality reduction is common in fields that deal with large numbers of observations and/or large numbers of variables, such as signal processing, speech recognition, neuroinformatics, and bioinformatics.
Methods are commonly divided into linear and nonlinear approaches. Approaches can also be divided into feature selection and feature extraction. Dimensionality reduction can be used for noise reduction, data visualization, cluster analysis, or as an intermediate step to facilitate other analyses.


== Feature selection ==

The process of feature selection aims to find a suitable subset of the input variables (features, or attributes) for the task at hand. The three strategies are: the filter strategy (e.g., information gain), the wrapper strategy (e.g., accuracy-guided search), and the embedded strategy (features are added or removed while building the model based on prediction errors).
Data analysis such as regression or classification can be done in the reduced space more accurately than in the original space.


== Feature projection ==

Feature projection (also called feature extraction) transforms the data from the high-dimensional space to a space of fewer dimensions."
"In machine learning, the polynomial kernel is a kernel function commonly used with support vector machines (SVMs) and other kernelized models, that represents the similarity of vectors (training samples) in a feature space over polynomials of the original variables, allowing learning of non-linear models.
Intuitively, the polynomial kernel looks not only at the given features of input samples to determine their similarity, but also combinations of these. In the context of regression analysis, such combinations are known as interaction features. The (implicit) feature space of a polynomial kernel is equivalent to that of polynomial regression, but without the combinatorial blowup in the number of parameters to be learned. When the input features are binary-valued (booleans), then the features correspond to logical conjunctions of input features.


== Definition ==
For degree-d polynomials, the polynomial kernel is defined as

  
    
      
        K
        (
        
          x
        
        ,
        
          y
        
        )
        =
        (
        
          
            x
          
          
            
              T
            
          
        
        
          y
        
        +
        c
        
          )
          
            d
          
        
      
    
    {\displaystyle K(\mathbf {x} ,\mathbf {y} )=(\mathbf {x} ^{\mathsf {T}}\mathbf {y} +c)^{d}}
  

where x and y are vectors of size n in the input space, i.e. vectors of features computed from training or test samples and c ≥ 0 is a free parameter trading off the influence of higher-order versus lower-order terms in the polynomial. When c = 0, the kernel is called homogeneous. (A further generalized polykernel divides xTy by a user-specified scalar parameter a.)
As a kernel, K corresponds to an inner product in a feature space based on some mapping φ:

  
    
      
        K
        (
        
          x
        
        ,
        
          y
        
        )
        =
        ⟨
        φ
        (
        
          x
        
        )
        ,
        φ
        (
        
          y
        
        )
        ⟩
      
    
    {\displaystyle K(\mathbf {x} ,\mathbf {y} )=\langle \varphi (\mathbf {x} ),\varphi (\mathbf {y} )\rangle }
  

The nature of φ can be seen from an example. Let d = 2, so we get the special case of the quadratic kernel."
"Stochastic gradient descent (often abbreviated SGD) is an iterative method for optimizing an objective function with suitable smoothness properties (e.g. differentiable or subdifferentiable). It can be regarded as a stochastic approximation of gradient descent optimization, since it replaces the actual gradient (calculated from the entire data set) by an estimate thereof (calculated from a randomly selected subset of the data). Especially in high-dimensional optimization problems this reduces the very high computational burden, achieving faster iterations in exchange for a lower convergence rate.
The basic idea behind stochastic approximation can be traced back to the Robbins–Monro algorithm of the 1950s. Today, stochastic gradient descent has become an important optimization method in machine learning.


== Background ==

Both statistical estimation and machine learning consider the problem of minimizing an objective function that has the form of a sum:

  
    
      
        Q
        (
        w
        )
        =
        
          
            1
            n
          
        
        
          ∑
          
            i
            =
            1
          
          
            n
          
        
        
          Q
          
            i
          
        
        (
        w
        )
        ,
      
    
    {\displaystyle Q(w)={\frac {1}{n}}\sum _{i=1}^{n}Q_{i}(w),}
  

where the parameter 
  
    
      
        w
      
    
    {\displaystyle w}
  
 that minimizes 
  
    
      
        Q
        (
        w
        )
      
    
    {\displaystyle Q(w)}
  
 is to be estimated. Each summand function 
  
    
      
        
          Q
          
            i
          
        
      
    
    {\displaystyle Q_{i}}
  
 is typically associated with the 
  
    
      
        i
      
    
    {\displaystyle i}
  
-th observation in the data set (used for training).
In classical statistics, sum-minimization problems arise in least squares and in maximum-likelihood estimation (for independent observations).  The general class of estimators that arise as minimizers of sums are called M-estimators."
"Super-resolution imaging (SR) is a class of techniques that enhance (increase) the resolution of an imaging system. In optical SR the diffraction limit of systems is transcended, while in geometrical SR the resolution of digital imaging sensors is enhanced.
In some radar and sonar imaging applications (e.g. magnetic resonance imaging (MRI), high-resolution computed tomography), subspace decomposition-based methods (e.g. MUSIC) and compressed sensing-based algorithms (e.g., SAMV) are employed to achieve SR over standard periodogram algorithm.
Super-resolution imaging techniques are used in general image processing and in super-resolution microscopy.


== Basic concepts ==
Because some of the ideas surrounding super-resolution raise fundamental issues, there is need at the outset to examine the relevant physical and information-theoretical principles:

Diffraction limit: The detail of a physical object that an optical instrument can reproduce in an image has limits that are mandated by laws of physics, whether formulated by the diffraction equations in the wave theory of light or equivalently the uncertainty principle for photons in quantum mechanics.  Information transfer can never be increased beyond this boundary, but packets outside the limits can be cleverly swapped for (or multiplexed with) some inside it.  One does not so much “break” as “run around” the diffraction limit. New procedures probing electro-magnetic disturbances at the molecular level (in the so-called near field) remain fully consistent with Maxwell's equations."
"Zero-shot learning (ZSL) is a problem setup in deep learning where, at test time, a learner observes samples from classes which were not observed during training, and needs to predict the class that they belong to. The name is a play on words based on the earlier concept of one-shot learning, in which classification can be learned from only one, or a few, examples.
Zero-shot methods generally work by associating observed and non-observed classes through some form of auxiliary information, which encodes observable distinguishing properties of objects. For example, given a set of images of animals to be classified, along with auxiliary textual descriptions of what animals look like, an artificial intelligence model which has been trained to recognize horses, but has never been given a zebra, can still recognize a zebra when it also knows that zebras look like striped horses. This problem is widely studied in computer vision, natural language processing, and machine perception.


== Background and history ==
The first paper on zero-shot learning in natural language processing appeared in a 2008 paper by Chang, Ratinov, Roth, and Srikumar, at the AAAI’08, but the name given to the learning paradigm there was dataless classification. The first paper on zero-shot learning in computer vision appeared at the same conference, under the name zero-data learning. The term zero-shot learning itself first appeared in the literature in a 2009 paper from Palatucci, Hinton, Pomerleau, and Mitchell at NIPS’09. This terminology was repeated later in another computer vision paper and the term zero-shot learning caught on, as a take-off on one-shot learning that was introduced in computer vision years earlier. 
In computer vision, zero-shot learning models learned parameters for seen classes along with their class representations and rely on representational similarity among class labels so that, during inference, instances can be classified into new classes."
"Natural language generation (NLG) is a software process that produces natural language output. A widely-cited survey of NLG methods describes  NLG as ""the subfield of artificial intelligence and computational linguistics that is concerned with the construction of computer systems that can produce understandable texts in English or other human languages from some underlying non-linguistic representation of information"".
While it is widely agreed that the output of any NLG process is text, there is some disagreement about whether the inputs of an NLG system need to be non-linguistic. Common applications of NLG methods include the production of various reports, for example weather  and patient reports; image captions; and chatbots like chatGPT.
Automated NLG can be compared to the process humans use when they turn ideas into writing or speech. Psycholinguists prefer the term language production for this process, which can also be described in mathematical terms, or modeled in a computer for psychological research. NLG systems can also be compared to translators of artificial computer languages, such as decompilers or transpilers, which also produce human-readable code generated from an intermediate representation. Human languages tend to be considerably more complex and allow for much more ambiguity and variety of expression than programming languages, which makes NLG more challenging.
NLG may be viewed as complementary to natural-language understanding (NLU): whereas in natural-language understanding, the system needs to disambiguate the input sentence to produce the machine representation language, in NLG the system needs to make decisions about how to put a representation into words. The practical considerations in building NLU vs. NLG systems are not symmetrical."
"Coca is any of the four cultivated plants in the family Erythroxylaceae, native to western South America. Coca is known worldwide for its psychoactive alkaloid, cocaine.
Different early-Holocene peoples in different areas of South America independently transformed Erythroxylum gracilipes plants into quotidian stimulant and medicinal crops now collectively called Coca. Archaeobotanical evidence show that Coca crops have been grown for well over 8,000 years in South America. They have had and still have a significant role in spiritual, economic, social and political dimensions for numerous indigenous cultures in the Andes and the Western Amazon arising from the use of the leaves as drugs and mild, daily stimulant.
The plant is grown as a cash crop in the Argentine Northwest, Bolivia, Alto Rio Negro Territory in Brazil, Colombia, Venezuela, Ecuador, and Peru, even in areas where its cultivation is unlawful. There are some reports that the plant is being cultivated in the south of Mexico, by using seeds imported from South America, as an alternative to smuggling its recreational product cocaine.
It also plays a fundamental role in many traditional Amazonian and Andean cultures as well as the Sierra Nevada de Santa Marta in northern Colombia.
The cocaine alkaloid content of dry Erythroxylum coca var. coca leaves was measured ranging from 0.23% to 0.96%."
"5 (five) is a number, numeral and digit. It is the natural number, and cardinal number, following 4 and preceding 6, and is a prime number. 
Humans, and many other animals, have 5 digits on their limbs. 


== Mathematics ==

5 is a Fermat prime, a Mersenne prime exponent, as well as a Fibonacci number. 5 is the first congruent number, as well as the length of the hypotenuse of the smallest integer-sided right triangle, making part of the smallest Pythagorean triple (3, 4, 5). 
5 is the first safe prime, and the first good prime. 11 forms the first pair of sexy primes with 5. 5 is the second Fermat prime, of a total of five known Fermat primes. 5 is also the first of three known Wilson primes (5, 13, 563). 


=== Geometry ===
A shape with five sides is called a pentagon."
"Symbolic regression (SR) is a type of regression analysis that searches the space of mathematical expressions to find the model that best fits a given dataset, both in terms of accuracy and simplicity.
No particular model is provided as a starting point for symbolic regression. Instead, initial expressions are formed by randomly combining mathematical building blocks such as mathematical operators, analytic functions, constants, and state variables. Usually, a subset of these primitives will be specified by the person operating it, but that's not a requirement of the technique. The symbolic regression problem for mathematical functions has been tackled with a variety of methods, including recombining equations most commonly using genetic programming, as well as more recent methods utilizing Bayesian methods and neural networks. Another non-classical alternative method to SR is called Universal Functions Originator (UFO), which has a different mechanism, search-space, and building strategy. Further methods such as Exact Learning attempt to transform the fitting problem into a moments problem in a natural function space, usually built around generalizations of the Meijer-G function.
By not requiring a priori specification of a model, symbolic regression isn't affected by human bias, or unknown gaps in domain knowledge. It attempts to uncover the intrinsic relationships of the dataset, by letting the patterns in the data itself reveal the appropriate models, rather than imposing a model structure that is deemed mathematically tractable from a human perspective. The fitness function that drives the evolution of the models takes into account not only error metrics (to ensure the models accurately predict the data), but also special complexity measures, thus ensuring that the resulting models reveal the data's underlying structure in a way that's understandable from a human perspective."
"In probability theory and statistics, a Gaussian process is a stochastic process (a collection of random variables indexed by time or space), such that every finite collection of those random variables has a multivariate normal distribution. The distribution of a Gaussian process is the joint distribution of all those (infinitely many) random variables, and as such, it is a distribution over functions with a continuous domain, e.g. time or space.
The concept of Gaussian processes is named after Carl Friedrich Gauss because it is based on the notion of the Gaussian distribution (normal distribution). Gaussian processes can be seen as an infinite-dimensional generalization of multivariate normal distributions.
Gaussian processes are useful in statistical modelling, benefiting from properties inherited from the normal distribution. For example, if a random process is modelled as a Gaussian process, the distributions of various derived quantities can be obtained explicitly. Such quantities include the average value of the process over a range of times and the error in estimating the average using sample values at a small set of times. While exact models often scale poorly as the amount of data increases, multiple approximation methods have been developed which often retain good accuracy while drastically reducing computation time.


== Definition ==
A time continuous stochastic process 
  
    
      
        
          {
          
            
              X
              
                t
              
            
            ;
            t
            ∈
            T
          
          }
        
      
    
    {\displaystyle \left\{X_{t};t\in T\right\}}
  
 is Gaussian if and only if for every finite set of indices 
  
    
      
        
          t
          
            1
          
        
        ,
        …
        ,
        
          t
          
            k
          
        
      
    
    {\displaystyle t_{1},\ldots ,t_{k}}
  
 in the index set 
  
    
      
        T
      
    
    {\displaystyle T}
  

  
    
      
        
          
            X
          
          
            
              t
              
                1
              
            
            ,
            …
            ,
            
              t
              
                k
              
            
          
        
        =
        (
        
          X
          
            
              t
              
                1
              
            
          
        
        ,
        …
        ,
        
          X
          
            
              t
              
                k
              
            
          
        
        )
      
    
    {\displaystyle \mathbf {X} _{t_{1},\ldots ,t_{k}}=(X_{t_{1}},\ldots ,X_{t_{k}})}
  

is a multivariate Gaussian random variable."
"Recurrent neural networks (RNNs) are a class of artificial neural network commonly used for sequential data processing. Unlike feedforward neural networks, which process data in a single pass, RNNs process data across multiple time steps, making them well-adapted for modelling and processing text, speech, and time series.
The building block of RNNs is the recurrent unit. This unit maintains a hidden state, essentially a form of memory, which is updated at each time step based on the current input and the previous hidden state. This feedback loop allows the network to learn from past inputs, and incorporate that knowledge into its current processing.
Early RNNs suffered from the vanishing gradient problem, limiting their ability to learn long-range dependencies. This was solved by the long short-term memory (LSTM) variant in 1997, thus making it the standard architecture for RNN.
RNNs have been applied to tasks such as unsegmented, connected handwriting recognition, speech recognition, natural language processing, and neural machine translation.


== History ==


=== Before modern ===
One origin of RNN was neuroscience. The word ""recurrent"" is used to describe loop-like structures in anatomy. In 1901, Cajal observed ""recurrent semicircles"" in the cerebellar cortex formed by parallel fiber, Purkinje cells, and granule cells."
"In computer science and operations research, the ant colony optimization algorithm (ACO) is a probabilistic technique for solving computational problems that can be reduced to finding good paths through graphs. Artificial ants represent multi-agent methods inspired by the behavior of real ants.
The pheromone-based communication of biological ants is often the predominant paradigm used. Combinations of artificial ants and local search algorithms have become a preferred method for numerous optimization tasks involving some sort of graph, e.g., vehicle routing and internet routing.
As an example, ant colony optimization is a class of optimization algorithms modeled on the actions of an ant colony. Artificial 'ants' (e.g. simulation agents) locate optimal solutions by moving through a parameter space representing all possible solutions. Real ants lay down pheromones to direct each other to resources while exploring their environment.  The simulated 'ants' similarly record their positions and the quality of their solutions, so that in later simulation iterations more ants locate better solutions.  One variation on this approach is the bees algorithm, which is more analogous to the foraging patterns of the honey bee, another social insect."
"Exponential smoothing or exponential moving average (EMA) is a rule of thumb technique for smoothing time series data using the exponential window function. Whereas in the simple moving average the past observations are weighted equally, exponential functions are used to assign exponentially decreasing weights over time. It is an easily learned and easily applied procedure for making some determination based on prior assumptions by the user, such as seasonality. Exponential smoothing is often used for analysis of time-series data.
Exponential smoothing is one of many window functions commonly applied to smooth data in signal processing, acting as low-pass filters to remove high-frequency noise. This method is preceded by Poisson's use of recursive exponential window functions in convolutions from the 19th century, as well as Kolmogorov and Zurbenko's use of recursive moving averages from their studies of turbulence in the 1940s.
The raw data sequence is often represented by 
  
    
      
        {
        
          x
          
            t
          
        
        }
      
    
    {\textstyle \{x_{t}\}}
  
 beginning at time 
  
    
      
        t
        =
        0
      
    
    {\textstyle t=0}
  
, and the output of the exponential smoothing algorithm is commonly written as 
  
    
      
        {
        
          s
          
            t
          
        
        }
      
    
    {\textstyle \{s_{t}\}}
  
, which may be regarded as a best estimate of what the next value of 
  
    
      
        x
      
    
    {\textstyle x}
  
 will be. When the sequence of observations begins at time 
  
    
      
        t
        =
        0
      
    
    {\textstyle t=0}
  
, the simplest form of exponential smoothing is given by the following formulas:

  
    
      
        
          
            
              
                
                  s
                  
                    0
                  
                
              
              
                
                =
                
                  x
                  
                    0
                  
                
              
            
            
              
                
                  s
                  
                    t
                  
                
              
              
                
                =
                α
                
                  x
                  
                    t
                  
                
                +
                (
                1
                −
                α
                )
                
                  s
                  
                    t
                    −
                    1
                  
                
                ,
                
                t
                >
                0
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}s_{0}&=x_{0}\\s_{t}&=\alpha x_{t}+(1-\alpha )s_{t-1},\quad t>0\end{aligned}}}
  

where 
  
    
      
        α
      
    
    {\textstyle \alpha }
  
 is the smoothing factor, and 
  
    
      
        0
        <
        α
        <
        1
      
    
    {\textstyle 0<\alpha <1}
  
. If 
  
    
      
        
          s
          
            t
            −
            1
          
        
      
    
    {\textstyle s_{t-1}}
  
 is substituted into 
  
    
      
        
          s
          
            t
          
        
      
    
    {\textstyle s_{t}}
  
 continuously so that the formula of 
  
    
      
        
          s
          
            t
          
        
      
    
    {\textstyle s_{t}}
  
 is fully expressed in terms of 
  
    
      
        {
        
          x
          
            t
          
        
        }
      
    
    {\textstyle \{x_{t}\}}
  
, then exponentially decaying weighting factors on each raw data 
  
    
      
        
          x
          
            t
          
        
      
    
    {\textstyle x_{t}}
  
 is revealed, showing how exponential smoothing is named.
The simple exponential smoothing is not able to predict what would be observed at 
  
    
      
        t
        +
        m
      
    
    {\textstyle t+m}
  
 based on the raw data up to 
  
    
      
        t
      
    
    {\textstyle t}
  
, while the double exponential smoothing and triple exponential smoothing can be used for the prediction due to the presence of 
  
    
      
        
          b
          
            t
          
        
      
    
    {\displaystyle b_{t}}
  
 as the sequence of best estimates of the linear trend."
"Federated learning (also known as collaborative learning) is a machine learning technique focusing on settings in which multiple entities (often referred to as clients) collaboratively train a model while ensuring that their data remains decentralized. This stands in contrast to machine learning settings in which data is centrally stored. One of the primary defining characteristics of federated learning is data heterogeneity. Due to the decentralized nature of the clients' data, there is no guarantee that data samples held by each client are independently and identically distributed.
Federated learning is generally concerned with and motivated by issues such as data privacy, data minimization, and data access rights. Its applications involve a variety of research areas including defence, telecommunications, the Internet of things, and pharmaceuticals.


== Definition ==
Federated learning aims at training a machine learning algorithm, for instance deep neural networks, on multiple local datasets contained in local nodes without explicitly exchanging data samples. The general principle consists in training local models on local data samples and exchanging parameters (e.g. the weights and biases of a deep neural network) between these local nodes at some frequency to generate a global model shared by all nodes.
The main difference between federated learning and distributed learning lies in the assumptions made on the properties of the local datasets, as distributed learning originally aims at parallelizing computing power where federated learning originally aims at training on heterogeneous datasets."
"The ethics of artificial intelligence covers a broad range of topics within the field that are considered to have particular ethical stakes. This includes algorithmic biases, fairness, automated decision-making, accountability, privacy, and regulation. 
It also covers various emerging or potential future challenges such as machine ethics (how to make machines that behave ethically), lethal autonomous weapon systems, arms race dynamics, AI safety and alignment, technological unemployment, AI-enabled misinformation, how to treat certain AI systems if they have a moral status (AI welfare and rights), artificial superintelligence and existential risks. 
Some application areas may also have particularly important ethical implications, like healthcare, education, criminal justice, or the military.


== Machine ethics ==

Machine ethics (or machine morality) is the field of research concerned with designing Artificial Moral Agents (AMAs), robots or artificially intelligent computers that behave morally or as though moral. To account for the nature of these agents, it has been suggested to consider certain philosophical ideas, like the standard characterizations of agency, rational agency, moral agency, and artificial agency, which are related to the concept of AMAs.
There are discussions on creating tests to see if an AI is capable of making ethical decisions. Alan Winfield concludes that the Turing test is flawed and the requirement for an AI to pass the test is too low. A proposed alternative test is one called the Ethical Turing Test, which would improve on the current test by having multiple judges decide if the AI's decision is ethical or unethical. Neuromorphic AI could be one way to create morally capable robots, as it aims to process information similarly to humans, nonlinearly and with millions of interconnected artificial neurons."
"In machine learning, supervised learning (SL) is a paradigm where a model is trained using input objects (e.g. a vector of predictor variables) and desired output values (also known as a supervisory signal), which are often human-made labels. The training process builds a function that maps new data to expected output values. An optimal scenario will allow for the algorithm to accurately determine output values for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a ""reasonable"" way (see inductive bias). This statistical quality of an algorithm is measured via a generalization error.


== Steps to follow ==
To solve a given problem of supervised learning, the following steps must be performed:

Determine the type of training samples. Before doing anything else, the user should decide what kind of data is to be used as a training set. In the case of handwriting analysis, for example, this might be a single handwritten character, an entire handwritten word, an entire sentence of handwriting, or a full paragraph of handwriting.
Gather a training set."
"Self-supervised learning (SSL) is a paradigm in machine learning where a model is trained on a task using the data itself to generate supervisory signals, rather than relying on externally-provided labels. In the context of neural networks, self-supervised learning aims to leverage inherent structures or relationships within the input data to create meaningful training signals. SSL tasks are designed so that solving them requires capturing essential features or relationships in the data. The input data is typically augmented or transformed in a way that creates pairs of related samples, where one sample serves as the input, and the other is used to formulate the supervisory signal. This augmentation can involve introducing noise, cropping, rotation, or other transformations. Self-supervised learning more closely imitates the way humans learn to classify objects. 
During SSL, the model learns in two steps. First, the task is solved based on an auxiliary or pretext classification task using pseudo-labels, which help to initialize the model parameters. Next, the actual task is performed with supervised or unsupervised learning.
Self-supervised learning has produced promising results in recent years, and has found practical application in fields such as audio processing, and is being used by Facebook and others for speech recognition."
"A facial recognition system is a technology potentially capable of matching a human face from a digital image or a video frame against a database of faces. Such a system is typically employed to authenticate users through ID verification services, and works by pinpointing and measuring facial features from a given image.
Development began on similar systems in the 1960s, beginning as a form of computer application. Since their inception, facial recognition systems have seen wider uses in recent times on smartphones and in other forms of technology, such as robotics. Because computerized facial recognition involves the measurement of a human's physiological characteristics, facial recognition systems are categorized as biometrics. Although the accuracy of facial recognition systems as a biometric technology is lower than iris recognition, fingerprint image acquisition, palm recognition or voice recognition, it is widely adopted due to its contactless process. Facial recognition systems have been deployed in advanced human–computer interaction, video surveillance, law enforcement, passenger screening, decisions on employment and housing and automatic indexing of images.
Facial recognition systems are employed throughout the world today by governments and private companies. Their effectiveness varies, and some systems have previously been scrapped because of their ineffectiveness. The use of facial recognition systems has also raised controversy, with claims that the systems violate citizens' privacy, commonly make incorrect identifications, encourage gender norms and racial profiling, and do not protect important biometric data."
"In digital image processing and computer vision, image segmentation is the process of partitioning a digital image into multiple image segments, also known as image regions or image objects (sets of pixels). The goal of segmentation is to simplify and/or change the representation of an image into something that is more meaningful and easier to analyze. Image segmentation is typically used to locate objects and boundaries (lines, curves, etc.) in images. More precisely, image segmentation is the process of assigning a label to every pixel in an image such that pixels with the same label share certain characteristics.
The result of image segmentation is a set of segments that collectively cover the entire image, or a set of contours extracted from the image (see edge detection). Each of the pixels in a region are similar with respect to some characteristic or computed property, such as color, intensity, or texture. Adjacent regions are significantly different with respect to the same characteristic(s). When applied to a stack of images, typical in medical imaging, the resulting contours after image segmentation can be used to create 3D reconstructions with the help of geometry reconstruction algorithms like marching cubes.


== Applications ==

Some of the practical applications of image segmentation are:

Content-based image retrieval
Machine vision
Medical imaging, and imaging studies in biomedical research, including volume rendered images from computed tomography, magnetic resonance imaging, as well as volume electron microscopy techniques such as FIB-SEM.
Locate tumors and other pathologies
Measure tissue volumes
Diagnosis, study of anatomical structure
Surgery planning
Virtual surgery simulation
Intra-surgery navigation
Radiotherapy
Object detection
Pedestrian detection
Face detection
Brake light detection
Locate objects in satellite images (roads, forests, crops, etc.)
Recognition Tasks
Face recognition
Fingerprint recognition
Iris recognition
Prohibited Item at Airport security checkpoints
Traffic control systems
Video surveillance
Video object co-segmentation and action localization
Several general-purpose algorithms and techniques have been developed for image segmentation. To be useful, these techniques must typically be combined with a domain's specific knowledge in order to effectively solve the domain's segmentation problems."
"An energy-based model (EBM) (also called Canonical Ensemble Learning or Learning via Canonical Ensemble – CEL and LCE, respectively) is an application of canonical ensemble formulation from statistical physics for learning from data. The approach prominently appears in generative artificial intelligence.
EBMs provide a unified framework for many probabilistic and non-probabilistic approaches to such learning, particularly for training graphical and other structured models.
An EBM learns the characteristics of a target dataset and generates a similar but larger dataset. EBMs detect the latent variables of a dataset and generate new datasets with a similar distribution.
Energy-based generative neural networks   is a class of generative models, which aim to learn explicit probability distributions of data in the form of energy-based models, the energy functions of which are parameterized by modern deep neural networks.
Boltzmann machines are a special form of energy-based models with a specific parametrization of the energy.


== Description ==
For a given input 
  
    
      
        x
      
    
    {\displaystyle x}
  
, the model describes an energy 
  
    
      
        
          E
          
            θ
          
        
        (
        x
        )
      
    
    {\displaystyle E_{\theta }(x)}
  
 such that the Boltzmann distribution 
  
    
      
        
          P
          
            θ
          
        
        (
        x
        )
        =
        exp
        ⁡
        (
        −
        β
        
          E
          
            θ
          
        
        (
        x
        )
        )
        
          /
        
        Z
        (
        θ
        )
      
    
    {\displaystyle P_{\theta }(x)=\exp(-\beta E_{\theta }(x))/Z(\theta )}
  
 is a probability (density), and typically 
  
    
      
        β
        =
        1
      
    
    {\displaystyle \beta =1}
  
.
Since the normalization constant:

  
    
      
        Z
        (
        θ
        )
        :=
        
          ∫
          
            x
            ∈
            X
          
        
        d
        x
        exp
        ⁡
        (
        −
        β
        
          E
          
            θ
          
        
        (
        x
        )
        )
      
    
    {\displaystyle Z(\theta ):=\int _{x\in X}dx\exp(-\beta E_{\theta }(x))}
  

(also known as the partition function) depends on all the Boltzmann factors of all possible inputs 
  
    
      
        x
      
    
    {\displaystyle x}
  
, it cannot be easily computed or reliably estimated during training simply using standard maximum likelihood estimation.
However, for maximizing the likelihood during training, the gradient of the log-likelihood of a single training example 
  
    
      
        x
      
    
    {\displaystyle x}
  
 is given by using the chain rule:

  
    
      
        
          ∂
          
            θ
          
        
        log
        ⁡
        
          (
          
            
              P
              
                θ
              
            
            (
            x
            )
          
          )
        
        =
        
          
            E
          
          
            
              x
              ′
            
            ∼
            
              P
              
                θ
              
            
          
        
        [
        
          ∂
          
            θ
          
        
        
          E
          
            θ
          
        
        (
        
          x
          ′
        
        )
        ]
        −
        
          ∂
          
            θ
          
        
        
          E
          
            θ
          
        
        (
        x
        )
        
        (
        ∗
        )
      
    
    {\displaystyle \partial _{\theta }\log \left(P_{\theta }(x)\right)=\mathbb {E} _{x'\sim P_{\theta }}[\partial _{\theta }E_{\theta }(x')]-\partial _{\theta }E_{\theta }(x)\,(*)}
  

The expectation in the above formula for the gradient can be approximately estimated by drawing samples 
  
    
      
        
          x
          ′
        
      
    
    {\displaystyle x'}
  
 from the distribution 
  
    
      
        
          P
          
            θ
          
        
      
    
    {\displaystyle P_{\theta }}
  
 using Markov chain Monte Carlo (MCMC)."
"Speech synthesis is the artificial production of human speech. A computer system used for this purpose is called a speech synthesizer, and can be implemented in software or hardware products. A text-to-speech (TTS) system converts normal language text into speech; other systems render symbolic linguistic representations like phonetic transcriptions into speech. The reverse process is speech recognition.
Synthesized speech can be created by concatenating pieces of recorded speech that are stored in a database. Systems differ in the size of the stored speech units; a system that stores phones or diphones provides the largest output range, but may lack clarity. For specific usage domains, the storage of entire words or sentences allows for high-quality output. Alternatively, a synthesizer can incorporate a model of the vocal tract and other human voice characteristics to create a completely ""synthetic"" voice output.
The quality of a speech synthesizer is judged by its similarity to the human voice and by its ability to be understood clearly. An intelligible text-to-speech program allows people with visual impairments or reading disabilities to listen to written words on a home computer."
"Natural language processing (NLP) is a subfield of computer science and especially artificial intelligence. It is primarily concerned with providing computers with the ability to process data encoded in natural language and is thus closely related to information retrieval, knowledge representation and computational linguistics, a subfield of linguistics. Typically data is collected in text corpora, using either rule-based, statistical or neural-based approaches in machine learning and deep learning.
Major tasks in natural language processing are speech recognition, text classification, natural-language understanding, and natural-language generation.


== History ==

Natural language processing has its roots in the 1950s. Already in 1950, Alan Turing published an article titled ""Computing Machinery and Intelligence"" which proposed what is now called the Turing test as a criterion of intelligence, though at the time that was not articulated as a problem separate from artificial intelligence. The proposed test includes a task that involves the automated interpretation and generation of natural language.


=== Symbolic NLP (1950s – early 1990s) ===
The premise of symbolic NLP is well-summarized by John Searle's Chinese room experiment: Given a collection of rules (e.g., a Chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other NLP tasks) by applying those rules to the data it confronts.

1950s: The Georgetown experiment in 1954 involved fully automatic translation of more than sixty Russian sentences into English. The authors claimed that within three or five years, machine translation would be a solved problem."
"Curriculum learning is a technique in machine learning in which a model is trained on examples of increasing difficulty, where the definition of ""difficulty"" may be provided externally or discovered automatically as part of the training process. This is intended to attain good performance more quickly, or to converge to a better local optimum if the global optimum is not found.


== Approach ==
Most generally, curriculum learning  is the technique of successively increasing the difficulty of examples in the training set that is presented to a model over multiple training iterations. This can produce better results than exposing the model to the full training set immediately under some circumstances; most typically, when the model is able to learn general principles from easier examples, and then gradually incorporate more complex and nuanced information as harder examples are introduced, such as edge cases. This has been shown to work in many domains, most likely as a form of regularization.
There are several major variations in how the technique is applied:

A concept of ""difficulty"" must be defined. This may come from human annotation or an external heuristic; for example in language modeling, shorter sentences might be classified as easier than longer ones. Another approach is to use the performance of another model, with examples accurately predicted by that model being classified as easier (providing a connection to boosting).
Difficulty can be increased steadily or in distinct epochs, and in a deterministic schedule or according to a probability distribution. This may also be moderated by a requirement for diversity at each stage, in cases where easier examples are likely to be disproportionately similar to each other."
"Accuracy and precision are two measures of observational error.
Accuracy is how close a given set of measurements (observations or readings) are to their true value.
Precision is how close the measurements are to each other.
The International Organization for Standardization (ISO) defines a related measure:
trueness, ""the closeness of agreement between the arithmetic mean of a large number of test results and the true or accepted reference value.""

While precision is a description of random errors (a measure of statistical variability),
accuracy has two different definitions:

More commonly, a description of systematic errors (a measure of statistical bias of a given measure of central tendency, such as the mean). In this definition of ""accuracy"", the concept is independent of ""precision"", so a particular set of data can be said to be accurate, precise, both, or neither. This concept corresponds to ISO's trueness.
A combination of both precision and trueness, accounting for the two types of observational error (random and systematic), so that high accuracy requires both high precision and high trueness. This usage corresponds to ISO's definition of accuracy (trueness and precision).


== Common technical definition ==
In simpler terms, given a statistical sample or set of data points from repeated measurements of the same quantity, the sample or set can be said to be accurate if their average is close to the true value of the quantity being measured, while the set can be said to be precise if their standard deviation is relatively small.
In the fields of science and engineering, the accuracy of a measurement system is the degree of closeness of measurements of a quantity to that quantity's true value."
"Markov decision process (MDP), also called a stochastic dynamic program or stochastic control problem, is a model for sequential decision making when outcomes are uncertain.
Originating from operations research in the 1950s, MDPs have since gained recognition in a variety of fields, including ecology, economics, healthcare, telecommunications and reinforcement learning. Reinforcement learning utilizes the MDP framework to model the interaction between a learning agent and its environment. In this framework, the interaction is characterized by states, actions, and rewards. The MDP framework is designed to provide a simplified representation of key elements of artificial intelligence challenges. These elements encompass the understanding of cause and effect, the management of uncertainty and nondeterminism, and the pursuit of explicit goals.
The name comes from its connection to Markov chains, a concept developed by the Russian mathematician Andrey Markov. The ""Markov"" in ""Markov decision process"" refers to the underlying structure of state transitions that still follow the Markov property. The process is called a ""decision process"" because it involves making decisions that influence these state transitions, extending the concept of a Markov chain into the realm of decision-making under uncertainty.


== Definition ==

A Markov decision process is a 4-tuple 
  
    
      
        (
        S
        ,
        A
        ,
        
          P
          
            a
          
        
        ,
        
          R
          
            a
          
        
        )
      
    
    {\displaystyle (S,A,P_{a},R_{a})}
  
, where:

  
    
      
        S
      
    
    {\displaystyle S}
  
 is a set of states called the state space."
"In linguistics, a treebank is a parsed text corpus that annotates syntactic or semantic sentence structure. The construction of parsed corpora in the early 1990s revolutionized computational linguistics, which benefitted from large-scale empirical data.


== Etymology ==
The term treebank was coined by linguist Geoffrey Leech in the 1980s, by analogy to other repositories such as a seedbank or bloodbank. This is because both syntactic and semantic structure are commonly represented compositionally as a tree structure. The term parsed corpus is often used interchangeably with the term treebank, with the emphasis on the primacy of sentences rather than trees.


== Construction ==
Treebanks are often created on top of a corpus that has already been annotated with part-of-speech tags. In turn, treebanks are sometimes enhanced with semantic or other linguistic information. Treebanks can be created completely manually, where linguists annotate each sentence with syntactic structure, or semi-automatically, where a parser assigns some syntactic structure which linguists then check and, if necessary, correct. In practice, fully checking and completing the parsing of natural language corpora is a labour-intensive project that can take teams of graduate linguists several years. The level of annotation detail and the breadth of the linguistic sample determine the difficulty of the task and the length of time required to build a treebank."
"Apache Spark  is an open-source unified analytics engine for large-scale data processing. Spark provides an interface for programming clusters with implicit data parallelism and fault tolerance. Originally developed at the University of California, Berkeley's AMPLab, the Spark codebase was later donated to the Apache Software Foundation, which has maintained it since.


== Overview ==
Apache Spark has its architectural foundation in the resilient distributed dataset (RDD), a read-only multiset of data items distributed over a cluster of machines, that is maintained in a fault-tolerant way. The Dataframe API was released as an abstraction on top of the RDD, followed by the Dataset API. In Spark 1.x, the RDD was the primary application programming interface (API), but as of Spark 2.x use of the Dataset API is encouraged even though the RDD API is not deprecated. The RDD technology still underlies the Dataset API.
Spark and its RDDs were developed in 2012 in response to limitations in the MapReduce cluster computing paradigm, which forces a particular linear dataflow structure on distributed programs: MapReduce programs read input data from disk, map a function across the data, reduce the results of the map, and store reduction results on disk. Spark's RDDs function as a working set for distributed programs that offers a (deliberately) restricted form of distributed shared memory.
Inside Apache Spark the workflow is managed as a directed acyclic graph (DAG). Nodes represent RDDs while edges represent the operations on the RDDs.
Spark facilitates the implementation of both iterative algorithms, which visit their data set multiple times in a loop, and interactive/exploratory data analysis, i.e., the repeated database-style querying of data."
"In statistics, the logistic model (or logit model) is a statistical model that models the log-odds of an event as a linear combination of one or more independent variables. In regression analysis, logistic regression (or logit regression) estimates the parameters of a logistic model (the coefficients in the linear or non linear combinations). In binary logistic regression there is a single binary dependent variable, coded by an indicator variable, where the two values are labeled ""0"" and ""1"", while the independent variables can each be a binary variable (two classes, coded by an indicator variable) or a continuous variable (any real value). The corresponding probability of the value labeled ""1"" can vary between 0 (certainly the value ""0"") and 1 (certainly the value ""1""), hence the labeling; the function that converts log-odds to probability is the logistic function, hence the name. The unit of measurement for the log-odds scale is called a logit, from logistic unit, hence the alternative names. See § Background and § Definition for formal mathematics, and § Example for a worked example.
Binary variables are widely used in statistics to model the probability of a certain class or event taking place, such as the probability of a team winning, of a patient being healthy, etc. (see § Applications), and the logistic model has been the most commonly used model for binary regression since about 1970. Binary variables can be generalized to categorical variables when there are more than two possible values (e.g. whether an image is of a cat, dog, lion, etc.), and the binary logistic regression generalized to multinomial logistic regression."
"The ImageNet project is a large visual database designed for use in visual object recognition software research. More than 14 million images have been hand-annotated by the project to indicate what objects are pictured and in at least one million of the images, bounding boxes are also provided. ImageNet contains more than 20,000 categories, with a typical category, such as ""balloon"" or ""strawberry"", consisting of several hundred images. The database of annotations of third-party image URLs is freely available directly from ImageNet, though the actual images are not owned by ImageNet. Since 2010, the ImageNet project runs an annual software contest, the ImageNet Large Scale Visual Recognition Challenge (ILSVRC), where software programs compete to correctly classify and detect objects and scenes. The challenge uses a ""trimmed"" list of one thousand non-overlapping classes.


== History ==
AI researcher Fei-Fei Li began working on the idea for ImageNet in 2006. At a time when most AI research focused on models and algorithms, Li wanted to expand and improve the data available to train AI algorithms. In 2007, Li met with Princeton professor Christiane Fellbaum, one of the creators of WordNet , to discuss the project. As a result of this meeting, Li went on to build ImageNet starting from the roughly 22,000 nouns of WordNet and using many of its features."
